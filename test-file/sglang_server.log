nohup: ignoring input
/home/users/ntu/yzheng05/.conda/envs/A-mem-sglang/lib/python3.11/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.
We recommend installing via `pip install torch-c-dlpack-ext`
  warnings.warn(
[2025-12-15 18:17:42] WARNING server_args.py:1406: Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-12-15 18:17:42] Fail to set RLIMIT_NOFILE: current limit exceeds maximum limit
[2025-12-15 18:17:42] server_args=ServerArgs(model_path='/scratch/users/ntu/yzheng05/models/Qwen2.5-7B', tokenizer_path='/scratch/users/ntu/yzheng05/models/Qwen2.5-7B', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', rl_quant_profile=None, trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=30000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.833, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=4096, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=889241230, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, mm_process_config={}, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/scratch/users/ntu/yzheng05/models/Qwen2.5-7B', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='flashinfer', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', enable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_moe_runner_backend=None, speculative_moe_a2a_backend=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=32, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, decrypted_config_file=None, decrypted_draft_config_file=None, mm_enable_dp_encoder=False, forward_hooks=None)
[2025-12-15 18:17:42] Using default HuggingFace chat template with detected content format: string
/home/users/ntu/yzheng05/.conda/envs/A-mem-sglang/lib/python3.11/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.
We recommend installing via `pip install torch-c-dlpack-ext`
  warnings.warn(
[2025-12-15 18:17:55] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2025-12-15 18:17:55] Init torch distributed ends. mem usage=0.00 GB
[2025-12-15 18:17:55] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/home/users/ntu/yzheng05/.conda/envs/A-mem-sglang/lib/python3.11/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.
We recommend installing via `pip install torch-c-dlpack-ext`
  warnings.warn(
[2025-12-15 18:17:56] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-12-15 18:17:56] Load weight begin. avail mem=39.14 GB
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.02s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.05s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.02s/it]

[2025-12-15 18:18:00] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=24.77 GB, mem usage=14.37 GB.
[2025-12-15 18:18:00] Using KV cache dtype: torch.bfloat16
[2025-12-15 18:18:00] KV Cache is allocated. #tokens: 341386, K size: 9.12 GB, V size: 9.12 GB
[2025-12-15 18:18:00] Memory pool end. avail mem=5.96 GB
[2025-12-15 18:18:00] Capture cuda graph begin. This can take up to several minutes. avail mem=5.35 GB
[2025-12-15 18:18:00] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32]
  0%|          | 0/8 [00:00<?, ?it/s]Capturing batches (bs=32 avail_mem=5.32 GB):   0%|          | 0/8 [00:00<?, ?it/s]Capturing batches (bs=32 avail_mem=5.32 GB):  12%|█▎        | 1/8 [00:01<00:07,  1.07s/it]Capturing batches (bs=24 avail_mem=5.27 GB):  12%|█▎        | 1/8 [00:01<00:07,  1.07s/it]Capturing batches (bs=16 avail_mem=5.25 GB):  12%|█▎        | 1/8 [00:01<00:07,  1.07s/it]Capturing batches (bs=16 avail_mem=5.25 GB):  38%|███▊      | 3/8 [00:01<00:01,  3.08it/s]Capturing batches (bs=12 avail_mem=5.24 GB):  38%|███▊      | 3/8 [00:01<00:01,  3.08it/s]Capturing batches (bs=8 avail_mem=5.22 GB):  38%|███▊      | 3/8 [00:01<00:01,  3.08it/s] Capturing batches (bs=4 avail_mem=5.21 GB):  38%|███▊      | 3/8 [00:01<00:01,  3.08it/s]Capturing batches (bs=4 avail_mem=5.21 GB):  75%|███████▌  | 6/8 [00:01<00:00,  6.34it/s]Capturing batches (bs=2 avail_mem=5.19 GB):  75%|███████▌  | 6/8 [00:01<00:00,  6.34it/s]Capturing batches (bs=1 avail_mem=5.18 GB):  75%|███████▌  | 6/8 [00:01<00:00,  6.34it/s]Capturing batches (bs=1 avail_mem=5.18 GB): 100%|██████████| 8/8 [00:01<00:00,  5.51it/s]
[2025-12-15 18:18:02] Capture cuda graph end. Time elapsed: 1.91 s. mem usage=0.19 GB. avail mem=5.16 GB.
[2025-12-15 18:18:02] max_total_num_tokens=341386, chunked_prefill_size=4096, max_prefill_tokens=16384, max_running_requests=4096, context_len=32768, available_gpu_mem=5.16 GB
[2025-12-15 18:18:03] INFO:     Started server process [3909647]
[2025-12-15 18:18:03] INFO:     Waiting for application startup.
[2025-12-15 18:18:03] Using default chat sampling params from model generation config: {'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[2025-12-15 18:18:03] Using default chat sampling params from model generation config: {'repetition_penalty': 1.05, 'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[2025-12-15 18:18:03] INFO:     Application startup complete.
[2025-12-15 18:18:03] INFO:     Uvicorn running on http://0.0.0.0:30000 (Press CTRL+C to quit)
[2025-12-15 18:18:04] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2025-12-15 18:18:04] INFO:     127.0.0.1:42782 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-12-15 18:18:04] Prefill batch, #new-seq: 1, #new-token: 6, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:18:05] INFO:     127.0.0.1:42784 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:18:05] The server is fired up and ready to roll!
[2025-12-15 18:20:54] Prefill batch, #new-seq: 1, #new-token: 229, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:21:03] Decode batch, #running-req: 1, #token: 262, token usage: 0.00, cuda graph: True, gen throughput (token/s): 0.22, #queue-req: 0, 
[2025-12-15 18:21:04] INFO:     127.0.0.1:42820 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:21:04] Prefill batch, #new-seq: 1, #new-token: 434, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:21:04] Decode batch, #running-req: 1, #token: 457, token usage: 0.00, cuda graph: True, gen throughput (token/s): 71.46, #queue-req: 0, 
[2025-12-15 18:21:04] INFO:     127.0.0.1:43338 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:21:11] Prefill batch, #new-seq: 1, #new-token: 32, #cached-token: 212, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:21:18] Decode batch, #running-req: 1, #token: 260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2.93, #queue-req: 0, 
[2025-12-15 18:21:18] Decode batch, #running-req: 1, #token: 300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:21:18] INFO:     127.0.0.1:43342 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:21:19] Prefill batch, #new-seq: 1, #new-token: 498, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:21:19] Decode batch, #running-req: 1, #token: 569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 56.52, #queue-req: 0, 
[2025-12-15 18:21:19] INFO:     127.0.0.1:43346 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:21:19] Prefill batch, #new-seq: 1, #new-token: 14, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:21:19] Decode batch, #running-req: 1, #token: 246, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.29, #queue-req: 0, 
[2025-12-15 18:21:20] INFO:     127.0.0.1:43350 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:21:20] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.85, #queue-req: 0, 
[2025-12-15 18:21:20] Prefill batch, #new-seq: 1, #new-token: 594, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:21:21] INFO:     127.0.0.1:43354 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:21:21] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 54.36, #queue-req: 0, 
[2025-12-15 18:21:21] Prefill batch, #new-seq: 1, #new-token: 22, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:21:21] Decode batch, #running-req: 1, #token: 280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.64, #queue-req: 0, 
[2025-12-15 18:21:22] Decode batch, #running-req: 1, #token: 320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.08, #queue-req: 0, 
[2025-12-15 18:21:22] Decode batch, #running-req: 1, #token: 360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.00, #queue-req: 0, 
[2025-12-15 18:21:23] Decode batch, #running-req: 1, #token: 400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.94, #queue-req: 0, 
[2025-12-15 18:21:23] Decode batch, #running-req: 1, #token: 440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.89, #queue-req: 0, 
[2025-12-15 18:21:24] Decode batch, #running-req: 1, #token: 480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.89, #queue-req: 0, 
[2025-12-15 18:21:24] Decode batch, #running-req: 1, #token: 520, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.87, #queue-req: 0, 
[2025-12-15 18:21:25] Decode batch, #running-req: 1, #token: 560, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.84, #queue-req: 0, 
[2025-12-15 18:21:25] Decode batch, #running-req: 1, #token: 600, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.85, #queue-req: 0, 
[2025-12-15 18:21:26] Decode batch, #running-req: 1, #token: 640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.85, #queue-req: 0, 
[2025-12-15 18:21:26] Decode batch, #running-req: 1, #token: 680, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.81, #queue-req: 0, 
[2025-12-15 18:21:27] Decode batch, #running-req: 1, #token: 720, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.82, #queue-req: 0, 
[2025-12-15 18:21:27] Decode batch, #running-req: 1, #token: 760, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.81, #queue-req: 0, 
[2025-12-15 18:21:28] Decode batch, #running-req: 1, #token: 800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.81, #queue-req: 0, 
[2025-12-15 18:21:28] Decode batch, #running-req: 1, #token: 840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.80, #queue-req: 0, 
[2025-12-15 18:21:29] Decode batch, #running-req: 1, #token: 880, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.81, #queue-req: 0, 
[2025-12-15 18:21:29] Decode batch, #running-req: 1, #token: 920, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.80, #queue-req: 0, 
[2025-12-15 18:21:30] Decode batch, #running-req: 1, #token: 960, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.81, #queue-req: 0, 
[2025-12-15 18:21:30] Decode batch, #running-req: 1, #token: 1000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.79, #queue-req: 0, 
[2025-12-15 18:21:31] Decode batch, #running-req: 1, #token: 1040, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.76, #queue-req: 0, 
[2025-12-15 18:21:31] Decode batch, #running-req: 1, #token: 1080, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.74, #queue-req: 0, 
[2025-12-15 18:21:32] Decode batch, #running-req: 1, #token: 1120, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.71, #queue-req: 0, 
[2025-12-15 18:21:32] Decode batch, #running-req: 1, #token: 1160, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.70, #queue-req: 0, 
[2025-12-15 18:21:33] Decode batch, #running-req: 1, #token: 1200, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.69, #queue-req: 0, 
[2025-12-15 18:21:33] INFO:     127.0.0.1:43358 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:21:33] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.52, #queue-req: 0, 
[2025-12-15 18:21:33] Prefill batch, #new-seq: 1, #new-token: 681, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:21:34] INFO:     127.0.0.1:43368 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:21:34] Prefill batch, #new-seq: 1, #new-token: 37, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:21:34] Decode batch, #running-req: 1, #token: 255, token usage: 0.00, cuda graph: True, gen throughput (token/s): 51.70, #queue-req: 0, 
[2025-12-15 18:21:34] Decode batch, #running-req: 1, #token: 295, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:21:35] Decode batch, #running-req: 1, #token: 335, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:21:35] INFO:     127.0.0.1:43372 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:21:35] Prefill batch, #new-seq: 1, #new-token: 821, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:21:35] Decode batch, #running-req: 1, #token: 915, token usage: 0.00, cuda graph: True, gen throughput (token/s): 71.00, #queue-req: 0, 
[2025-12-15 18:21:36] INFO:     127.0.0.1:43376 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:21:36] Prefill batch, #new-seq: 1, #new-token: 20, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:21:36] Decode batch, #running-req: 1, #token: 274, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.49, #queue-req: 0, 
[2025-12-15 18:21:37] Decode batch, #running-req: 1, #token: 314, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.16, #queue-req: 0, 
[2025-12-15 18:21:37] INFO:     127.0.0.1:43380 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:21:37] Prefill batch, #new-seq: 1, #new-token: 948, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:21:37] Decode batch, #running-req: 1, #token: 1031, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.82, #queue-req: 0, 
[2025-12-15 18:21:38] Decode batch, #running-req: 1, #token: 1071, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.73, #queue-req: 0, 
[2025-12-15 18:21:38] Decode batch, #running-req: 1, #token: 1111, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.72, #queue-req: 0, 
[2025-12-15 18:21:39] Decode batch, #running-req: 1, #token: 1151, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.71, #queue-req: 0, 
[2025-12-15 18:21:39] Decode batch, #running-req: 1, #token: 1191, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.68, #queue-req: 0, 
[2025-12-15 18:21:40] Decode batch, #running-req: 1, #token: 1231, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.68, #queue-req: 0, 
[2025-12-15 18:21:40] Decode batch, #running-req: 1, #token: 1271, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.67, #queue-req: 0, 
[2025-12-15 18:21:41] Decode batch, #running-req: 1, #token: 1311, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.68, #queue-req: 0, 
[2025-12-15 18:21:41] Decode batch, #running-req: 1, #token: 1351, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.68, #queue-req: 0, 
[2025-12-15 18:21:42] Decode batch, #running-req: 1, #token: 1391, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.66, #queue-req: 0, 
[2025-12-15 18:21:42] Decode batch, #running-req: 1, #token: 1431, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.66, #queue-req: 0, 
[2025-12-15 18:21:43] Decode batch, #running-req: 1, #token: 1471, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.66, #queue-req: 0, 
[2025-12-15 18:21:43] Decode batch, #running-req: 1, #token: 1511, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.64, #queue-req: 0, 
[2025-12-15 18:21:44] Decode batch, #running-req: 1, #token: 1551, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.62, #queue-req: 0, 
[2025-12-15 18:21:44] Decode batch, #running-req: 1, #token: 1591, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.61, #queue-req: 0, 
[2025-12-15 18:21:45] Decode batch, #running-req: 1, #token: 1631, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.61, #queue-req: 0, 
[2025-12-15 18:21:45] Decode batch, #running-req: 1, #token: 1671, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.58, #queue-req: 0, 
[2025-12-15 18:21:46] Decode batch, #running-req: 1, #token: 1711, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.58, #queue-req: 0, 
[2025-12-15 18:21:46] Decode batch, #running-req: 1, #token: 1751, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:21:47] Decode batch, #running-req: 1, #token: 1791, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.55, #queue-req: 0, 
[2025-12-15 18:21:47] Decode batch, #running-req: 1, #token: 1831, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.58, #queue-req: 0, 
[2025-12-15 18:21:48] Decode batch, #running-req: 1, #token: 1871, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.55, #queue-req: 0, 
[2025-12-15 18:21:48] Decode batch, #running-req: 1, #token: 1911, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.53, #queue-req: 0, 
[2025-12-15 18:21:49] Decode batch, #running-req: 1, #token: 1951, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.53, #queue-req: 0, 
[2025-12-15 18:21:49] Decode batch, #running-req: 1, #token: 1991, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.53, #queue-req: 0, 
[2025-12-15 18:21:49] INFO:     127.0.0.1:43384 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:21:49] Prefill batch, #new-seq: 1, #new-token: 16, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:21:50] Decode batch, #running-req: 1, #token: 259, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.01, #queue-req: 0, 
[2025-12-15 18:21:50] Decode batch, #running-req: 1, #token: 299, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.24, #queue-req: 0, 
[2025-12-15 18:21:51] Decode batch, #running-req: 1, #token: 339, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:21:51] INFO:     127.0.0.1:43388 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:21:51] Prefill batch, #new-seq: 1, #new-token: 1004, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:21:51] Decode batch, #running-req: 1, #token: 1085, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.56, #queue-req: 0, 
[2025-12-15 18:21:51] INFO:     127.0.0.1:43392 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:21:51] Prefill batch, #new-seq: 1, #new-token: 13, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:21:52] Decode batch, #running-req: 1, #token: 253, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.36, #queue-req: 0, 
[2025-12-15 18:21:52] Decode batch, #running-req: 1, #token: 293, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:21:52] INFO:     127.0.0.1:43396 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:21:52] Prefill batch, #new-seq: 1, #new-token: 940, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:21:53] Decode batch, #running-req: 1, #token: 1032, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.80, #queue-req: 0, 
[2025-12-15 18:21:53] INFO:     127.0.0.1:43400 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:21:53] Prefill batch, #new-seq: 1, #new-token: 16, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:21:53] Decode batch, #running-req: 1, #token: 260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.52, #queue-req: 0, 
[2025-12-15 18:21:53] INFO:     127.0.0.1:43404 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:21:54] Prefill batch, #new-seq: 1, #new-token: 880, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:21:54] Decode batch, #running-req: 1, #token: 965, token usage: 0.00, cuda graph: True, gen throughput (token/s): 70.93, #queue-req: 0, 
[2025-12-15 18:21:54] INFO:     127.0.0.1:43408 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:21:54] Prefill batch, #new-seq: 1, #new-token: 17, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:21:54] Decode batch, #running-req: 1, #token: 258, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.48, #queue-req: 0, 
[2025-12-15 18:21:55] Decode batch, #running-req: 1, #token: 298, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.21, #queue-req: 0, 
[2025-12-15 18:21:55] INFO:     127.0.0.1:43412 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:21:55] Prefill batch, #new-seq: 1, #new-token: 908, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:21:56] Decode batch, #running-req: 1, #token: 970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 66.03, #queue-req: 0, 
[2025-12-15 18:21:56] Decode batch, #running-req: 1, #token: 1010, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.79, #queue-req: 0, 
[2025-12-15 18:21:56] INFO:     127.0.0.1:43416 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:21:56] Prefill batch, #new-seq: 1, #new-token: 20, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:21:57] Decode batch, #running-req: 1, #token: 272, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.54, #queue-req: 0, 
[2025-12-15 18:21:57] Decode batch, #running-req: 1, #token: 312, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:21:57] INFO:     127.0.0.1:43420 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:21:57] Prefill batch, #new-seq: 1, #new-token: 1064, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:21:58] Decode batch, #running-req: 1, #token: 1156, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.67, #queue-req: 0, 
[2025-12-15 18:21:58] INFO:     127.0.0.1:43424 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:21:58] Prefill batch, #new-seq: 1, #new-token: 45, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:21:58] Decode batch, #running-req: 1, #token: 301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.44, #queue-req: 0, 
[2025-12-15 18:21:59] Decode batch, #running-req: 1, #token: 341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.17, #queue-req: 0, 
[2025-12-15 18:21:59] INFO:     127.0.0.1:43428 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:21:59] Prefill batch, #new-seq: 1, #new-token: 1047, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:21:59] Decode batch, #running-req: 1, #token: 1139, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.69, #queue-req: 0, 
[2025-12-15 18:21:59] INFO:     127.0.0.1:43432 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:21:59] Prefill batch, #new-seq: 1, #new-token: 15, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:00] Decode batch, #running-req: 1, #token: 265, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.51, #queue-req: 0, 
[2025-12-15 18:22:00] INFO:     127.0.0.1:43436 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:00] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.94, #queue-req: 0, 
[2025-12-15 18:22:00] Prefill batch, #new-seq: 1, #new-token: 951, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:01] INFO:     127.0.0.1:43440 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:01] Prefill batch, #new-seq: 1, #new-token: 16, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:01] Decode batch, #running-req: 1, #token: 239, token usage: 0.00, cuda graph: True, gen throughput (token/s): 67.20, #queue-req: 0, 
[2025-12-15 18:22:01] INFO:     127.0.0.1:43444 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:01] Prefill batch, #new-seq: 1, #new-token: 897, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:01] Decode batch, #running-req: 1, #token: 963, token usage: 0.00, cuda graph: True, gen throughput (token/s): 70.10, #queue-req: 0, 
[2025-12-15 18:22:02] INFO:     127.0.0.1:43448 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:02] Prefill batch, #new-seq: 1, #new-token: 20, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:02] Decode batch, #running-req: 1, #token: 245, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.28, #queue-req: 0, 
[2025-12-15 18:22:02] Decode batch, #running-req: 1, #token: 285, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.28, #queue-req: 0, 
[2025-12-15 18:22:03] INFO:     127.0.0.1:43452 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:03] Prefill batch, #new-seq: 1, #new-token: 873, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:03] Decode batch, #running-req: 1, #token: 937, token usage: 0.00, cuda graph: True, gen throughput (token/s): 71.14, #queue-req: 0, 
[2025-12-15 18:22:04] INFO:     127.0.0.1:43456 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:04] Prefill batch, #new-seq: 1, #new-token: 29, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:04] Decode batch, #running-req: 1, #token: 248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.17, #queue-req: 0, 
[2025-12-15 18:22:04] Decode batch, #running-req: 1, #token: 288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.25, #queue-req: 0, 
[2025-12-15 18:22:04] INFO:     127.0.0.1:43460 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:04] Prefill batch, #new-seq: 1, #new-token: 891, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:05] Decode batch, #running-req: 1, #token: 975, token usage: 0.00, cuda graph: True, gen throughput (token/s): 70.97, #queue-req: 0, 
[2025-12-15 18:22:05] INFO:     127.0.0.1:43464 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:05] Prefill batch, #new-seq: 1, #new-token: 24, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:05] Decode batch, #running-req: 1, #token: 268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.38, #queue-req: 0, 
[2025-12-15 18:22:05] INFO:     127.0.0.1:43468 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:05] Prefill batch, #new-seq: 1, #new-token: 889, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:06] Decode batch, #running-req: 1, #token: 970, token usage: 0.00, cuda graph: True, gen throughput (token/s): 70.09, #queue-req: 0, 
[2025-12-15 18:22:06] INFO:     127.0.0.1:43472 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:06] Prefill batch, #new-seq: 1, #new-token: 26, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:06] Decode batch, #running-req: 1, #token: 267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.45, #queue-req: 0, 
[2025-12-15 18:22:07] Decode batch, #running-req: 1, #token: 307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.17, #queue-req: 0, 
[2025-12-15 18:22:07] INFO:     127.0.0.1:43476 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:07] Prefill batch, #new-seq: 1, #new-token: 943, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:07] Decode batch, #running-req: 1, #token: 1022, token usage: 0.00, cuda graph: True, gen throughput (token/s): 65.46, #queue-req: 0, 
[2025-12-15 18:22:08] INFO:     127.0.0.1:43480 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:08] Prefill batch, #new-seq: 1, #new-token: 45, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:08] Decode batch, #running-req: 1, #token: 279, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.18, #queue-req: 0, 
[2025-12-15 18:22:08] Decode batch, #running-req: 1, #token: 319, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:22:08] INFO:     127.0.0.1:43484 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:09] Prefill batch, #new-seq: 1, #new-token: 974, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:09] Decode batch, #running-req: 1, #token: 1066, token usage: 0.00, cuda graph: True, gen throughput (token/s): 56.58, #queue-req: 0, 
[2025-12-15 18:22:09] INFO:     127.0.0.1:43488 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:09] Prefill batch, #new-seq: 1, #new-token: 31, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:10] Decode batch, #running-req: 1, #token: 286, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.49, #queue-req: 0, 
[2025-12-15 18:22:10] Decode batch, #running-req: 1, #token: 326, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:22:10] INFO:     127.0.0.1:43492 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:10] Prefill batch, #new-seq: 1, #new-token: 1067, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:11] Decode batch, #running-req: 1, #token: 1161, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.69, #queue-req: 0, 
[2025-12-15 18:22:11] INFO:     127.0.0.1:43496 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:11] Prefill batch, #new-seq: 1, #new-token: 45, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:11] Decode batch, #running-req: 1, #token: 301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.42, #queue-req: 0, 
[2025-12-15 18:22:12] INFO:     127.0.0.1:43500 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:12] Prefill batch, #new-seq: 1, #new-token: 1050, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:12] Decode batch, #running-req: 1, #token: 1118, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.91, #queue-req: 0, 
[2025-12-15 18:22:12] INFO:     127.0.0.1:43504 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:12] Prefill batch, #new-seq: 1, #new-token: 30, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:12] Decode batch, #running-req: 1, #token: 252, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.15, #queue-req: 0, 
[2025-12-15 18:22:13] Decode batch, #running-req: 1, #token: 292, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:22:13] Decode batch, #running-req: 1, #token: 332, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.21, #queue-req: 0, 
[2025-12-15 18:22:14] INFO:     127.0.0.1:43510 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:14] Prefill batch, #new-seq: 1, #new-token: 1075, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:14] Decode batch, #running-req: 1, #token: 1140, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.82, #queue-req: 0, 
[2025-12-15 18:22:14] INFO:     127.0.0.1:43514 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:14] Prefill batch, #new-seq: 1, #new-token: 37, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:14] Decode batch, #running-req: 1, #token: 264, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.06, #queue-req: 0, 
[2025-12-15 18:22:15] Decode batch, #running-req: 1, #token: 304, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:22:15] INFO:     127.0.0.1:43518 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:15] Prefill batch, #new-seq: 1, #new-token: 1018, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:15] Decode batch, #running-req: 1, #token: 1107, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.69, #queue-req: 0, 
[2025-12-15 18:22:16] INFO:     127.0.0.1:43522 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:16] Prefill batch, #new-seq: 1, #new-token: 26, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:16] Decode batch, #running-req: 1, #token: 271, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.39, #queue-req: 0, 
[2025-12-15 18:22:17] Decode batch, #running-req: 1, #token: 311, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.21, #queue-req: 0, 
[2025-12-15 18:22:17] Decode batch, #running-req: 1, #token: 351, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.12, #queue-req: 0, 
[2025-12-15 18:22:17] INFO:     127.0.0.1:43526 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:17] Prefill batch, #new-seq: 1, #new-token: 1128, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:18] Decode batch, #running-req: 1, #token: 1212, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.57, #queue-req: 0, 
[2025-12-15 18:22:18] INFO:     127.0.0.1:43530 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:18] Prefill batch, #new-seq: 1, #new-token: 41, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:18] Decode batch, #running-req: 1, #token: 290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.25, #queue-req: 0, 
[2025-12-15 18:22:19] Decode batch, #running-req: 1, #token: 330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.21, #queue-req: 0, 
[2025-12-15 18:22:19] Decode batch, #running-req: 1, #token: 370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.07, #queue-req: 0, 
[2025-12-15 18:22:19] INFO:     127.0.0.1:43534 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:19] Prefill batch, #new-seq: 1, #new-token: 1014, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:20] Decode batch, #running-req: 1, #token: 1108, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.61, #queue-req: 0, 
[2025-12-15 18:22:20] INFO:     127.0.0.1:43542 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:20] Prefill batch, #new-seq: 1, #new-token: 25, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:20] Decode batch, #running-req: 1, #token: 275, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.47, #queue-req: 0, 
[2025-12-15 18:22:21] Decode batch, #running-req: 1, #token: 315, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.23, #queue-req: 0, 
[2025-12-15 18:22:21] INFO:     127.0.0.1:43546 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:21] Prefill batch, #new-seq: 1, #new-token: 1130, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:21] Decode batch, #running-req: 1, #token: 1204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.72, #queue-req: 0, 
[2025-12-15 18:22:22] INFO:     127.0.0.1:43550 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:22] Prefill batch, #new-seq: 1, #new-token: 27, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:22] Decode batch, #running-req: 1, #token: 264, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.25, #queue-req: 0, 
[2025-12-15 18:22:22] Decode batch, #running-req: 1, #token: 304, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:22:23] INFO:     127.0.0.1:43554 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:23] Prefill batch, #new-seq: 1, #new-token: 1028, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:23] Decode batch, #running-req: 1, #token: 1097, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.96, #queue-req: 0, 
[2025-12-15 18:22:23] INFO:     127.0.0.1:43558 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:23] Prefill batch, #new-seq: 1, #new-token: 87, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:23] Decode batch, #running-req: 1, #token: 321, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.03, #queue-req: 0, 
[2025-12-15 18:22:24] Decode batch, #running-req: 1, #token: 361, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.08, #queue-req: 0, 
[2025-12-15 18:22:24] Decode batch, #running-req: 1, #token: 401, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.04, #queue-req: 0, 
[2025-12-15 18:22:25] INFO:     127.0.0.1:43562 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:25] Prefill batch, #new-seq: 1, #new-token: 1202, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:25] Decode batch, #running-req: 1, #token: 1283, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.13, #queue-req: 0, 
[2025-12-15 18:22:25] INFO:     127.0.0.1:43566 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:25] Prefill batch, #new-seq: 1, #new-token: 10, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:26] Decode batch, #running-req: 1, #token: 258, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.39, #queue-req: 0, 
[2025-12-15 18:22:26] Decode batch, #running-req: 1, #token: 298, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.26, #queue-req: 0, 
[2025-12-15 18:22:26] INFO:     127.0.0.1:43570 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:26] Prefill batch, #new-seq: 1, #new-token: 1071, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:27] Decode batch, #running-req: 1, #token: 1138, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.90, #queue-req: 0, 
[2025-12-15 18:22:27] INFO:     127.0.0.1:43574 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:27] Prefill batch, #new-seq: 1, #new-token: 22, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:27] Decode batch, #running-req: 1, #token: 242, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.08, #queue-req: 0, 
[2025-12-15 18:22:28] Decode batch, #running-req: 1, #token: 282, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.32, #queue-req: 0, 
[2025-12-15 18:22:28] Decode batch, #running-req: 1, #token: 322, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:22:28] INFO:     127.0.0.1:43578 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:28] Prefill batch, #new-seq: 1, #new-token: 1182, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:29] Decode batch, #running-req: 1, #token: 1270, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.32, #queue-req: 0, 
[2025-12-15 18:22:29] INFO:     127.0.0.1:43582 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:29] Prefill batch, #new-seq: 1, #new-token: 21, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:29] Decode batch, #running-req: 1, #token: 264, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.34, #queue-req: 0, 
[2025-12-15 18:22:30] INFO:     127.0.0.1:43586 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:30] Prefill batch, #new-seq: 1, #new-token: 1083, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:30] Decode batch, #running-req: 1, #token: 1146, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.91, #queue-req: 0, 
[2025-12-15 18:22:30] INFO:     127.0.0.1:43590 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:30] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.48, #queue-req: 0, 
[2025-12-15 18:22:30] Prefill batch, #new-seq: 1, #new-token: 28, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:31] Decode batch, #running-req: 1, #token: 287, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.73, #queue-req: 0, 
[2025-12-15 18:22:31] Decode batch, #running-req: 1, #token: 327, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:22:32] INFO:     127.0.0.1:43594 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:32] Prefill batch, #new-seq: 1, #new-token: 1168, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:32] Decode batch, #running-req: 1, #token: 1252, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.34, #queue-req: 0, 
[2025-12-15 18:22:32] INFO:     127.0.0.1:43598 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:32] Prefill batch, #new-seq: 1, #new-token: 26, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:32] Decode batch, #running-req: 1, #token: 266, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.35, #queue-req: 0, 
[2025-12-15 18:22:33] Decode batch, #running-req: 1, #token: 306, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:22:33] INFO:     127.0.0.1:43602 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:33] Prefill batch, #new-seq: 1, #new-token: 1159, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:34] Decode batch, #running-req: 1, #token: 1234, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.40, #queue-req: 0, 
[2025-12-15 18:22:34] INFO:     127.0.0.1:43606 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:34] Prefill batch, #new-seq: 1, #new-token: 25, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:34] Decode batch, #running-req: 1, #token: 263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.25, #queue-req: 0, 
[2025-12-15 18:22:35] Decode batch, #running-req: 1, #token: 303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:22:35] INFO:     127.0.0.1:43610 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:35] Prefill batch, #new-seq: 1, #new-token: 1097, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:35] Decode batch, #running-req: 1, #token: 1157, token usage: 0.00, cuda graph: True, gen throughput (token/s): 51.40, #queue-req: 0, 
[2025-12-15 18:22:36] INFO:     127.0.0.1:43614 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:36] Prefill batch, #new-seq: 1, #new-token: 30, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:36] Decode batch, #running-req: 1, #token: 253, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.03, #queue-req: 0, 
[2025-12-15 18:22:36] Decode batch, #running-req: 1, #token: 293, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:22:37] Decode batch, #running-req: 1, #token: 333, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:22:37] INFO:     127.0.0.1:43618 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:37] Prefill batch, #new-seq: 1, #new-token: 1111, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:37] Decode batch, #running-req: 1, #token: 1191, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.67, #queue-req: 0, 
[2025-12-15 18:22:38] INFO:     127.0.0.1:43622 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:38] Prefill batch, #new-seq: 1, #new-token: 66, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:38] INFO:     127.0.0.1:43626 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:38] Prefill batch, #new-seq: 1, #new-token: 948, #cached-token: 61, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:38] Decode batch, #running-req: 1, #token: 1020, token usage: 0.00, cuda graph: True, gen throughput (token/s): 67.00, #queue-req: 0, 
[2025-12-15 18:22:38] INFO:     127.0.0.1:43630 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:38] Prefill batch, #new-seq: 1, #new-token: 51, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:39] Decode batch, #running-req: 1, #token: 284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.22, #queue-req: 0, 
[2025-12-15 18:22:39] Decode batch, #running-req: 1, #token: 324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.17, #queue-req: 0, 
[2025-12-15 18:22:39] INFO:     127.0.0.1:43634 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:39] Prefill batch, #new-seq: 1, #new-token: 1018, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:40] Decode batch, #running-req: 1, #token: 1092, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.72, #queue-req: 0, 
[2025-12-15 18:22:40] INFO:     127.0.0.1:43638 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:40] Prefill batch, #new-seq: 1, #new-token: 86, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:40] Decode batch, #running-req: 1, #token: 323, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.18, #queue-req: 0, 
[2025-12-15 18:22:41] Decode batch, #running-req: 1, #token: 363, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.10, #queue-req: 0, 
[2025-12-15 18:22:41] Decode batch, #running-req: 1, #token: 403, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.04, #queue-req: 0, 
[2025-12-15 18:22:41] INFO:     127.0.0.1:43642 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:41] Prefill batch, #new-seq: 1, #new-token: 1080, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:42] Decode batch, #running-req: 1, #token: 1162, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.61, #queue-req: 0, 
[2025-12-15 18:22:42] INFO:     127.0.0.1:43646 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:42] Prefill batch, #new-seq: 1, #new-token: 60, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:42] Decode batch, #running-req: 1, #token: 311, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.24, #queue-req: 0, 
[2025-12-15 18:22:43] Decode batch, #running-req: 1, #token: 351, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.14, #queue-req: 0, 
[2025-12-15 18:22:43] INFO:     127.0.0.1:43650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:43] Prefill batch, #new-seq: 1, #new-token: 1225, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:43] Decode batch, #running-req: 1, #token: 1295, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.24, #queue-req: 0, 
[2025-12-15 18:22:44] INFO:     127.0.0.1:43654 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:44] Prefill batch, #new-seq: 1, #new-token: 70, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:44] Decode batch, #running-req: 1, #token: 297, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.05, #queue-req: 0, 
[2025-12-15 18:22:44] Decode batch, #running-req: 1, #token: 337, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.16, #queue-req: 0, 
[2025-12-15 18:22:45] Decode batch, #running-req: 1, #token: 377, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.10, #queue-req: 0, 
[2025-12-15 18:22:45] INFO:     127.0.0.1:43660 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:45] Prefill batch, #new-seq: 1, #new-token: 1325, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:45] Decode batch, #running-req: 1, #token: 1415, token usage: 0.00, cuda graph: True, gen throughput (token/s): 67.26, #queue-req: 0, 
[2025-12-15 18:22:46] INFO:     127.0.0.1:43664 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:46] Prefill batch, #new-seq: 1, #new-token: 86, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:46] Decode batch, #running-req: 1, #token: 338, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.26, #queue-req: 0, 
[2025-12-15 18:22:46] Decode batch, #running-req: 1, #token: 378, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.08, #queue-req: 0, 
[2025-12-15 18:22:47] INFO:     127.0.0.1:43668 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:47] Prefill batch, #new-seq: 1, #new-token: 1287, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:47] Decode batch, #running-req: 1, #token: 1360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 67.50, #queue-req: 0, 
[2025-12-15 18:22:47] INFO:     127.0.0.1:43672 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:47] Prefill batch, #new-seq: 1, #new-token: 67, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:48] Decode batch, #running-req: 1, #token: 302, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.06, #queue-req: 0, 
[2025-12-15 18:22:48] Decode batch, #running-req: 1, #token: 342, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.14, #queue-req: 0, 
[2025-12-15 18:22:49] Decode batch, #running-req: 1, #token: 382, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.06, #queue-req: 0, 
[2025-12-15 18:22:49] INFO:     127.0.0.1:43676 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:49] Prefill batch, #new-seq: 1, #new-token: 1408, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:49] Decode batch, #running-req: 1, #token: 1491, token usage: 0.00, cuda graph: True, gen throughput (token/s): 67.14, #queue-req: 0, 
[2025-12-15 18:22:49] INFO:     127.0.0.1:43680 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:49] Prefill batch, #new-seq: 1, #new-token: 45, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:50] Decode batch, #running-req: 1, #token: 290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.15, #queue-req: 0, 
[2025-12-15 18:22:50] Decode batch, #running-req: 1, #token: 330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:22:51] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.03, #queue-req: 0, 
[2025-12-15 18:22:51] INFO:     127.0.0.1:43684 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:51] Prefill batch, #new-seq: 1, #new-token: 1351, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:51] Decode batch, #running-req: 1, #token: 1451, token usage: 0.00, cuda graph: True, gen throughput (token/s): 67.22, #queue-req: 0, 
[2025-12-15 18:22:51] INFO:     127.0.0.1:43688 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:51] Prefill batch, #new-seq: 1, #new-token: 29, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:52] Decode batch, #running-req: 1, #token: 278, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.44, #queue-req: 0, 
[2025-12-15 18:22:52] Decode batch, #running-req: 1, #token: 318, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:22:53] INFO:     127.0.0.1:43692 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:53] Prefill batch, #new-seq: 1, #new-token: 1329, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:53] Decode batch, #running-req: 1, #token: 1402, token usage: 0.00, cuda graph: True, gen throughput (token/s): 67.42, #queue-req: 0, 
[2025-12-15 18:22:53] Decode batch, #running-req: 1, #token: 1442, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.75, #queue-req: 0, 
[2025-12-15 18:22:54] Decode batch, #running-req: 1, #token: 1482, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.80, #queue-req: 0, 
[2025-12-15 18:22:54] Decode batch, #running-req: 1, #token: 1522, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.77, #queue-req: 0, 
[2025-12-15 18:22:55] Decode batch, #running-req: 1, #token: 1562, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.76, #queue-req: 0, 
[2025-12-15 18:22:55] Decode batch, #running-req: 1, #token: 1602, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.75, #queue-req: 0, 
[2025-12-15 18:22:56] Decode batch, #running-req: 1, #token: 1642, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.71, #queue-req: 0, 
[2025-12-15 18:22:56] Decode batch, #running-req: 1, #token: 1682, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.70, #queue-req: 0, 
[2025-12-15 18:22:57] Decode batch, #running-req: 1, #token: 1722, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.65, #queue-req: 0, 
[2025-12-15 18:22:57] Decode batch, #running-req: 1, #token: 1762, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.67, #queue-req: 0, 
[2025-12-15 18:22:57] INFO:     127.0.0.1:43696 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:58] Prefill batch, #new-seq: 1, #new-token: 38, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:58] Decode batch, #running-req: 1, #token: 293, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.37, #queue-req: 0, 
[2025-12-15 18:22:58] Decode batch, #running-req: 1, #token: 333, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:22:59] INFO:     127.0.0.1:43700 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:22:59] Prefill batch, #new-seq: 1, #new-token: 1251, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:22:59] Decode batch, #running-req: 1, #token: 1343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.12, #queue-req: 0, 
[2025-12-15 18:23:00] Decode batch, #running-req: 1, #token: 1383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.76, #queue-req: 0, 
[2025-12-15 18:23:00] INFO:     127.0.0.1:43704 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:00] Prefill batch, #new-seq: 1, #new-token: 47, #cached-token: 223, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:00] Decode batch, #running-req: 1, #token: 273, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.01, #queue-req: 0, 
[2025-12-15 18:23:01] Decode batch, #running-req: 1, #token: 313, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.17, #queue-req: 0, 
[2025-12-15 18:23:01] Decode batch, #running-req: 1, #token: 353, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.13, #queue-req: 0, 
[2025-12-15 18:23:01] INFO:     127.0.0.1:43708 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:01] Prefill batch, #new-seq: 1, #new-token: 1165, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:02] Decode batch, #running-req: 1, #token: 1252, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.33, #queue-req: 0, 
[2025-12-15 18:23:02] INFO:     127.0.0.1:43712 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:02] Prefill batch, #new-seq: 1, #new-token: 15, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:02] Decode batch, #running-req: 1, #token: 263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.34, #queue-req: 0, 
[2025-12-15 18:23:03] Decode batch, #running-req: 1, #token: 303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:23:03] INFO:     127.0.0.1:43716 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:03] Prefill batch, #new-seq: 1, #new-token: 968, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:03] Decode batch, #running-req: 1, #token: 1053, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.80, #queue-req: 0, 
[2025-12-15 18:23:04] INFO:     127.0.0.1:43720 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:04] Prefill batch, #new-seq: 1, #new-token: 59, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:04] Decode batch, #running-req: 1, #token: 294, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.16, #queue-req: 0, 
[2025-12-15 18:23:04] Decode batch, #running-req: 1, #token: 334, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:23:05] INFO:     127.0.0.1:43724 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:05] Prefill batch, #new-seq: 1, #new-token: 1251, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:05] Decode batch, #running-req: 1, #token: 1315, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.30, #queue-req: 0, 
[2025-12-15 18:23:05] INFO:     127.0.0.1:43728 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:05] Prefill batch, #new-seq: 1, #new-token: 27, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:05] Decode batch, #running-req: 1, #token: 258, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.06, #queue-req: 0, 
[2025-12-15 18:23:06] Decode batch, #running-req: 1, #token: 298, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:23:06] INFO:     127.0.0.1:43732 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:06] Prefill batch, #new-seq: 1, #new-token: 1188, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:06] Decode batch, #running-req: 1, #token: 1258, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.47, #queue-req: 0, 
[2025-12-15 18:23:07] INFO:     127.0.0.1:43738 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:07] Prefill batch, #new-seq: 1, #new-token: 13, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:07] Decode batch, #running-req: 1, #token: 236, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.02, #queue-req: 0, 
[2025-12-15 18:23:08] Decode batch, #running-req: 1, #token: 276, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.35, #queue-req: 0, 
[2025-12-15 18:23:08] INFO:     127.0.0.1:43742 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:08] Prefill batch, #new-seq: 1, #new-token: 902, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:08] Decode batch, #running-req: 1, #token: 974, token usage: 0.00, cuda graph: True, gen throughput (token/s): 70.01, #queue-req: 0, 
[2025-12-15 18:23:08] INFO:     127.0.0.1:43746 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:08] Prefill batch, #new-seq: 1, #new-token: 31, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:09] Decode batch, #running-req: 1, #token: 270, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.27, #queue-req: 0, 
[2025-12-15 18:23:09] Decode batch, #running-req: 1, #token: 310, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:23:10] Decode batch, #running-req: 1, #token: 350, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.13, #queue-req: 0, 
[2025-12-15 18:23:10] INFO:     127.0.0.1:43750 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:10] Prefill batch, #new-seq: 1, #new-token: 935, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:10] Decode batch, #running-req: 1, #token: 1029, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.80, #queue-req: 0, 
[2025-12-15 18:23:10] INFO:     127.0.0.1:43754 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:10] Prefill batch, #new-seq: 1, #new-token: 23, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:11] Decode batch, #running-req: 1, #token: 273, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.47, #queue-req: 0, 
[2025-12-15 18:23:11] Decode batch, #running-req: 1, #token: 313, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:23:12] INFO:     127.0.0.1:43758 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:12] Prefill batch, #new-seq: 1, #new-token: 1000, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:12] Decode batch, #running-req: 1, #token: 1073, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.82, #queue-req: 0, 
[2025-12-15 18:23:12] INFO:     127.0.0.1:43764 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:12] Prefill batch, #new-seq: 1, #new-token: 35, #cached-token: 226, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:12] Decode batch, #running-req: 1, #token: 274, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.16, #queue-req: 0, 
[2025-12-15 18:23:13] Decode batch, #running-req: 1, #token: 314, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:23:13] Decode batch, #running-req: 1, #token: 354, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.11, #queue-req: 0, 
[2025-12-15 18:23:13] INFO:     127.0.0.1:43768 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:13] Prefill batch, #new-seq: 1, #new-token: 1084, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:14] Decode batch, #running-req: 1, #token: 1176, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.67, #queue-req: 0, 
[2025-12-15 18:23:14] INFO:     127.0.0.1:43772 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:14] Prefill batch, #new-seq: 1, #new-token: 18, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:14] Decode batch, #running-req: 1, #token: 270, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.49, #queue-req: 0, 
[2025-12-15 18:23:15] INFO:     127.0.0.1:43776 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:15] Prefill batch, #new-seq: 1, #new-token: 944, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:15] Decode batch, #running-req: 1, #token: 1024, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.85, #queue-req: 0, 
[2025-12-15 18:23:15] INFO:     127.0.0.1:43780 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:15] Prefill batch, #new-seq: 1, #new-token: 24, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:16] Decode batch, #running-req: 1, #token: 264, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.43, #queue-req: 0, 
[2025-12-15 18:23:16] INFO:     127.0.0.1:43784 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:16] Prefill batch, #new-seq: 1, #new-token: 1003, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:16] Decode batch, #running-req: 1, #token: 1073, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.84, #queue-req: 0, 
[2025-12-15 18:23:16] INFO:     127.0.0.1:43788 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:16] Prefill batch, #new-seq: 1, #new-token: 22, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:17] Decode batch, #running-req: 1, #token: 254, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.36, #queue-req: 0, 
[2025-12-15 18:23:17] Decode batch, #running-req: 1, #token: 294, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.26, #queue-req: 0, 
[2025-12-15 18:23:18] Decode batch, #running-req: 1, #token: 334, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:23:18] INFO:     127.0.0.1:43792 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:18] Prefill batch, #new-seq: 1, #new-token: 999, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:18] Decode batch, #running-req: 1, #token: 1092, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.65, #queue-req: 0, 
[2025-12-15 18:23:18] INFO:     127.0.0.1:43796 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:18] Prefill batch, #new-seq: 1, #new-token: 20, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:19] Decode batch, #running-req: 1, #token: 272, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.58, #queue-req: 0, 
[2025-12-15 18:23:19] INFO:     127.0.0.1:43800 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:19] Prefill batch, #new-seq: 1, #new-token: 1048, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:19] Decode batch, #running-req: 1, #token: 1112, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.03, #queue-req: 0, 
[2025-12-15 18:23:20] INFO:     127.0.0.1:43804 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:20] Prefill batch, #new-seq: 1, #new-token: 32, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:20] Decode batch, #running-req: 1, #token: 259, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.23, #queue-req: 0, 
[2025-12-15 18:23:20] Decode batch, #running-req: 1, #token: 299, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:23:20] INFO:     127.0.0.1:43808 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:20] Prefill batch, #new-seq: 1, #new-token: 1093, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:21] Decode batch, #running-req: 1, #token: 1183, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.68, #queue-req: 0, 
[2025-12-15 18:23:21] INFO:     127.0.0.1:43812 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:21] Prefill batch, #new-seq: 1, #new-token: 35, #cached-token: 223, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:21] Decode batch, #running-req: 1, #token: 291, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.40, #queue-req: 0, 
[2025-12-15 18:23:22] Decode batch, #running-req: 1, #token: 331, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:23:22] INFO:     127.0.0.1:43816 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:22] Prefill batch, #new-seq: 1, #new-token: 1031, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:22] Decode batch, #running-req: 1, #token: 1100, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.89, #queue-req: 0, 
[2025-12-15 18:23:23] INFO:     127.0.0.1:43820 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:23] Prefill batch, #new-seq: 1, #new-token: 19, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:23] Decode batch, #running-req: 1, #token: 252, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.26, #queue-req: 0, 
[2025-12-15 18:23:24] Decode batch, #running-req: 1, #token: 292, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:23:24] INFO:     127.0.0.1:43824 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:24] Prefill batch, #new-seq: 1, #new-token: 1023, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:24] Decode batch, #running-req: 1, #token: 1104, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.69, #queue-req: 0, 
[2025-12-15 18:23:24] INFO:     127.0.0.1:43828 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:24] Prefill batch, #new-seq: 1, #new-token: 60, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:25] Decode batch, #running-req: 1, #token: 307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.33, #queue-req: 0, 
[2025-12-15 18:23:25] Decode batch, #running-req: 1, #token: 347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.15, #queue-req: 0, 
[2025-12-15 18:23:25] INFO:     127.0.0.1:43832 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:25] Prefill batch, #new-seq: 1, #new-token: 1149, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:26] Decode batch, #running-req: 1, #token: 1239, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.59, #queue-req: 0, 
[2025-12-15 18:23:26] INFO:     127.0.0.1:43836 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:26] Prefill batch, #new-seq: 1, #new-token: 30, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:26] Decode batch, #running-req: 1, #token: 283, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.42, #queue-req: 0, 
[2025-12-15 18:23:27] Decode batch, #running-req: 1, #token: 323, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:23:27] INFO:     127.0.0.1:43840 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:27] Prefill batch, #new-seq: 1, #new-token: 1092, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:27] Decode batch, #running-req: 1, #token: 1181, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.66, #queue-req: 0, 
[2025-12-15 18:23:27] INFO:     127.0.0.1:43846 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:27] Prefill batch, #new-seq: 1, #new-token: 52, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:28] Decode batch, #running-req: 1, #token: 304, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.42, #queue-req: 0, 
[2025-12-15 18:23:28] Decode batch, #running-req: 1, #token: 344, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:23:29] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.99, #queue-req: 0, 
[2025-12-15 18:23:29] INFO:     127.0.0.1:43850 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:29] Prefill batch, #new-seq: 1, #new-token: 1138, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:29] Decode batch, #running-req: 1, #token: 1234, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.53, #queue-req: 0, 
[2025-12-15 18:23:29] INFO:     127.0.0.1:45014 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:30] Prefill batch, #new-seq: 1, #new-token: 43, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:30] Decode batch, #running-req: 1, #token: 294, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.23, #queue-req: 0, 
[2025-12-15 18:23:30] INFO:     127.0.0.1:45018 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:30] Prefill batch, #new-seq: 1, #new-token: 1074, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:31] Decode batch, #running-req: 1, #token: 1160, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.72, #queue-req: 0, 
[2025-12-15 18:23:31] INFO:     127.0.0.1:45022 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:31] Prefill batch, #new-seq: 1, #new-token: 17, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:31] Decode batch, #running-req: 1, #token: 260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.42, #queue-req: 0, 
[2025-12-15 18:23:32] Decode batch, #running-req: 1, #token: 300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.23, #queue-req: 0, 
[2025-12-15 18:23:32] INFO:     127.0.0.1:45026 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:32] Prefill batch, #new-seq: 1, #new-token: 1024, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:32] Decode batch, #running-req: 1, #token: 1089, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.85, #queue-req: 0, 
[2025-12-15 18:23:33] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.68, #queue-req: 0, 
[2025-12-15 18:23:33] INFO:     127.0.0.1:45030 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:33] Prefill batch, #new-seq: 1, #new-token: 60, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:33] Decode batch, #running-req: 1, #token: 318, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.47, #queue-req: 0, 
[2025-12-15 18:23:33] INFO:     127.0.0.1:45034 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:34] Prefill batch, #new-seq: 1, #new-token: 1137, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:34] Decode batch, #running-req: 1, #token: 1208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 61.04, #queue-req: 0, 
[2025-12-15 18:23:34] INFO:     127.0.0.1:45038 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:34] Prefill batch, #new-seq: 1, #new-token: 21, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:34] Decode batch, #running-req: 1, #token: 253, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.23, #queue-req: 0, 
[2025-12-15 18:23:35] INFO:     127.0.0.1:45042 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:35] Prefill batch, #new-seq: 1, #new-token: 967, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:35] Decode batch, #running-req: 1, #token: 1029, token usage: 0.00, cuda graph: True, gen throughput (token/s): 70.02, #queue-req: 0, 
[2025-12-15 18:23:35] Decode batch, #running-req: 1, #token: 1069, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.72, #queue-req: 0, 
[2025-12-15 18:23:35] INFO:     127.0.0.1:45046 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:35] Prefill batch, #new-seq: 1, #new-token: 13, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:36] Decode batch, #running-req: 1, #token: 272, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.58, #queue-req: 0, 
[2025-12-15 18:23:36] Decode batch, #running-req: 1, #token: 312, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.11, #queue-req: 0, 
[2025-12-15 18:23:37] Decode batch, #running-req: 1, #token: 352, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.03, #queue-req: 0, 
[2025-12-15 18:23:37] Decode batch, #running-req: 1, #token: 392, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.96, #queue-req: 0, 
[2025-12-15 18:23:38] Decode batch, #running-req: 1, #token: 432, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.92, #queue-req: 0, 
[2025-12-15 18:23:38] Decode batch, #running-req: 1, #token: 472, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.92, #queue-req: 0, 
[2025-12-15 18:23:39] Decode batch, #running-req: 1, #token: 512, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.91, #queue-req: 0, 
[2025-12-15 18:23:39] Decode batch, #running-req: 1, #token: 552, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.87, #queue-req: 0, 
[2025-12-15 18:23:40] Decode batch, #running-req: 1, #token: 592, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.87, #queue-req: 0, 
[2025-12-15 18:23:40] Decode batch, #running-req: 1, #token: 632, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.88, #queue-req: 0, 
[2025-12-15 18:23:41] Decode batch, #running-req: 1, #token: 672, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.85, #queue-req: 0, 
[2025-12-15 18:23:41] Decode batch, #running-req: 1, #token: 712, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.83, #queue-req: 0, 
[2025-12-15 18:23:42] Decode batch, #running-req: 1, #token: 752, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.84, #queue-req: 0, 
[2025-12-15 18:23:42] Decode batch, #running-req: 1, #token: 792, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.84, #queue-req: 0, 
[2025-12-15 18:23:43] Decode batch, #running-req: 1, #token: 832, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.83, #queue-req: 0, 
[2025-12-15 18:23:43] Decode batch, #running-req: 1, #token: 872, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.82, #queue-req: 0, 
[2025-12-15 18:23:44] Decode batch, #running-req: 1, #token: 912, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.84, #queue-req: 0, 
[2025-12-15 18:23:44] Decode batch, #running-req: 1, #token: 952, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.82, #queue-req: 0, 
[2025-12-15 18:23:45] Decode batch, #running-req: 1, #token: 992, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.82, #queue-req: 0, 
[2025-12-15 18:23:45] Decode batch, #running-req: 1, #token: 1032, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.80, #queue-req: 0, 
[2025-12-15 18:23:46] Decode batch, #running-req: 1, #token: 1072, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.75, #queue-req: 0, 
[2025-12-15 18:23:46] Decode batch, #running-req: 1, #token: 1112, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.75, #queue-req: 0, 
[2025-12-15 18:23:47] Decode batch, #running-req: 1, #token: 1152, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.74, #queue-req: 0, 
[2025-12-15 18:23:47] Decode batch, #running-req: 1, #token: 1192, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.72, #queue-req: 0, 
[2025-12-15 18:23:48] Decode batch, #running-req: 1, #token: 1232, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.71, #queue-req: 0, 
[2025-12-15 18:23:48] INFO:     127.0.0.1:45050 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:48] Prefill batch, #new-seq: 1, #new-token: 880, #cached-token: 66, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:48] Decode batch, #running-req: 1, #token: 984, token usage: 0.00, cuda graph: True, gen throughput (token/s): 70.58, #queue-req: 0, 
[2025-12-15 18:23:49] INFO:     127.0.0.1:45054 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:49] Prefill batch, #new-seq: 1, #new-token: 31, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:49] Decode batch, #running-req: 1, #token: 277, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.46, #queue-req: 0, 
[2025-12-15 18:23:50] Decode batch, #running-req: 1, #token: 317, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:23:50] INFO:     127.0.0.1:45058 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:50] Prefill batch, #new-seq: 1, #new-token: 1222, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:50] Decode batch, #running-req: 1, #token: 1312, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.22, #queue-req: 0, 
[2025-12-15 18:23:50] INFO:     127.0.0.1:45062 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:50] Prefill batch, #new-seq: 1, #new-token: 18, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:51] Decode batch, #running-req: 1, #token: 269, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.54, #queue-req: 0, 
[2025-12-15 18:23:51] Decode batch, #running-req: 1, #token: 309, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:23:51] INFO:     127.0.0.1:45066 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:51] Prefill batch, #new-seq: 1, #new-token: 1139, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:52] Decode batch, #running-req: 1, #token: 1223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.61, #queue-req: 0, 
[2025-12-15 18:23:52] INFO:     127.0.0.1:45070 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:52] Prefill batch, #new-seq: 1, #new-token: 77, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:52] Decode batch, #running-req: 1, #token: 316, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.14, #queue-req: 0, 
[2025-12-15 18:23:53] Decode batch, #running-req: 1, #token: 356, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.10, #queue-req: 0, 
[2025-12-15 18:23:53] INFO:     127.0.0.1:45074 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:53] Prefill batch, #new-seq: 1, #new-token: 1225, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:53] Decode batch, #running-req: 1, #token: 1301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.25, #queue-req: 0, 
[2025-12-15 18:23:54] INFO:     127.0.0.1:45078 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:54] Prefill batch, #new-seq: 1, #new-token: 33, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:54] Decode batch, #running-req: 1, #token: 264, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.11, #queue-req: 0, 
[2025-12-15 18:23:54] Decode batch, #running-req: 1, #token: 304, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:23:55] Decode batch, #running-req: 1, #token: 344, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.17, #queue-req: 0, 
[2025-12-15 18:23:55] INFO:     127.0.0.1:45082 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:55] Prefill batch, #new-seq: 1, #new-token: 1268, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:55] Decode batch, #running-req: 1, #token: 1337, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.20, #queue-req: 0, 
[2025-12-15 18:23:56] INFO:     127.0.0.1:45088 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:56] Prefill batch, #new-seq: 1, #new-token: 68, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:56] Decode batch, #running-req: 1, #token: 296, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.01, #queue-req: 0, 
[2025-12-15 18:23:56] Decode batch, #running-req: 1, #token: 336, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.16, #queue-req: 0, 
[2025-12-15 18:23:57] Decode batch, #running-req: 1, #token: 376, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.08, #queue-req: 0, 
[2025-12-15 18:23:57] INFO:     127.0.0.1:45094 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:57] Prefill batch, #new-seq: 1, #new-token: 1298, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:58] Decode batch, #running-req: 1, #token: 1378, token usage: 0.00, cuda graph: True, gen throughput (token/s): 67.38, #queue-req: 0, 
[2025-12-15 18:23:58] INFO:     127.0.0.1:45098 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:58] Prefill batch, #new-seq: 1, #new-token: 32, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:58] Decode batch, #running-req: 1, #token: 276, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.16, #queue-req: 0, 
[2025-12-15 18:23:58] INFO:     127.0.0.1:45102 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:58] Prefill batch, #new-seq: 1, #new-token: 1335, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:59] Decode batch, #running-req: 1, #token: 1399, token usage: 0.00, cuda graph: True, gen throughput (token/s): 67.52, #queue-req: 0, 
[2025-12-15 18:23:59] INFO:     127.0.0.1:45106 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:23:59] Prefill batch, #new-seq: 1, #new-token: 4, #cached-token: 223, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:23:59] Decode batch, #running-req: 1, #token: 235, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.07, #queue-req: 0, 
[2025-12-15 18:24:00] Decode batch, #running-req: 1, #token: 275, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.35, #queue-req: 0, 
[2025-12-15 18:24:00] INFO:     127.0.0.1:45110 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:00] Prefill batch, #new-seq: 1, #new-token: 925, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:00] Decode batch, #running-req: 1, #token: 1002, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.94, #queue-req: 0, 
[2025-12-15 18:24:01] INFO:     127.0.0.1:45114 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:01] Prefill batch, #new-seq: 1, #new-token: 16, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:01] Decode batch, #running-req: 1, #token: 249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.36, #queue-req: 0, 
[2025-12-15 18:24:01] Decode batch, #running-req: 1, #token: 289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.27, #queue-req: 0, 
[2025-12-15 18:24:02] Decode batch, #running-req: 1, #token: 329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:24:02] INFO:     127.0.0.1:45118 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:02] Prefill batch, #new-seq: 1, #new-token: 1093, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:02] Decode batch, #running-req: 1, #token: 1188, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.61, #queue-req: 0, 
[2025-12-15 18:24:02] INFO:     127.0.0.1:45122 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:02] Prefill batch, #new-seq: 1, #new-token: 49, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:03] Decode batch, #running-req: 1, #token: 297, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.32, #queue-req: 0, 
[2025-12-15 18:24:03] Decode batch, #running-req: 1, #token: 337, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.16, #queue-req: 0, 
[2025-12-15 18:24:04] INFO:     127.0.0.1:45126 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:04] Prefill batch, #new-seq: 1, #new-token: 1157, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:04] Decode batch, #running-req: 1, #token: 1239, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.32, #queue-req: 0, 
[2025-12-15 18:24:04] INFO:     127.0.0.1:45130 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:04] Prefill batch, #new-seq: 1, #new-token: 36, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:04] Decode batch, #running-req: 1, #token: 276, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.16, #queue-req: 0, 
[2025-12-15 18:24:05] Decode batch, #running-req: 1, #token: 316, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:24:05] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.06, #queue-req: 0, 
[2025-12-15 18:24:05] INFO:     127.0.0.1:45134 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:06] Prefill batch, #new-seq: 1, #new-token: 1162, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:06] Decode batch, #running-req: 1, #token: 1258, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.23, #queue-req: 0, 
[2025-12-15 18:24:06] INFO:     127.0.0.1:45138 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:06] Prefill batch, #new-seq: 1, #new-token: 47, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:07] Decode batch, #running-req: 1, #token: 300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.30, #queue-req: 0, 
[2025-12-15 18:24:07] Decode batch, #running-req: 1, #token: 340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.16, #queue-req: 0, 
[2025-12-15 18:24:07] INFO:     127.0.0.1:45142 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:07] Prefill batch, #new-seq: 1, #new-token: 1395, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:08] Decode batch, #running-req: 1, #token: 1474, token usage: 0.00, cuda graph: True, gen throughput (token/s): 67.24, #queue-req: 0, 
[2025-12-15 18:24:08] INFO:     127.0.0.1:45146 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:08] Prefill batch, #new-seq: 1, #new-token: 56, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:08] Decode batch, #running-req: 1, #token: 300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.19, #queue-req: 0, 
[2025-12-15 18:24:09] Decode batch, #running-req: 1, #token: 340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.17, #queue-req: 0, 
[2025-12-15 18:24:09] INFO:     127.0.0.1:45150 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:09] Prefill batch, #new-seq: 1, #new-token: 1042, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:09] Decode batch, #running-req: 1, #token: 1134, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.72, #queue-req: 0, 
[2025-12-15 18:24:09] INFO:     127.0.0.1:45154 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:09] Prefill batch, #new-seq: 1, #new-token: 17, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:10] Decode batch, #running-req: 1, #token: 273, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.50, #queue-req: 0, 
[2025-12-15 18:24:10] INFO:     127.0.0.1:45160 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:10] Prefill batch, #new-seq: 1, #new-token: 1028, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:10] Decode batch, #running-req: 1, #token: 1087, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.05, #queue-req: 0, 
[2025-12-15 18:24:11] Decode batch, #running-req: 1, #token: 1127, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.72, #queue-req: 0, 
[2025-12-15 18:24:11] INFO:     127.0.0.1:45164 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:11] Prefill batch, #new-seq: 1, #new-token: 36, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:11] Decode batch, #running-req: 1, #token: 295, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.42, #queue-req: 0, 
[2025-12-15 18:24:12] INFO:     127.0.0.1:45168 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:12] Prefill batch, #new-seq: 1, #new-token: 1074, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:12] Decode batch, #running-req: 1, #token: 1151, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.82, #queue-req: 0, 
[2025-12-15 18:24:12] INFO:     127.0.0.1:45172 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:12] Prefill batch, #new-seq: 1, #new-token: 18, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:13] Decode batch, #running-req: 1, #token: 259, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.45, #queue-req: 0, 
[2025-12-15 18:24:13] Decode batch, #running-req: 1, #token: 299, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.16, #queue-req: 0, 
[2025-12-15 18:24:13] INFO:     127.0.0.1:45176 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:13] Prefill batch, #new-seq: 1, #new-token: 966, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:14] Decode batch, #running-req: 1, #token: 1058, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.72, #queue-req: 0, 
[2025-12-15 18:24:14] INFO:     127.0.0.1:45180 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:14] Prefill batch, #new-seq: 1, #new-token: 23, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:14] Decode batch, #running-req: 1, #token: 274, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.49, #queue-req: 0, 
[2025-12-15 18:24:14] INFO:     127.0.0.1:45184 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:14] Prefill batch, #new-seq: 1, #new-token: 1004, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:15] Decode batch, #running-req: 1, #token: 1088, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.69, #queue-req: 0, 
[2025-12-15 18:24:15] INFO:     127.0.0.1:45188 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:15] Prefill batch, #new-seq: 1, #new-token: 19, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:15] Decode batch, #running-req: 1, #token: 257, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.45, #queue-req: 0, 
[2025-12-15 18:24:16] Decode batch, #running-req: 1, #token: 297, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:24:16] INFO:     127.0.0.1:45192 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:16] Prefill batch, #new-seq: 1, #new-token: 1022, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:16] Decode batch, #running-req: 1, #token: 1104, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.75, #queue-req: 0, 
[2025-12-15 18:24:17] INFO:     127.0.0.1:45196 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:17] Prefill batch, #new-seq: 1, #new-token: 40, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:17] Decode batch, #running-req: 1, #token: 279, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.25, #queue-req: 0, 
[2025-12-15 18:24:17] Decode batch, #running-req: 1, #token: 319, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:24:18] Decode batch, #running-req: 1, #token: 359, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.10, #queue-req: 0, 
[2025-12-15 18:24:18] INFO:     127.0.0.1:45200 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:18] Prefill batch, #new-seq: 1, #new-token: 1039, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:18] Decode batch, #running-req: 1, #token: 1125, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.75, #queue-req: 0, 
[2025-12-15 18:24:19] INFO:     127.0.0.1:45204 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:19] Prefill batch, #new-seq: 1, #new-token: 25, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:19] Decode batch, #running-req: 1, #token: 268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.27, #queue-req: 0, 
[2025-12-15 18:24:19] Decode batch, #running-req: 1, #token: 308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.23, #queue-req: 0, 
[2025-12-15 18:24:20] Decode batch, #running-req: 1, #token: 348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.13, #queue-req: 0, 
[2025-12-15 18:24:20] INFO:     127.0.0.1:45208 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:20] Prefill batch, #new-seq: 1, #new-token: 1099, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:21] Decode batch, #running-req: 1, #token: 1195, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.56, #queue-req: 0, 
[2025-12-15 18:24:21] INFO:     127.0.0.1:45212 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:21] Prefill batch, #new-seq: 1, #new-token: 19, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:21] Decode batch, #running-req: 1, #token: 278, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.51, #queue-req: 0, 
[2025-12-15 18:24:22] Decode batch, #running-req: 1, #token: 318, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:24:22] INFO:     127.0.0.1:45216 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:22] Prefill batch, #new-seq: 1, #new-token: 1038, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:22] Decode batch, #running-req: 1, #token: 1118, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.86, #queue-req: 0, 
[2025-12-15 18:24:22] INFO:     127.0.0.1:45220 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:22] Prefill batch, #new-seq: 1, #new-token: 33, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:23] Decode batch, #running-req: 1, #token: 275, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.33, #queue-req: 0, 
[2025-12-15 18:24:23] Decode batch, #running-req: 1, #token: 315, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:24:23] INFO:     127.0.0.1:45224 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:23] Prefill batch, #new-seq: 1, #new-token: 1212, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:24] Decode batch, #running-req: 1, #token: 1297, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.05, #queue-req: 0, 
[2025-12-15 18:24:24] INFO:     127.0.0.1:45228 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:24] Prefill batch, #new-seq: 1, #new-token: 16, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:24] Decode batch, #running-req: 1, #token: 259, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.46, #queue-req: 0, 
[2025-12-15 18:24:25] Decode batch, #running-req: 1, #token: 299, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.25, #queue-req: 0, 
[2025-12-15 18:24:25] INFO:     127.0.0.1:45232 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:25] Prefill batch, #new-seq: 1, #new-token: 962, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:25] Decode batch, #running-req: 1, #token: 1033, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.92, #queue-req: 0, 
[2025-12-15 18:24:26] INFO:     127.0.0.1:45236 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:26] Prefill batch, #new-seq: 1, #new-token: 17, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:26] Decode batch, #running-req: 1, #token: 238, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.14, #queue-req: 0, 
[2025-12-15 18:24:26] INFO:     127.0.0.1:45240 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:26] Prefill batch, #new-seq: 1, #new-token: 978, #cached-token: 65, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:26] Decode batch, #running-req: 1, #token: 1071, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.84, #queue-req: 0, 
[2025-12-15 18:24:27] INFO:     127.0.0.1:45244 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:27] Prefill batch, #new-seq: 1, #new-token: 18, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:27] Decode batch, #running-req: 1, #token: 263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.52, #queue-req: 0, 
[2025-12-15 18:24:27] Decode batch, #running-req: 1, #token: 303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.17, #queue-req: 0, 
[2025-12-15 18:24:28] INFO:     127.0.0.1:45248 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:28] Prefill batch, #new-seq: 1, #new-token: 941, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:28] Decode batch, #running-req: 1, #token: 1029, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.82, #queue-req: 0, 
[2025-12-15 18:24:28] INFO:     127.0.0.1:45252 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:28] Prefill batch, #new-seq: 1, #new-token: 13, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:29] Decode batch, #running-req: 1, #token: 255, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.44, #queue-req: 0, 
[2025-12-15 18:24:29] INFO:     127.0.0.1:45256 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:29] Prefill batch, #new-seq: 1, #new-token: 898, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:29] Decode batch, #running-req: 1, #token: 969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 70.05, #queue-req: 0, 
[2025-12-15 18:24:29] INFO:     127.0.0.1:45260 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:29] Prefill batch, #new-seq: 1, #new-token: 12, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:30] Decode batch, #running-req: 1, #token: 250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.39, #queue-req: 0, 
[2025-12-15 18:24:30] INFO:     127.0.0.1:45264 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:30] Prefill batch, #new-seq: 1, #new-token: 864, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:30] Decode batch, #running-req: 1, #token: 929, token usage: 0.00, cuda graph: True, gen throughput (token/s): 71.24, #queue-req: 0, 
[2025-12-15 18:24:31] INFO:     127.0.0.1:45268 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:31] Prefill batch, #new-seq: 1, #new-token: 43, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:31] Decode batch, #running-req: 1, #token: 268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.14, #queue-req: 0, 
[2025-12-15 18:24:31] Decode batch, #running-req: 1, #token: 308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:24:32] Decode batch, #running-req: 1, #token: 348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.12, #queue-req: 0, 
[2025-12-15 18:24:32] INFO:     127.0.0.1:45272 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:32] Prefill batch, #new-seq: 1, #new-token: 1285, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:32] Decode batch, #running-req: 1, #token: 1373, token usage: 0.00, cuda graph: True, gen throughput (token/s): 67.41, #queue-req: 0, 
[2025-12-15 18:24:32] INFO:     127.0.0.1:45276 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:32] Prefill batch, #new-seq: 1, #new-token: 48, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:33] Decode batch, #running-req: 1, #token: 300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.28, #queue-req: 0, 
[2025-12-15 18:24:33] Decode batch, #running-req: 1, #token: 340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:24:34] Decode batch, #running-req: 1, #token: 380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.07, #queue-req: 0, 
[2025-12-15 18:24:34] INFO:     127.0.0.1:45280 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:34] Prefill batch, #new-seq: 1, #new-token: 1176, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:34] Decode batch, #running-req: 1, #token: 1272, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.26, #queue-req: 0, 
[2025-12-15 18:24:34] INFO:     127.0.0.1:45284 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:34] Prefill batch, #new-seq: 1, #new-token: 22, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:35] Decode batch, #running-req: 1, #token: 274, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.46, #queue-req: 0, 
[2025-12-15 18:24:35] INFO:     127.0.0.1:45288 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:35] Prefill batch, #new-seq: 1, #new-token: 936, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:36] Decode batch, #running-req: 1, #token: 1030, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.83, #queue-req: 0, 
[2025-12-15 18:24:36] INFO:     127.0.0.1:45292 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:36] Prefill batch, #new-seq: 1, #new-token: 31, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:36] Decode batch, #running-req: 1, #token: 287, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.52, #queue-req: 0, 
[2025-12-15 18:24:36] INFO:     127.0.0.1:45296 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:36] Prefill batch, #new-seq: 1, #new-token: 1040, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:37] Decode batch, #running-req: 1, #token: 1105, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.98, #queue-req: 0, 
[2025-12-15 18:24:37] INFO:     127.0.0.1:45300 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:37] Prefill batch, #new-seq: 1, #new-token: 40, #cached-token: 223, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:37] Decode batch, #running-req: 1, #token: 266, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.03, #queue-req: 0, 
[2025-12-15 18:24:38] Decode batch, #running-req: 1, #token: 306, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:24:38] INFO:     127.0.0.1:45304 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:38] Prefill batch, #new-seq: 1, #new-token: 1186, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:38] Decode batch, #running-req: 1, #token: 1268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.36, #queue-req: 0, 
[2025-12-15 18:24:38] INFO:     127.0.0.1:45308 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:38] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 223, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:39] Decode batch, #running-req: 1, #token: 259, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.38, #queue-req: 0, 
[2025-12-15 18:24:39] Decode batch, #running-req: 1, #token: 299, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:24:40] INFO:     127.0.0.1:45312 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:40] Prefill batch, #new-seq: 1, #new-token: 1063, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:40] Decode batch, #running-req: 1, #token: 1133, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.89, #queue-req: 0, 
[2025-12-15 18:24:40] INFO:     127.0.0.1:45318 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:40] Prefill batch, #new-seq: 1, #new-token: 33, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:40] Decode batch, #running-req: 1, #token: 257, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.16, #queue-req: 0, 
[2025-12-15 18:24:41] Decode batch, #running-req: 1, #token: 297, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:24:41] INFO:     127.0.0.1:45322 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:41] Prefill batch, #new-seq: 1, #new-token: 1072, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:41] Decode batch, #running-req: 1, #token: 1163, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.68, #queue-req: 0, 
[2025-12-15 18:24:41] INFO:     127.0.0.1:45326 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:42] Prefill batch, #new-seq: 1, #new-token: 38, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:42] Decode batch, #running-req: 1, #token: 297, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.38, #queue-req: 0, 
[2025-12-15 18:24:42] Decode batch, #running-req: 1, #token: 337, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:24:43] INFO:     127.0.0.1:45330 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:43] Prefill batch, #new-seq: 1, #new-token: 1040, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:43] Decode batch, #running-req: 1, #token: 1113, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.85, #queue-req: 0, 
[2025-12-15 18:24:43] INFO:     127.0.0.1:45334 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:43] Prefill batch, #new-seq: 1, #new-token: 61, #cached-token: 223, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:44] Decode batch, #running-req: 1, #token: 297, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.11, #queue-req: 0, 
[2025-12-15 18:24:44] Decode batch, #running-req: 1, #token: 337, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.17, #queue-req: 0, 
[2025-12-15 18:24:45] Decode batch, #running-req: 1, #token: 377, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.09, #queue-req: 0, 
[2025-12-15 18:24:45] INFO:     127.0.0.1:45338 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:45] Prefill batch, #new-seq: 1, #new-token: 1276, #cached-token: 63, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:45] Decode batch, #running-req: 1, #token: 1359, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.10, #queue-req: 0, 
[2025-12-15 18:24:45] INFO:     127.0.0.1:45342 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:45] Prefill batch, #new-seq: 1, #new-token: 29, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:46] Decode batch, #running-req: 1, #token: 267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.13, #queue-req: 0, 
[2025-12-15 18:24:46] Decode batch, #running-req: 1, #token: 307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:24:47] Decode batch, #running-req: 1, #token: 347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.13, #queue-req: 0, 
[2025-12-15 18:24:47] INFO:     127.0.0.1:45346 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:47] Prefill batch, #new-seq: 1, #new-token: 1082, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:47] Decode batch, #running-req: 1, #token: 1178, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.61, #queue-req: 0, 
[2025-12-15 18:24:47] INFO:     127.0.0.1:45350 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:47] Prefill batch, #new-seq: 1, #new-token: 35, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:48] Decode batch, #running-req: 1, #token: 288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.31, #queue-req: 0, 
[2025-12-15 18:24:48] Decode batch, #running-req: 1, #token: 328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:24:48] INFO:     127.0.0.1:45354 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:48] Prefill batch, #new-seq: 1, #new-token: 1235, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:49] Decode batch, #running-req: 1, #token: 1319, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.25, #queue-req: 0, 
[2025-12-15 18:24:49] INFO:     127.0.0.1:45358 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:49] Prefill batch, #new-seq: 1, #new-token: 15, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:49] Decode batch, #running-req: 1, #token: 263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.43, #queue-req: 0, 
[2025-12-15 18:24:50] INFO:     127.0.0.1:45362 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:50] Prefill batch, #new-seq: 1, #new-token: 1119, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:50] Decode batch, #running-req: 1, #token: 1182, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.89, #queue-req: 0, 
[2025-12-15 18:24:50] INFO:     127.0.0.1:45366 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:50] Prefill batch, #new-seq: 1, #new-token: 36, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:50] Decode batch, #running-req: 1, #token: 263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.08, #queue-req: 0, 
[2025-12-15 18:24:51] INFO:     127.0.0.1:45370 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:51] Prefill batch, #new-seq: 1, #new-token: 1215, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:51] Decode batch, #running-req: 1, #token: 1282, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.41, #queue-req: 0, 
[2025-12-15 18:24:51] INFO:     127.0.0.1:45374 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:51] Prefill batch, #new-seq: 1, #new-token: 45, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:52] Decode batch, #running-req: 1, #token: 278, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.09, #queue-req: 0, 
[2025-12-15 18:24:52] Decode batch, #running-req: 1, #token: 318, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.21, #queue-req: 0, 
[2025-12-15 18:24:52] INFO:     127.0.0.1:45378 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:52] Prefill batch, #new-seq: 1, #new-token: 1208, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:53] Decode batch, #running-req: 1, #token: 1273, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.34, #queue-req: 0, 
[2025-12-15 18:24:53] INFO:     127.0.0.1:45384 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:53] Prefill batch, #new-seq: 1, #new-token: 87, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:53] Decode batch, #running-req: 1, #token: 314, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.01, #queue-req: 0, 
[2025-12-15 18:24:54] Decode batch, #running-req: 1, #token: 354, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.12, #queue-req: 0, 
[2025-12-15 18:24:54] INFO:     127.0.0.1:45388 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:54] Prefill batch, #new-seq: 1, #new-token: 1243, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:54] Decode batch, #running-req: 1, #token: 1307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.29, #queue-req: 0, 
[2025-12-15 18:24:55] INFO:     127.0.0.1:45392 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:55] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.45, #queue-req: 0, 
[2025-12-15 18:24:55] Prefill batch, #new-seq: 1, #new-token: 27, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:55] Decode batch, #running-req: 1, #token: 289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.63, #queue-req: 0, 
[2025-12-15 18:24:56] Decode batch, #running-req: 1, #token: 329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.23, #queue-req: 0, 
[2025-12-15 18:24:56] INFO:     127.0.0.1:45396 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:56] Prefill batch, #new-seq: 1, #new-token: 1161, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:56] Decode batch, #running-req: 1, #token: 1225, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.51, #queue-req: 0, 
[2025-12-15 18:24:57] INFO:     127.0.0.1:45400 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:57] Prefill batch, #new-seq: 1, #new-token: 45, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:57] Decode batch, #running-req: 1, #token: 267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.00, #queue-req: 0, 
[2025-12-15 18:24:57] Decode batch, #running-req: 1, #token: 307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:24:58] INFO:     127.0.0.1:45404 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:58] Prefill batch, #new-seq: 1, #new-token: 1229, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:58] Decode batch, #running-req: 1, #token: 1310, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.28, #queue-req: 0, 
[2025-12-15 18:24:58] INFO:     127.0.0.1:45408 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:24:58] Prefill batch, #new-seq: 1, #new-token: 29, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:24:59] Decode batch, #running-req: 1, #token: 265, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.22, #queue-req: 0, 
[2025-12-15 18:24:59] Decode batch, #running-req: 1, #token: 305, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.21, #queue-req: 0, 
[2025-12-15 18:25:00] Decode batch, #running-req: 1, #token: 345, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.13, #queue-req: 0, 
[2025-12-15 18:25:00] INFO:     127.0.0.1:45414 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:00] Prefill batch, #new-seq: 1, #new-token: 1257, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:00] Decode batch, #running-req: 1, #token: 1354, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.09, #queue-req: 0, 
[2025-12-15 18:25:00] INFO:     127.0.0.1:45418 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:00] Prefill batch, #new-seq: 1, #new-token: 35, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:01] Decode batch, #running-req: 1, #token: 290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.42, #queue-req: 0, 
[2025-12-15 18:25:01] Decode batch, #running-req: 1, #token: 330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:25:01] INFO:     127.0.0.1:45422 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:01] Prefill batch, #new-seq: 1, #new-token: 1199, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:02] Decode batch, #running-req: 1, #token: 1286, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.22, #queue-req: 0, 
[2025-12-15 18:25:02] INFO:     127.0.0.1:45426 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:02] Prefill batch, #new-seq: 1, #new-token: 24, #cached-token: 222, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:02] Decode batch, #running-req: 1, #token: 276, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.41, #queue-req: 0, 
[2025-12-15 18:25:03] Decode batch, #running-req: 1, #token: 316, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:25:03] INFO:     127.0.0.1:45430 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:03] Prefill batch, #new-seq: 1, #new-token: 1236, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:03] Decode batch, #running-req: 1, #token: 1310, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.23, #queue-req: 0, 
[2025-12-15 18:25:04] INFO:     127.0.0.1:45434 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:04] Prefill batch, #new-seq: 1, #new-token: 57, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:04] Decode batch, #running-req: 1, #token: 292, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.20, #queue-req: 0, 
[2025-12-15 18:25:04] INFO:     127.0.0.1:45438 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:04] Prefill batch, #new-seq: 1, #new-token: 1174, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:04] Decode batch, #running-req: 1, #token: 1235, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.58, #queue-req: 0, 
[2025-12-15 18:25:05] INFO:     127.0.0.1:45442 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:05] Prefill batch, #new-seq: 1, #new-token: 60, #cached-token: 223, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:05] Decode batch, #running-req: 1, #token: 289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 75.95, #queue-req: 0, 
[2025-12-15 18:25:05] Decode batch, #running-req: 1, #token: 329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.16, #queue-req: 0, 
[2025-12-15 18:25:06] INFO:     127.0.0.1:45446 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:06] Prefill batch, #new-seq: 1, #new-token: 1153, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:06] Decode batch, #running-req: 1, #token: 1210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.55, #queue-req: 0, 
[2025-12-15 18:25:07] Decode batch, #running-req: 1, #token: 1250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.64, #queue-req: 0, 
[2025-12-15 18:25:07] INFO:     127.0.0.1:45450 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:07] Prefill batch, #new-seq: 1, #new-token: 56, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:07] Decode batch, #running-req: 1, #token: 308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.27, #queue-req: 0, 
[2025-12-15 18:25:08] Decode batch, #running-req: 1, #token: 348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.14, #queue-req: 0, 
[2025-12-15 18:25:08] INFO:     127.0.0.1:45454 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:08] Prefill batch, #new-seq: 1, #new-token: 1279, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:08] Decode batch, #running-req: 1, #token: 1346, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.18, #queue-req: 0, 
[2025-12-15 18:25:09] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.62, #queue-req: 0, 
[2025-12-15 18:25:09] INFO:     127.0.0.1:45458 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:09] Prefill batch, #new-seq: 1, #new-token: 15, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:09] Decode batch, #running-req: 1, #token: 276, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.59, #queue-req: 0, 
[2025-12-15 18:25:10] INFO:     127.0.0.1:45462 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:10] Prefill batch, #new-seq: 1, #new-token: 1087, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:10] Decode batch, #running-req: 1, #token: 1145, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.02, #queue-req: 0, 
[2025-12-15 18:25:10] INFO:     127.0.0.1:45466 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:10] Prefill batch, #new-seq: 1, #new-token: 60, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:10] Decode batch, #running-req: 1, #token: 293, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.03, #queue-req: 0, 
[2025-12-15 18:25:11] Decode batch, #running-req: 1, #token: 333, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:25:11] Decode batch, #running-req: 1, #token: 373, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.10, #queue-req: 0, 
[2025-12-15 18:25:12] INFO:     127.0.0.1:45470 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:12] Prefill batch, #new-seq: 1, #new-token: 1339, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:12] Decode batch, #running-req: 1, #token: 1399, token usage: 0.00, cuda graph: True, gen throughput (token/s): 67.42, #queue-req: 0, 
[2025-12-15 18:25:12] INFO:     127.0.0.1:45474 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:12] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.16, #queue-req: 0, 
[2025-12-15 18:25:12] Prefill batch, #new-seq: 1, #new-token: 15, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:13] Decode batch, #running-req: 1, #token: 275, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.97, #queue-req: 0, 
[2025-12-15 18:25:13] INFO:     127.0.0.1:45478 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:13] Prefill batch, #new-seq: 1, #new-token: 1037, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:13] Decode batch, #running-req: 1, #token: 1106, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.97, #queue-req: 0, 
[2025-12-15 18:25:14] INFO:     127.0.0.1:45482 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:14] Prefill batch, #new-seq: 1, #new-token: 38, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:14] Decode batch, #running-req: 1, #token: 272, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.27, #queue-req: 0, 
[2025-12-15 18:25:15] Decode batch, #running-req: 1, #token: 312, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:25:15] Decode batch, #running-req: 1, #token: 352, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.14, #queue-req: 0, 
[2025-12-15 18:25:15] INFO:     127.0.0.1:45486 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:15] Prefill batch, #new-seq: 1, #new-token: 1255, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:16] Decode batch, #running-req: 1, #token: 1332, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.23, #queue-req: 0, 
[2025-12-15 18:25:16] INFO:     127.0.0.1:45492 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:16] Prefill batch, #new-seq: 1, #new-token: 42, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:16] Decode batch, #running-req: 1, #token: 280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.08, #queue-req: 0, 
[2025-12-15 18:25:17] Decode batch, #running-req: 1, #token: 320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:25:17] Decode batch, #running-req: 1, #token: 360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.13, #queue-req: 0, 
[2025-12-15 18:25:17] INFO:     127.0.0.1:45496 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:17] Prefill batch, #new-seq: 1, #new-token: 1395, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:18] Decode batch, #running-req: 1, #token: 1494, token usage: 0.00, cuda graph: True, gen throughput (token/s): 67.11, #queue-req: 0, 
[2025-12-15 18:25:18] INFO:     127.0.0.1:45500 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:18] Prefill batch, #new-seq: 1, #new-token: 9, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:18] Decode batch, #running-req: 1, #token: 265, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.55, #queue-req: 0, 
[2025-12-15 18:25:19] INFO:     127.0.0.1:45504 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:19] Prefill batch, #new-seq: 1, #new-token: 981, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:19] Decode batch, #running-req: 1, #token: 1047, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.88, #queue-req: 0, 
[2025-12-15 18:25:19] INFO:     127.0.0.1:45508 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:19] Prefill batch, #new-seq: 1, #new-token: 44, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:19] Decode batch, #running-req: 1, #token: 272, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.10, #queue-req: 0, 
[2025-12-15 18:25:20] Decode batch, #running-req: 1, #token: 312, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:25:20] INFO:     127.0.0.1:45512 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:20] Prefill batch, #new-seq: 1, #new-token: 973, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:20] Decode batch, #running-req: 1, #token: 1065, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.74, #queue-req: 0, 
[2025-12-15 18:25:20] INFO:     127.0.0.1:45516 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:21] Prefill batch, #new-seq: 1, #new-token: 21, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:21] Decode batch, #running-req: 1, #token: 272, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.54, #queue-req: 0, 
[2025-12-15 18:25:21] INFO:     127.0.0.1:45520 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:21] Prefill batch, #new-seq: 1, #new-token: 954, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:22] Decode batch, #running-req: 1, #token: 1026, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.95, #queue-req: 0, 
[2025-12-15 18:25:22] INFO:     127.0.0.1:45524 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:22] Prefill batch, #new-seq: 1, #new-token: 35, #cached-token: 225, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:22] Decode batch, #running-req: 1, #token: 274, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.20, #queue-req: 0, 
[2025-12-15 18:25:23] Decode batch, #running-req: 1, #token: 314, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:25:23] Decode batch, #running-req: 1, #token: 354, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.10, #queue-req: 0, 
[2025-12-15 18:25:23] INFO:     127.0.0.1:45528 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:23] Prefill batch, #new-seq: 1, #new-token: 1000, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:24] Decode batch, #running-req: 1, #token: 1094, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.60, #queue-req: 0, 
[2025-12-15 18:25:24] INFO:     127.0.0.1:45532 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:24] Prefill batch, #new-seq: 1, #new-token: 10, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:24] Decode batch, #running-req: 1, #token: 264, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.64, #queue-req: 0, 
[2025-12-15 18:25:24] INFO:     127.0.0.1:45536 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:24] Prefill batch, #new-seq: 1, #new-token: 1161, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:25] Decode batch, #running-req: 1, #token: 1244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.38, #queue-req: 0, 
[2025-12-15 18:25:25] INFO:     127.0.0.1:45540 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:25] Prefill batch, #new-seq: 1, #new-token: 24, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:25] Decode batch, #running-req: 1, #token: 270, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.40, #queue-req: 0, 
[2025-12-15 18:25:26] Decode batch, #running-req: 1, #token: 310, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.21, #queue-req: 0, 
[2025-12-15 18:25:26] INFO:     127.0.0.1:45544 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:26] Prefill batch, #new-seq: 1, #new-token: 1080, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:26] Decode batch, #running-req: 1, #token: 1158, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.73, #queue-req: 0, 
[2025-12-15 18:25:27] INFO:     127.0.0.1:45548 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:27] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:27] Decode batch, #running-req: 1, #token: 240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.27, #queue-req: 0, 
[2025-12-15 18:25:27] Decode batch, #running-req: 1, #token: 280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.34, #queue-req: 0, 
[2025-12-15 18:25:28] Decode batch, #running-req: 1, #token: 320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:25:28] Decode batch, #running-req: 1, #token: 360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.12, #queue-req: 0, 
[2025-12-15 18:25:29] Decode batch, #running-req: 1, #token: 400, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.05, #queue-req: 0, 
[2025-12-15 18:25:29] INFO:     127.0.0.1:45552 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:30] Prefill batch, #new-seq: 1, #new-token: 1115, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:30] Decode batch, #running-req: 1, #token: 1207, token usage: 0.00, cuda graph: True, gen throughput (token/s): 33.40, #queue-req: 0, 
[2025-12-15 18:25:30] INFO:     127.0.0.1:45556 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:30] Prefill batch, #new-seq: 1, #new-token: 19, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:31] Decode batch, #running-req: 1, #token: 263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.37, #queue-req: 0, 
[2025-12-15 18:25:31] Decode batch, #running-req: 1, #token: 303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.17, #queue-req: 0, 
[2025-12-15 18:25:31] INFO:     127.0.0.1:45560 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:31] Prefill batch, #new-seq: 1, #new-token: 1123, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:32] Decode batch, #running-req: 1, #token: 1218, token usage: 0.00, cuda graph: True, gen throughput (token/s): 56.76, #queue-req: 0, 
[2025-12-15 18:25:32] INFO:     127.0.0.1:45564 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:32] Prefill batch, #new-seq: 1, #new-token: 13, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:32] Decode batch, #running-req: 1, #token: 263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.52, #queue-req: 0, 
[2025-12-15 18:25:33] INFO:     127.0.0.1:45568 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:33] Prefill batch, #new-seq: 1, #new-token: 1079, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:33] Decode batch, #running-req: 1, #token: 1146, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.90, #queue-req: 0, 
[2025-12-15 18:25:33] INFO:     127.0.0.1:45574 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:33] Prefill batch, #new-seq: 1, #new-token: 16, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:33] Decode batch, #running-req: 1, #token: 248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.25, #queue-req: 0, 
[2025-12-15 18:25:34] Decode batch, #running-req: 1, #token: 288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.30, #queue-req: 0, 
[2025-12-15 18:25:34] INFO:     127.0.0.1:45578 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:34] Prefill batch, #new-seq: 1, #new-token: 1217, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:34] Decode batch, #running-req: 1, #token: 1295, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.27, #queue-req: 0, 
[2025-12-15 18:25:35] INFO:     127.0.0.1:45582 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:35] Prefill batch, #new-seq: 1, #new-token: 19, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:35] Decode batch, #running-req: 1, #token: 260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.48, #queue-req: 0, 
[2025-12-15 18:25:36] Decode batch, #running-req: 1, #token: 300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.23, #queue-req: 0, 
[2025-12-15 18:25:36] INFO:     127.0.0.1:45586 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:36] Prefill batch, #new-seq: 1, #new-token: 1157, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:36] Decode batch, #running-req: 1, #token: 1226, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.47, #queue-req: 0, 
[2025-12-15 18:25:36] INFO:     127.0.0.1:45590 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:36] Prefill batch, #new-seq: 1, #new-token: 19, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:37] Decode batch, #running-req: 1, #token: 251, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.30, #queue-req: 0, 
[2025-12-15 18:25:37] INFO:     127.0.0.1:45594 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:37] Prefill batch, #new-seq: 1, #new-token: 1130, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:37] Decode batch, #running-req: 1, #token: 1189, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.93, #queue-req: 0, 
[2025-12-15 18:25:38] INFO:     127.0.0.1:45598 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:38] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:38] Decode batch, #running-req: 1, #token: 227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.03, #queue-req: 0, 
[2025-12-15 18:25:38] Decode batch, #running-req: 1, #token: 267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.42, #queue-req: 0, 
[2025-12-15 18:25:39] INFO:     127.0.0.1:45602 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:39] Prefill batch, #new-seq: 1, #new-token: 932, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:39] Decode batch, #running-req: 1, #token: 1000, token usage: 0.00, cuda graph: True, gen throughput (token/s): 70.03, #queue-req: 0, 
[2025-12-15 18:25:39] INFO:     127.0.0.1:45608 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:39] Prefill batch, #new-seq: 1, #new-token: 12, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:39] Decode batch, #running-req: 1, #token: 241, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.24, #queue-req: 0, 
[2025-12-15 18:25:40] Decode batch, #running-req: 1, #token: 281, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.28, #queue-req: 0, 
[2025-12-15 18:25:40] INFO:     127.0.0.1:45612 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:40] Prefill batch, #new-seq: 1, #new-token: 834, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:40] Decode batch, #running-req: 1, #token: 919, token usage: 0.00, cuda graph: True, gen throughput (token/s): 71.08, #queue-req: 0, 
[2025-12-15 18:25:41] Decode batch, #running-req: 1, #token: 959, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.91, #queue-req: 0, 
[2025-12-15 18:25:41] Decode batch, #running-req: 1, #token: 999, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.93, #queue-req: 0, 
[2025-12-15 18:25:42] Decode batch, #running-req: 1, #token: 1039, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.88, #queue-req: 0, 
[2025-12-15 18:25:42] Decode batch, #running-req: 1, #token: 1079, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.79, #queue-req: 0, 
[2025-12-15 18:25:43] Decode batch, #running-req: 1, #token: 1119, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.74, #queue-req: 0, 
[2025-12-15 18:25:43] Decode batch, #running-req: 1, #token: 1159, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.72, #queue-req: 0, 
[2025-12-15 18:25:44] Decode batch, #running-req: 1, #token: 1199, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.71, #queue-req: 0, 
[2025-12-15 18:25:44] Decode batch, #running-req: 1, #token: 1239, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.71, #queue-req: 0, 
[2025-12-15 18:25:45] Decode batch, #running-req: 1, #token: 1279, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.70, #queue-req: 0, 
[2025-12-15 18:25:45] Decode batch, #running-req: 1, #token: 1319, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.70, #queue-req: 0, 
[2025-12-15 18:25:46] Decode batch, #running-req: 1, #token: 1359, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.69, #queue-req: 0, 
[2025-12-15 18:25:46] Decode batch, #running-req: 1, #token: 1399, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.70, #queue-req: 0, 
[2025-12-15 18:25:47] Decode batch, #running-req: 1, #token: 1439, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.69, #queue-req: 0, 
[2025-12-15 18:25:47] Decode batch, #running-req: 1, #token: 1479, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.68, #queue-req: 0, 
[2025-12-15 18:25:48] Decode batch, #running-req: 1, #token: 1519, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.66, #queue-req: 0, 
[2025-12-15 18:25:48] Decode batch, #running-req: 1, #token: 1559, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.66, #queue-req: 0, 
[2025-12-15 18:25:49] Decode batch, #running-req: 1, #token: 1599, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.64, #queue-req: 0, 
[2025-12-15 18:25:49] Decode batch, #running-req: 1, #token: 1639, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.64, #queue-req: 0, 
[2025-12-15 18:25:50] Decode batch, #running-req: 1, #token: 1679, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.61, #queue-req: 0, 
[2025-12-15 18:25:50] Decode batch, #running-req: 1, #token: 1719, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.62, #queue-req: 0, 
[2025-12-15 18:25:51] Decode batch, #running-req: 1, #token: 1759, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:25:51] Decode batch, #running-req: 1, #token: 1799, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:25:52] Decode batch, #running-req: 1, #token: 1839, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.60, #queue-req: 0, 
[2025-12-15 18:25:52] Decode batch, #running-req: 1, #token: 1879, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.58, #queue-req: 0, 
[2025-12-15 18:25:53] INFO:     127.0.0.1:45616 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:53] Prefill batch, #new-seq: 1, #new-token: 51, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:53] Decode batch, #running-req: 1, #token: 303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 75.85, #queue-req: 0, 
[2025-12-15 18:25:53] Decode batch, #running-req: 1, #token: 343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:25:54] INFO:     127.0.0.1:45620 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:25:54] Prefill batch, #new-seq: 1, #new-token: 1188, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:25:54] Decode batch, #running-req: 1, #token: 1266, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.34, #queue-req: 0, 
[2025-12-15 18:25:55] Decode batch, #running-req: 1, #token: 1306, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.77, #queue-req: 0, 
[2025-12-15 18:25:55] Decode batch, #running-req: 1, #token: 1346, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.81, #queue-req: 0, 
[2025-12-15 18:25:56] Decode batch, #running-req: 1, #token: 1386, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.80, #queue-req: 0, 
[2025-12-15 18:25:56] Decode batch, #running-req: 1, #token: 1426, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.73, #queue-req: 0, 
[2025-12-15 18:25:57] Decode batch, #running-req: 1, #token: 1466, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.71, #queue-req: 0, 
[2025-12-15 18:25:57] Decode batch, #running-req: 1, #token: 1506, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.70, #queue-req: 0, 
[2025-12-15 18:25:58] Decode batch, #running-req: 1, #token: 1546, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.69, #queue-req: 0, 
[2025-12-15 18:25:58] Decode batch, #running-req: 1, #token: 1586, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.69, #queue-req: 0, 
[2025-12-15 18:25:59] Decode batch, #running-req: 1, #token: 1626, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.67, #queue-req: 0, 
[2025-12-15 18:25:59] Decode batch, #running-req: 1, #token: 1666, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.63, #queue-req: 0, 
[2025-12-15 18:26:00] Decode batch, #running-req: 1, #token: 1706, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.63, #queue-req: 0, 
[2025-12-15 18:26:00] Decode batch, #running-req: 1, #token: 1746, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.61, #queue-req: 0, 
[2025-12-15 18:26:01] Decode batch, #running-req: 1, #token: 1786, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.59, #queue-req: 0, 
[2025-12-15 18:26:01] Decode batch, #running-req: 1, #token: 1826, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.62, #queue-req: 0, 
[2025-12-15 18:26:02] Decode batch, #running-req: 1, #token: 1866, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.62, #queue-req: 0, 
[2025-12-15 18:26:02] Decode batch, #running-req: 1, #token: 1906, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.59, #queue-req: 0, 
[2025-12-15 18:26:03] Decode batch, #running-req: 1, #token: 1946, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.58, #queue-req: 0, 
[2025-12-15 18:26:03] Decode batch, #running-req: 1, #token: 1986, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.58, #queue-req: 0, 
[2025-12-15 18:26:04] Decode batch, #running-req: 1, #token: 2026, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:26:04] Decode batch, #running-req: 1, #token: 2066, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.53, #queue-req: 0, 
[2025-12-15 18:26:05] Decode batch, #running-req: 1, #token: 2106, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.49, #queue-req: 0, 
[2025-12-15 18:26:05] Decode batch, #running-req: 1, #token: 2146, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.49, #queue-req: 0, 
[2025-12-15 18:26:06] Decode batch, #running-req: 1, #token: 2186, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.47, #queue-req: 0, 
[2025-12-15 18:26:06] Decode batch, #running-req: 1, #token: 2226, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.45, #queue-req: 0, 
[2025-12-15 18:26:06] INFO:     127.0.0.1:45624 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:06] Prefill batch, #new-seq: 1, #new-token: 17, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:07] Decode batch, #running-req: 1, #token: 259, token usage: 0.00, cuda graph: True, gen throughput (token/s): 75.83, #queue-req: 0, 
[2025-12-15 18:26:07] Decode batch, #running-req: 1, #token: 299, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:26:08] Decode batch, #running-req: 1, #token: 339, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.16, #queue-req: 0, 
[2025-12-15 18:26:08] INFO:     127.0.0.1:45628 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:08] Prefill batch, #new-seq: 1, #new-token: 1034, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:08] Decode batch, #running-req: 1, #token: 1129, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.78, #queue-req: 0, 
[2025-12-15 18:26:08] INFO:     127.0.0.1:45634 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:08] Prefill batch, #new-seq: 1, #new-token: 45, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:09] Decode batch, #running-req: 1, #token: 298, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.23, #queue-req: 0, 
[2025-12-15 18:26:09] Decode batch, #running-req: 1, #token: 338, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.15, #queue-req: 0, 
[2025-12-15 18:26:09] INFO:     127.0.0.1:45638 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:09] Prefill batch, #new-seq: 1, #new-token: 1150, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:10] Decode batch, #running-req: 1, #token: 1233, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.52, #queue-req: 0, 
[2025-12-15 18:26:10] INFO:     127.0.0.1:45642 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:10] Prefill batch, #new-seq: 1, #new-token: 40, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:10] Decode batch, #running-req: 1, #token: 283, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.29, #queue-req: 0, 
[2025-12-15 18:26:11] Decode batch, #running-req: 1, #token: 323, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:26:11] INFO:     127.0.0.1:45646 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:11] Prefill batch, #new-seq: 1, #new-token: 1191, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:11] Decode batch, #running-req: 1, #token: 1271, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.30, #queue-req: 0, 
[2025-12-15 18:26:12] INFO:     127.0.0.1:45650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:12] Prefill batch, #new-seq: 1, #new-token: 25, #cached-token: 228, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:12] Decode batch, #running-req: 1, #token: 274, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.29, #queue-req: 0, 
[2025-12-15 18:26:12] INFO:     127.0.0.1:45654 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:12] Prefill batch, #new-seq: 1, #new-token: 941, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:13] Decode batch, #running-req: 1, #token: 1009, token usage: 0.00, cuda graph: True, gen throughput (token/s): 70.05, #queue-req: 0, 
[2025-12-15 18:26:13] INFO:     127.0.0.1:45658 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:13] Prefill batch, #new-seq: 1, #new-token: 28, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:13] Decode batch, #running-req: 1, #token: 259, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.28, #queue-req: 0, 
[2025-12-15 18:26:14] Decode batch, #running-req: 1, #token: 299, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:26:14] Decode batch, #running-req: 1, #token: 339, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.16, #queue-req: 0, 
[2025-12-15 18:26:14] INFO:     127.0.0.1:45662 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:14] Prefill batch, #new-seq: 1, #new-token: 956, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:15] Decode batch, #running-req: 1, #token: 1051, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.72, #queue-req: 0, 
[2025-12-15 18:26:15] INFO:     127.0.0.1:45666 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:15] Prefill batch, #new-seq: 1, #new-token: 46, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:15] Decode batch, #running-req: 1, #token: 305, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.41, #queue-req: 0, 
[2025-12-15 18:26:16] Decode batch, #running-req: 1, #token: 345, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.14, #queue-req: 0, 
[2025-12-15 18:26:16] INFO:     127.0.0.1:45670 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:16] Prefill batch, #new-seq: 1, #new-token: 1027, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:16] Decode batch, #running-req: 1, #token: 1121, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.77, #queue-req: 0, 
[2025-12-15 18:26:16] INFO:     127.0.0.1:45674 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:16] Prefill batch, #new-seq: 1, #new-token: 53, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:17] INFO:     127.0.0.1:45678 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:17] Prefill batch, #new-seq: 1, #new-token: 1152, #cached-token: 65, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:17] Decode batch, #running-req: 1, #token: 1234, token usage: 0.00, cuda graph: True, gen throughput (token/s): 65.81, #queue-req: 0, 
[2025-12-15 18:26:17] INFO:     127.0.0.1:45682 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:17] Prefill batch, #new-seq: 1, #new-token: 38, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:17] Decode batch, #running-req: 1, #token: 281, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.22, #queue-req: 0, 
[2025-12-15 18:26:18] Decode batch, #running-req: 1, #token: 321, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:26:18] Decode batch, #running-req: 1, #token: 361, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.11, #queue-req: 0, 
[2025-12-15 18:26:19] INFO:     127.0.0.1:45686 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:19] Prefill batch, #new-seq: 1, #new-token: 1158, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:19] Decode batch, #running-req: 1, #token: 1242, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.34, #queue-req: 0, 
[2025-12-15 18:26:19] INFO:     127.0.0.1:45690 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:19] Prefill batch, #new-seq: 1, #new-token: 50, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:19] Decode batch, #running-req: 1, #token: 284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.09, #queue-req: 0, 
[2025-12-15 18:26:20] Decode batch, #running-req: 1, #token: 324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:26:20] INFO:     127.0.0.1:45694 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:20] Prefill batch, #new-seq: 1, #new-token: 1206, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:21] Decode batch, #running-req: 1, #token: 1276, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.37, #queue-req: 0, 
[2025-12-15 18:26:21] INFO:     127.0.0.1:45698 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:21] Prefill batch, #new-seq: 1, #new-token: 44, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:21] Decode batch, #running-req: 1, #token: 274, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.01, #queue-req: 0, 
[2025-12-15 18:26:22] Decode batch, #running-req: 1, #token: 314, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:26:22] Decode batch, #running-req: 1, #token: 354, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.13, #queue-req: 0, 
[2025-12-15 18:26:22] INFO:     127.0.0.1:45704 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:22] Prefill batch, #new-seq: 1, #new-token: 1226, #cached-token: 61, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:23] Decode batch, #running-req: 1, #token: 1308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.26, #queue-req: 0, 
[2025-12-15 18:26:23] INFO:     127.0.0.1:45708 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:23] Prefill batch, #new-seq: 1, #new-token: 17, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:23] Decode batch, #running-req: 1, #token: 248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.17, #queue-req: 0, 
[2025-12-15 18:26:24] Decode batch, #running-req: 1, #token: 288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.26, #queue-req: 0, 
[2025-12-15 18:26:24] INFO:     127.0.0.1:45712 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:24] Prefill batch, #new-seq: 1, #new-token: 1029, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:24] Decode batch, #running-req: 1, #token: 1109, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.78, #queue-req: 0, 
[2025-12-15 18:26:24] INFO:     127.0.0.1:45716 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:25] Prefill batch, #new-seq: 1, #new-token: 39, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:25] INFO:     127.0.0.1:45720 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:25] Prefill batch, #new-seq: 1, #new-token: 1045, #cached-token: 66, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:25] Decode batch, #running-req: 1, #token: 1118, token usage: 0.00, cuda graph: True, gen throughput (token/s): 66.01, #queue-req: 0, 
[2025-12-15 18:26:25] INFO:     127.0.0.1:45724 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:25] Prefill batch, #new-seq: 1, #new-token: 16, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:25] Decode batch, #running-req: 1, #token: 241, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.15, #queue-req: 0, 
[2025-12-15 18:26:26] Decode batch, #running-req: 1, #token: 281, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.31, #queue-req: 0, 
[2025-12-15 18:26:26] INFO:     127.0.0.1:45728 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:26] Prefill batch, #new-seq: 1, #new-token: 886, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:26] Decode batch, #running-req: 1, #token: 954, token usage: 0.00, cuda graph: True, gen throughput (token/s): 71.08, #queue-req: 0, 
[2025-12-15 18:26:27] INFO:     127.0.0.1:45732 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:27] Prefill batch, #new-seq: 1, #new-token: 20, #cached-token: 227, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:27] Decode batch, #running-req: 1, #token: 259, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.23, #queue-req: 0, 
[2025-12-15 18:26:27] Decode batch, #running-req: 1, #token: 299, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:26:27] INFO:     127.0.0.1:45736 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:27] Prefill batch, #new-seq: 1, #new-token: 1086, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:28] INFO:     127.0.0.1:45740 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:28] Prefill batch, #new-seq: 1, #new-token: 35, #cached-token: 223, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:28] Decode batch, #running-req: 1, #token: 262, token usage: 0.00, cuda graph: True, gen throughput (token/s): 65.79, #queue-req: 0, 
[2025-12-15 18:26:29] Decode batch, #running-req: 1, #token: 302, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:26:29] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.10, #queue-req: 0, 
[2025-12-15 18:26:29] INFO:     127.0.0.1:45744 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:29] Prefill batch, #new-seq: 1, #new-token: 990, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:30] Decode batch, #running-req: 1, #token: 1090, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.63, #queue-req: 0, 
[2025-12-15 18:26:30] INFO:     127.0.0.1:45748 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:30] Prefill batch, #new-seq: 1, #new-token: 8, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:30] Decode batch, #running-req: 1, #token: 260, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.55, #queue-req: 0, 
[2025-12-15 18:26:31] Decode batch, #running-req: 1, #token: 300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:26:31] INFO:     127.0.0.1:45752 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:31] Prefill batch, #new-seq: 1, #new-token: 893, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:31] Decode batch, #running-req: 1, #token: 973, token usage: 0.00, cuda graph: True, gen throughput (token/s): 70.99, #queue-req: 0, 
[2025-12-15 18:26:32] INFO:     127.0.0.1:45756 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:32] Prefill batch, #new-seq: 1, #new-token: 40, #cached-token: 225, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:32] Decode batch, #running-req: 1, #token: 281, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.22, #queue-req: 0, 
[2025-12-15 18:26:32] Decode batch, #running-req: 1, #token: 321, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.21, #queue-req: 0, 
[2025-12-15 18:26:33] INFO:     127.0.0.1:45760 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:33] Prefill batch, #new-seq: 1, #new-token: 953, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:33] Decode batch, #running-req: 1, #token: 1013, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.91, #queue-req: 0, 
[2025-12-15 18:26:33] Decode batch, #running-req: 1, #token: 1053, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.80, #queue-req: 0, 
[2025-12-15 18:26:33] INFO:     127.0.0.1:45764 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:33] Prefill batch, #new-seq: 1, #new-token: 11, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:34] Decode batch, #running-req: 1, #token: 265, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.55, #queue-req: 0, 
[2025-12-15 18:26:34] Decode batch, #running-req: 1, #token: 305, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.23, #queue-req: 0, 
[2025-12-15 18:26:35] INFO:     127.0.0.1:45768 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:35] Prefill batch, #new-seq: 1, #new-token: 1045, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:35] Decode batch, #running-req: 1, #token: 1123, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.86, #queue-req: 0, 
[2025-12-15 18:26:35] INFO:     127.0.0.1:45772 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:35] Prefill batch, #new-seq: 1, #new-token: 28, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:35] Decode batch, #running-req: 1, #token: 259, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.25, #queue-req: 0, 
[2025-12-15 18:26:36] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.15, #queue-req: 0, 
[2025-12-15 18:26:36] INFO:     127.0.0.1:45776 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:36] Prefill batch, #new-seq: 1, #new-token: 1172, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:36] INFO:     127.0.0.1:45780 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:37] Prefill batch, #new-seq: 1, #new-token: 15, #cached-token: 223, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:37] Decode batch, #running-req: 1, #token: 241, token usage: 0.00, cuda graph: True, gen throughput (token/s): 65.55, #queue-req: 0, 
[2025-12-15 18:26:37] Decode batch, #running-req: 1, #token: 281, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.30, #queue-req: 0, 
[2025-12-15 18:26:38] Decode batch, #running-req: 1, #token: 321, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:26:38] INFO:     127.0.0.1:45784 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:38] Prefill batch, #new-seq: 1, #new-token: 1057, #cached-token: 61, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:38] Decode batch, #running-req: 1, #token: 1141, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.82, #queue-req: 0, 
[2025-12-15 18:26:38] INFO:     127.0.0.1:45790 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:38] Prefill batch, #new-seq: 1, #new-token: 23, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:39] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.33, #queue-req: 0, 
[2025-12-15 18:26:39] INFO:     127.0.0.1:45794 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:39] Prefill batch, #new-seq: 1, #new-token: 1020, #cached-token: 61, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:39] Decode batch, #running-req: 1, #token: 1089, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.78, #queue-req: 0, 
[2025-12-15 18:26:40] INFO:     127.0.0.1:45798 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:40] Prefill batch, #new-seq: 1, #new-token: 21, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:40] Decode batch, #running-req: 1, #token: 248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.22, #queue-req: 0, 
[2025-12-15 18:26:40] Decode batch, #running-req: 1, #token: 288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.27, #queue-req: 0, 
[2025-12-15 18:26:40] INFO:     127.0.0.1:45802 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:40] Prefill batch, #new-seq: 1, #new-token: 907, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:41] Decode batch, #running-req: 1, #token: 993, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.88, #queue-req: 0, 
[2025-12-15 18:26:41] INFO:     127.0.0.1:45806 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:41] Prefill batch, #new-seq: 1, #new-token: 36, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:41] Decode batch, #running-req: 1, #token: 279, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.37, #queue-req: 0, 
[2025-12-15 18:26:42] Decode batch, #running-req: 1, #token: 319, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:26:42] INFO:     127.0.0.1:45810 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:42] Prefill batch, #new-seq: 1, #new-token: 1158, #cached-token: 62, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:42] Decode batch, #running-req: 1, #token: 1241, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.38, #queue-req: 0, 
[2025-12-15 18:26:43] Decode batch, #running-req: 1, #token: 1281, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.76, #queue-req: 0, 
[2025-12-15 18:26:43] Decode batch, #running-req: 1, #token: 1321, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.82, #queue-req: 0, 
[2025-12-15 18:26:44] Decode batch, #running-req: 1, #token: 1361, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.82, #queue-req: 0, 
[2025-12-15 18:26:44] Decode batch, #running-req: 1, #token: 1401, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.77, #queue-req: 0, 
[2025-12-15 18:26:45] Decode batch, #running-req: 1, #token: 1441, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.75, #queue-req: 0, 
[2025-12-15 18:26:45] Decode batch, #running-req: 1, #token: 1481, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.68, #queue-req: 0, 
[2025-12-15 18:26:46] Decode batch, #running-req: 1, #token: 1521, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.66, #queue-req: 0, 
[2025-12-15 18:26:46] Decode batch, #running-req: 1, #token: 1561, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.65, #queue-req: 0, 
[2025-12-15 18:26:47] Decode batch, #running-req: 1, #token: 1601, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.64, #queue-req: 0, 
[2025-12-15 18:26:47] Decode batch, #running-req: 1, #token: 1641, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.63, #queue-req: 0, 
[2025-12-15 18:26:48] Decode batch, #running-req: 1, #token: 1681, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.61, #queue-req: 0, 
[2025-12-15 18:26:48] Decode batch, #running-req: 1, #token: 1721, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.58, #queue-req: 0, 
[2025-12-15 18:26:49] Decode batch, #running-req: 1, #token: 1761, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:26:49] Decode batch, #running-req: 1, #token: 1801, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.58, #queue-req: 0, 
[2025-12-15 18:26:50] Decode batch, #running-req: 1, #token: 1841, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.59, #queue-req: 0, 
[2025-12-15 18:26:50] Decode batch, #running-req: 1, #token: 1881, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:26:51] Decode batch, #running-req: 1, #token: 1921, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.55, #queue-req: 0, 
[2025-12-15 18:26:51] Decode batch, #running-req: 1, #token: 1961, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.54, #queue-req: 0, 
[2025-12-15 18:26:52] Decode batch, #running-req: 1, #token: 2001, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:26:52] Decode batch, #running-req: 1, #token: 2041, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.52, #queue-req: 0, 
[2025-12-15 18:26:53] Decode batch, #running-req: 1, #token: 2081, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.48, #queue-req: 0, 
[2025-12-15 18:26:53] Decode batch, #running-req: 1, #token: 2121, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.48, #queue-req: 0, 
[2025-12-15 18:26:54] Decode batch, #running-req: 1, #token: 2161, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.45, #queue-req: 0, 
[2025-12-15 18:26:54] Decode batch, #running-req: 1, #token: 2201, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.43, #queue-req: 0, 
[2025-12-15 18:26:55] INFO:     127.0.0.1:45814 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:55] Prefill batch, #new-seq: 1, #new-token: 25, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:55] Decode batch, #running-req: 1, #token: 265, token usage: 0.00, cuda graph: True, gen throughput (token/s): 75.88, #queue-req: 0, 
[2025-12-15 18:26:56] Decode batch, #running-req: 1, #token: 305, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.21, #queue-req: 0, 
[2025-12-15 18:26:56] Decode batch, #running-req: 1, #token: 345, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.14, #queue-req: 0, 
[2025-12-15 18:26:56] INFO:     127.0.0.1:45820 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:26:56] Prefill batch, #new-seq: 1, #new-token: 1089, #cached-token: 62, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:26:57] Decode batch, #running-req: 1, #token: 1183, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.58, #queue-req: 0, 
[2025-12-15 18:26:57] Decode batch, #running-req: 1, #token: 1223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.77, #queue-req: 0, 
[2025-12-15 18:26:58] Decode batch, #running-req: 1, #token: 1263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.80, #queue-req: 0, 
[2025-12-15 18:26:58] Decode batch, #running-req: 1, #token: 1303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.81, #queue-req: 0, 
[2025-12-15 18:26:59] Decode batch, #running-req: 1, #token: 1343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.81, #queue-req: 0, 
[2025-12-15 18:26:59] Decode batch, #running-req: 1, #token: 1383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.79, #queue-req: 0, 
[2025-12-15 18:27:00] Decode batch, #running-req: 1, #token: 1423, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.78, #queue-req: 0, 
[2025-12-15 18:27:00] Decode batch, #running-req: 1, #token: 1463, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.76, #queue-req: 0, 
[2025-12-15 18:27:01] Decode batch, #running-req: 1, #token: 1503, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.75, #queue-req: 0, 
[2025-12-15 18:27:01] INFO:     127.0.0.1:45824 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:01] Prefill batch, #new-seq: 1, #new-token: 23, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:01] Decode batch, #running-req: 1, #token: 275, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.39, #queue-req: 0, 
[2025-12-15 18:27:02] Decode batch, #running-req: 1, #token: 315, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:27:02] INFO:     127.0.0.1:45828 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:02] Prefill batch, #new-seq: 1, #new-token: 1108, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:02] Decode batch, #running-req: 1, #token: 1191, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.61, #queue-req: 0, 
[2025-12-15 18:27:02] INFO:     127.0.0.1:45834 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:02] Prefill batch, #new-seq: 1, #new-token: 15, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:03] Decode batch, #running-req: 1, #token: 257, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.41, #queue-req: 0, 
[2025-12-15 18:27:03] INFO:     127.0.0.1:45838 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:03] Prefill batch, #new-seq: 1, #new-token: 938, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:03] Decode batch, #running-req: 1, #token: 1001, token usage: 0.00, cuda graph: True, gen throughput (token/s): 70.07, #queue-req: 0, 
[2025-12-15 18:27:04] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.70, #queue-req: 0, 
[2025-12-15 18:27:04] INFO:     127.0.0.1:45842 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:04] Prefill batch, #new-seq: 1, #new-token: 10, #cached-token: 222, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:04] INFO:     127.0.0.1:45846 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:04] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.47, #queue-req: 0, 
[2025-12-15 18:27:04] Prefill batch, #new-seq: 1, #new-token: 1004, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:05] Decode batch, #running-req: 1, #token: 1101, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.76, #queue-req: 0, 
[2025-12-15 18:27:05] INFO:     127.0.0.1:45850 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:05] Prefill batch, #new-seq: 1, #new-token: 17, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:05] Decode batch, #running-req: 1, #token: 268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.60, #queue-req: 0, 
[2025-12-15 18:27:06] INFO:     127.0.0.1:45854 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:06] Prefill batch, #new-seq: 1, #new-token: 889, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:06] Decode batch, #running-req: 1, #token: 969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 71.03, #queue-req: 0, 
[2025-12-15 18:27:06] INFO:     127.0.0.1:45858 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:06] Prefill batch, #new-seq: 1, #new-token: 8, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:07] Decode batch, #running-req: 1, #token: 251, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.47, #queue-req: 0, 
[2025-12-15 18:27:07] Decode batch, #running-req: 1, #token: 291, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.24, #queue-req: 0, 
[2025-12-15 18:27:08] Decode batch, #running-req: 1, #token: 331, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:27:08] INFO:     127.0.0.1:45862 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:08] Prefill batch, #new-seq: 1, #new-token: 1105, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:08] Decode batch, #running-req: 1, #token: 1203, token usage: 0.00, cuda graph: True, gen throughput (token/s): 65.25, #queue-req: 0, 
[2025-12-15 18:27:09] Decode batch, #running-req: 1, #token: 1243, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.70, #queue-req: 0, 
[2025-12-15 18:27:09] Decode batch, #running-req: 1, #token: 1283, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.70, #queue-req: 0, 
[2025-12-15 18:27:10] Decode batch, #running-req: 1, #token: 1323, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.71, #queue-req: 0, 
[2025-12-15 18:27:10] Decode batch, #running-req: 1, #token: 1363, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.69, #queue-req: 0, 
[2025-12-15 18:27:11] Decode batch, #running-req: 1, #token: 1403, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.68, #queue-req: 0, 
[2025-12-15 18:27:11] Decode batch, #running-req: 1, #token: 1443, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.68, #queue-req: 0, 
[2025-12-15 18:27:12] Decode batch, #running-req: 1, #token: 1483, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.67, #queue-req: 0, 
[2025-12-15 18:27:12] Decode batch, #running-req: 1, #token: 1523, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.66, #queue-req: 0, 
[2025-12-15 18:27:13] Decode batch, #running-req: 1, #token: 1563, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.65, #queue-req: 0, 
[2025-12-15 18:27:13] Decode batch, #running-req: 1, #token: 1603, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.65, #queue-req: 0, 
[2025-12-15 18:27:14] Decode batch, #running-req: 1, #token: 1643, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.62, #queue-req: 0, 
[2025-12-15 18:27:14] Decode batch, #running-req: 1, #token: 1683, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.61, #queue-req: 0, 
[2025-12-15 18:27:15] Decode batch, #running-req: 1, #token: 1723, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.60, #queue-req: 0, 
[2025-12-15 18:27:15] Decode batch, #running-req: 1, #token: 1763, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:27:16] Decode batch, #running-req: 1, #token: 1803, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:27:16] Decode batch, #running-req: 1, #token: 1843, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.58, #queue-req: 0, 
[2025-12-15 18:27:17] Decode batch, #running-req: 1, #token: 1883, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:27:17] Decode batch, #running-req: 1, #token: 1923, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.55, #queue-req: 0, 
[2025-12-15 18:27:18] Decode batch, #running-req: 1, #token: 1963, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.54, #queue-req: 0, 
[2025-12-15 18:27:18] Decode batch, #running-req: 1, #token: 2003, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.53, #queue-req: 0, 
[2025-12-15 18:27:19] Decode batch, #running-req: 1, #token: 2043, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.51, #queue-req: 0, 
[2025-12-15 18:27:19] Decode batch, #running-req: 1, #token: 2083, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.47, #queue-req: 0, 
[2025-12-15 18:27:20] Decode batch, #running-req: 1, #token: 2123, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.45, #queue-req: 0, 
[2025-12-15 18:27:20] Decode batch, #running-req: 1, #token: 2163, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.44, #queue-req: 0, 
[2025-12-15 18:27:20] INFO:     127.0.0.1:45866 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:20] Prefill batch, #new-seq: 1, #new-token: 44, #cached-token: 226, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:21] Decode batch, #running-req: 1, #token: 308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 75.96, #queue-req: 0, 
[2025-12-15 18:27:21] INFO:     127.0.0.1:45874 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:21] Prefill batch, #new-seq: 1, #new-token: 1144, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:21] Decode batch, #running-req: 1, #token: 1222, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.65, #queue-req: 0, 
[2025-12-15 18:27:22] INFO:     127.0.0.1:45878 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:22] Prefill batch, #new-seq: 1, #new-token: 16, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:22] Decode batch, #running-req: 1, #token: 249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.25, #queue-req: 0, 
[2025-12-15 18:27:22] INFO:     127.0.0.1:45882 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:22] Prefill batch, #new-seq: 1, #new-token: 1013, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:22] Decode batch, #running-req: 1, #token: 1075, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.95, #queue-req: 0, 
[2025-12-15 18:27:23] Decode batch, #running-req: 1, #token: 1115, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.74, #queue-req: 0, 
[2025-12-15 18:27:23] INFO:     127.0.0.1:45886 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:23] Prefill batch, #new-seq: 1, #new-token: 23, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:23] Decode batch, #running-req: 1, #token: 279, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.58, #queue-req: 0, 
[2025-12-15 18:27:24] Decode batch, #running-req: 1, #token: 319, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:27:24] INFO:     127.0.0.1:45890 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:24] Prefill batch, #new-seq: 1, #new-token: 1168, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:25] Decode batch, #running-req: 1, #token: 1241, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.39, #queue-req: 0, 
[2025-12-15 18:27:25] INFO:     127.0.0.1:45894 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:25] Prefill batch, #new-seq: 1, #new-token: 14, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:25] Decode batch, #running-req: 1, #token: 247, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.16, #queue-req: 0, 
[2025-12-15 18:27:26] Decode batch, #running-req: 1, #token: 287, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.27, #queue-req: 0, 
[2025-12-15 18:27:26] INFO:     127.0.0.1:45898 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:26] Prefill batch, #new-seq: 1, #new-token: 1142, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:26] Decode batch, #running-req: 1, #token: 1205, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.84, #queue-req: 0, 
[2025-12-15 18:27:27] INFO:     127.0.0.1:45902 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:27] Prefill batch, #new-seq: 1, #new-token: 28, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:27] Decode batch, #running-req: 1, #token: 250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.00, #queue-req: 0, 
[2025-12-15 18:27:27] Decode batch, #running-req: 1, #token: 290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.25, #queue-req: 0, 
[2025-12-15 18:27:27] INFO:     127.0.0.1:45906 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:27] Prefill batch, #new-seq: 1, #new-token: 1043, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:28] Decode batch, #running-req: 1, #token: 1128, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.81, #queue-req: 0, 
[2025-12-15 18:27:28] INFO:     127.0.0.1:45910 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:28] Prefill batch, #new-seq: 1, #new-token: 17, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:28] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.38, #queue-req: 0, 
[2025-12-15 18:27:29] Decode batch, #running-req: 1, #token: 296, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.12, #queue-req: 0, 
[2025-12-15 18:27:29] Decode batch, #running-req: 1, #token: 336, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.09, #queue-req: 0, 
[2025-12-15 18:27:30] Decode batch, #running-req: 1, #token: 376, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.00, #queue-req: 0, 
[2025-12-15 18:27:30] Decode batch, #running-req: 1, #token: 416, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.96, #queue-req: 0, 
[2025-12-15 18:27:31] Decode batch, #running-req: 1, #token: 456, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.92, #queue-req: 0, 
[2025-12-15 18:27:31] Decode batch, #running-req: 1, #token: 496, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.92, #queue-req: 0, 
[2025-12-15 18:27:32] Decode batch, #running-req: 1, #token: 536, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.89, #queue-req: 0, 
[2025-12-15 18:27:32] Decode batch, #running-req: 1, #token: 576, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.88, #queue-req: 0, 
[2025-12-15 18:27:33] Decode batch, #running-req: 1, #token: 616, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.87, #queue-req: 0, 
[2025-12-15 18:27:33] Decode batch, #running-req: 1, #token: 656, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.87, #queue-req: 0, 
[2025-12-15 18:27:34] Decode batch, #running-req: 1, #token: 696, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.84, #queue-req: 0, 
[2025-12-15 18:27:34] Decode batch, #running-req: 1, #token: 736, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.84, #queue-req: 0, 
[2025-12-15 18:27:35] Decode batch, #running-req: 1, #token: 776, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.83, #queue-req: 0, 
[2025-12-15 18:27:35] Decode batch, #running-req: 1, #token: 816, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.85, #queue-req: 0, 
[2025-12-15 18:27:36] Decode batch, #running-req: 1, #token: 856, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.84, #queue-req: 0, 
[2025-12-15 18:27:36] Decode batch, #running-req: 1, #token: 896, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.83, #queue-req: 0, 
[2025-12-15 18:27:37] Decode batch, #running-req: 1, #token: 936, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.82, #queue-req: 0, 
[2025-12-15 18:27:37] Decode batch, #running-req: 1, #token: 976, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.81, #queue-req: 0, 
[2025-12-15 18:27:38] Decode batch, #running-req: 1, #token: 1016, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.81, #queue-req: 0, 
[2025-12-15 18:27:38] Decode batch, #running-req: 1, #token: 1056, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.77, #queue-req: 0, 
[2025-12-15 18:27:39] Decode batch, #running-req: 1, #token: 1096, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.74, #queue-req: 0, 
[2025-12-15 18:27:39] Decode batch, #running-req: 1, #token: 1136, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.74, #queue-req: 0, 
[2025-12-15 18:27:40] Decode batch, #running-req: 1, #token: 1176, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.73, #queue-req: 0, 
[2025-12-15 18:27:40] Decode batch, #running-req: 1, #token: 1216, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.72, #queue-req: 0, 
[2025-12-15 18:27:41] INFO:     127.0.0.1:46378 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:41] Prefill batch, #new-seq: 1, #new-token: 902, #cached-token: 65, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:41] Decode batch, #running-req: 1, #token: 986, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.49, #queue-req: 0, 
[2025-12-15 18:27:41] INFO:     127.0.0.1:46382 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:41] Prefill batch, #new-seq: 1, #new-token: 13, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:41] Decode batch, #running-req: 1, #token: 253, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.40, #queue-req: 0, 
[2025-12-15 18:27:42] Decode batch, #running-req: 1, #token: 293, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.24, #queue-req: 0, 
[2025-12-15 18:27:42] Decode batch, #running-req: 1, #token: 333, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:27:43] INFO:     127.0.0.1:46386 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:43] Prefill batch, #new-seq: 1, #new-token: 1094, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:43] Decode batch, #running-req: 1, #token: 1180, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.64, #queue-req: 0, 
[2025-12-15 18:27:43] INFO:     127.0.0.1:46390 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:43] Prefill batch, #new-seq: 1, #new-token: 17, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:43] Decode batch, #running-req: 1, #token: 261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.51, #queue-req: 0, 
[2025-12-15 18:27:44] Decode batch, #running-req: 1, #token: 301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:27:44] INFO:     127.0.0.1:46394 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:44] Prefill batch, #new-seq: 1, #new-token: 1005, #cached-token: 61, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:45] Decode batch, #running-req: 1, #token: 1068, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.91, #queue-req: 0, 
[2025-12-15 18:27:45] Decode batch, #running-req: 1, #token: 1108, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.71, #queue-req: 0, 
[2025-12-15 18:27:45] INFO:     127.0.0.1:46398 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:45] Prefill batch, #new-seq: 1, #new-token: 46, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:46] Decode batch, #running-req: 1, #token: 301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.38, #queue-req: 0, 
[2025-12-15 18:27:46] INFO:     127.0.0.1:46402 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:46] Prefill batch, #new-seq: 1, #new-token: 996, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:46] Decode batch, #running-req: 1, #token: 1058, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.88, #queue-req: 0, 
[2025-12-15 18:27:47] INFO:     127.0.0.1:46406 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:47] Prefill batch, #new-seq: 1, #new-token: 25, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:47] Decode batch, #running-req: 1, #token: 254, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.19, #queue-req: 0, 
[2025-12-15 18:27:47] Decode batch, #running-req: 1, #token: 294, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.25, #queue-req: 0, 
[2025-12-15 18:27:47] INFO:     127.0.0.1:46412 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:47] Prefill batch, #new-seq: 1, #new-token: 1249, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:48] Decode batch, #running-req: 1, #token: 1330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.30, #queue-req: 0, 
[2025-12-15 18:27:48] INFO:     127.0.0.1:46416 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:48] Prefill batch, #new-seq: 1, #new-token: 19, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:48] Decode batch, #running-req: 1, #token: 268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.42, #queue-req: 0, 
[2025-12-15 18:27:49] Decode batch, #running-req: 1, #token: 308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.21, #queue-req: 0, 
[2025-12-15 18:27:49] INFO:     127.0.0.1:46420 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:49] Prefill batch, #new-seq: 1, #new-token: 996, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:49] Decode batch, #running-req: 1, #token: 1070, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.74, #queue-req: 0, 
[2025-12-15 18:27:50] INFO:     127.0.0.1:46424 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:50] Prefill batch, #new-seq: 1, #new-token: 31, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:50] Decode batch, #running-req: 1, #token: 259, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.24, #queue-req: 0, 
[2025-12-15 18:27:50] Decode batch, #running-req: 1, #token: 299, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:27:51] Decode batch, #running-req: 1, #token: 339, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:27:51] INFO:     127.0.0.1:46428 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:51] Prefill batch, #new-seq: 1, #new-token: 1171, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:51] Decode batch, #running-req: 1, #token: 1259, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.27, #queue-req: 0, 
[2025-12-15 18:27:52] INFO:     127.0.0.1:46432 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:52] Prefill batch, #new-seq: 1, #new-token: 16, #cached-token: 223, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:52] Decode batch, #running-req: 1, #token: 267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.30, #queue-req: 0, 
[2025-12-15 18:27:52] Decode batch, #running-req: 1, #token: 307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:27:53] INFO:     127.0.0.1:46436 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:53] Prefill batch, #new-seq: 1, #new-token: 1210, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:53] Decode batch, #running-req: 1, #token: 1291, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.30, #queue-req: 0, 
[2025-12-15 18:27:53] INFO:     127.0.0.1:46440 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:53] Prefill batch, #new-seq: 1, #new-token: 42, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:54] Decode batch, #running-req: 1, #token: 282, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.25, #queue-req: 0, 
[2025-12-15 18:27:54] Decode batch, #running-req: 1, #token: 322, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:27:54] INFO:     127.0.0.1:46444 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:54] Prefill batch, #new-seq: 1, #new-token: 1162, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:55] Decode batch, #running-req: 1, #token: 1228, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.47, #queue-req: 0, 
[2025-12-15 18:27:55] INFO:     127.0.0.1:46448 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:55] Prefill batch, #new-seq: 1, #new-token: 23, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:55] Decode batch, #running-req: 1, #token: 251, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.14, #queue-req: 0, 
[2025-12-15 18:27:56] Decode batch, #running-req: 1, #token: 291, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.24, #queue-req: 0, 
[2025-12-15 18:27:56] INFO:     127.0.0.1:46452 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:56] Prefill batch, #new-seq: 1, #new-token: 878, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:56] Decode batch, #running-req: 1, #token: 950, token usage: 0.00, cuda graph: True, gen throughput (token/s): 71.06, #queue-req: 0, 
[2025-12-15 18:27:57] INFO:     127.0.0.1:46456 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:57] Prefill batch, #new-seq: 1, #new-token: 29, #cached-token: 223, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:57] Decode batch, #running-req: 1, #token: 258, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.19, #queue-req: 0, 
[2025-12-15 18:27:57] Decode batch, #running-req: 1, #token: 298, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:27:58] INFO:     127.0.0.1:46460 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:58] Prefill batch, #new-seq: 1, #new-token: 1168, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:58] Decode batch, #running-req: 1, #token: 1231, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.56, #queue-req: 0, 
[2025-12-15 18:27:58] Decode batch, #running-req: 1, #token: 1271, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.64, #queue-req: 0, 
[2025-12-15 18:27:58] INFO:     127.0.0.1:46464 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:58] Prefill batch, #new-seq: 1, #new-token: 11, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:27:59] Decode batch, #running-req: 1, #token: 264, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.56, #queue-req: 0, 
[2025-12-15 18:27:59] Decode batch, #running-req: 1, #token: 304, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:27:59] INFO:     127.0.0.1:46468 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:27:59] Prefill batch, #new-seq: 1, #new-token: 941, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:00] Decode batch, #running-req: 1, #token: 1032, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.80, #queue-req: 0, 
[2025-12-15 18:28:00] Decode batch, #running-req: 1, #token: 1072, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.83, #queue-req: 0, 
[2025-12-15 18:28:01] Decode batch, #running-req: 1, #token: 1112, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.85, #queue-req: 0, 
[2025-12-15 18:28:01] Decode batch, #running-req: 1, #token: 1152, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.83, #queue-req: 0, 
[2025-12-15 18:28:02] Decode batch, #running-req: 1, #token: 1192, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.81, #queue-req: 0, 
[2025-12-15 18:28:02] INFO:     127.0.0.1:46472 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:02] Prefill batch, #new-seq: 1, #new-token: 22, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:02] Decode batch, #running-req: 1, #token: 271, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.52, #queue-req: 0, 
[2025-12-15 18:28:03] INFO:     127.0.0.1:46478 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:03] Prefill batch, #new-seq: 1, #new-token: 929, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:03] Decode batch, #running-req: 1, #token: 1001, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.97, #queue-req: 0, 
[2025-12-15 18:28:03] INFO:     127.0.0.1:46482 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:03] Prefill batch, #new-seq: 1, #new-token: 32, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:04] Decode batch, #running-req: 1, #token: 268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.30, #queue-req: 0, 
[2025-12-15 18:28:04] Decode batch, #running-req: 1, #token: 308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.24, #queue-req: 0, 
[2025-12-15 18:28:05] Decode batch, #running-req: 1, #token: 348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:28:05] INFO:     127.0.0.1:46486 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:05] Prefill batch, #new-seq: 1, #new-token: 1253, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:05] Decode batch, #running-req: 1, #token: 1349, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.08, #queue-req: 0, 
[2025-12-15 18:28:05] INFO:     127.0.0.1:46490 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:05] Prefill batch, #new-seq: 1, #new-token: 19, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:06] Decode batch, #running-req: 1, #token: 269, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.51, #queue-req: 0, 
[2025-12-15 18:28:06] INFO:     127.0.0.1:46494 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:06] Prefill batch, #new-seq: 1, #new-token: 1116, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:06] Decode batch, #running-req: 1, #token: 1196, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.74, #queue-req: 0, 
[2025-12-15 18:28:07] INFO:     127.0.0.1:46498 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:07] Prefill batch, #new-seq: 1, #new-token: 12, #cached-token: 225, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:07] Decode batch, #running-req: 1, #token: 257, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.33, #queue-req: 0, 
[2025-12-15 18:28:07] Decode batch, #running-req: 1, #token: 297, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:28:08] INFO:     127.0.0.1:46502 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:08] Prefill batch, #new-seq: 1, #new-token: 893, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:08] Decode batch, #running-req: 1, #token: 966, token usage: 0.00, cuda graph: True, gen throughput (token/s): 71.05, #queue-req: 0, 
[2025-12-15 18:28:08] INFO:     127.0.0.1:46506 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:08] Prefill batch, #new-seq: 1, #new-token: 26, #cached-token: 223, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:08] Decode batch, #running-req: 1, #token: 265, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.31, #queue-req: 0, 
[2025-12-15 18:28:09] Decode batch, #running-req: 1, #token: 305, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:28:09] INFO:     127.0.0.1:46510 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:09] Prefill batch, #new-seq: 1, #new-token: 1040, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:09] Decode batch, #running-req: 1, #token: 1103, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.94, #queue-req: 0, 
[2025-12-15 18:28:10] INFO:     127.0.0.1:46514 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:10] Prefill batch, #new-seq: 1, #new-token: 21, #cached-token: 222, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:10] Decode batch, #running-req: 1, #token: 248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.09, #queue-req: 0, 
[2025-12-15 18:28:10] Decode batch, #running-req: 1, #token: 288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.28, #queue-req: 0, 
[2025-12-15 18:28:11] Decode batch, #running-req: 1, #token: 328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:28:11] INFO:     127.0.0.1:46518 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:11] Prefill batch, #new-seq: 1, #new-token: 1072, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:12] Decode batch, #running-req: 1, #token: 1157, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.73, #queue-req: 0, 
[2025-12-15 18:28:12] INFO:     127.0.0.1:46524 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:12] Prefill batch, #new-seq: 1, #new-token: 20, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:12] Decode batch, #running-req: 1, #token: 259, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.32, #queue-req: 0, 
[2025-12-15 18:28:13] Decode batch, #running-req: 1, #token: 299, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:28:13] INFO:     127.0.0.1:46528 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:13] Prefill batch, #new-seq: 1, #new-token: 1052, #cached-token: 61, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:13] Decode batch, #running-req: 1, #token: 1117, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.97, #queue-req: 0, 
[2025-12-15 18:28:14] Decode batch, #running-req: 1, #token: 1157, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.71, #queue-req: 0, 
[2025-12-15 18:28:14] INFO:     127.0.0.1:46532 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:14] Prefill batch, #new-seq: 1, #new-token: 25, #cached-token: 222, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:14] Decode batch, #running-req: 1, #token: 284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.37, #queue-req: 0, 
[2025-12-15 18:28:15] Decode batch, #running-req: 1, #token: 324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.23, #queue-req: 0, 
[2025-12-15 18:28:15] Decode batch, #running-req: 1, #token: 364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.10, #queue-req: 0, 
[2025-12-15 18:28:15] INFO:     127.0.0.1:46536 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:15] Prefill batch, #new-seq: 1, #new-token: 1145, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:16] Decode batch, #running-req: 1, #token: 1237, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.53, #queue-req: 0, 
[2025-12-15 18:28:16] INFO:     127.0.0.1:46540 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:16] Prefill batch, #new-seq: 1, #new-token: 6, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:16] Decode batch, #running-req: 1, #token: 258, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.50, #queue-req: 0, 
[2025-12-15 18:28:16] INFO:     127.0.0.1:46544 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:16] Prefill batch, #new-seq: 1, #new-token: 874, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:17] Decode batch, #running-req: 1, #token: 956, token usage: 0.00, cuda graph: True, gen throughput (token/s): 70.93, #queue-req: 0, 
[2025-12-15 18:28:17] INFO:     127.0.0.1:46548 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:17] Prefill batch, #new-seq: 1, #new-token: 15, #cached-token: 222, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:17] Decode batch, #running-req: 1, #token: 261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.44, #queue-req: 0, 
[2025-12-15 18:28:18] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.12, #queue-req: 0, 
[2025-12-15 18:28:18] INFO:     127.0.0.1:46552 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:18] Prefill batch, #new-seq: 1, #new-token: 855, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:18] Decode batch, #running-req: 1, #token: 950, token usage: 0.00, cuda graph: True, gen throughput (token/s): 70.94, #queue-req: 0, 
[2025-12-15 18:28:19] INFO:     127.0.0.1:46556 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:19] Prefill batch, #new-seq: 1, #new-token: 69, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:19] Decode batch, #running-req: 1, #token: 321, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.35, #queue-req: 0, 
[2025-12-15 18:28:19] Decode batch, #running-req: 1, #token: 361, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.12, #queue-req: 0, 
[2025-12-15 18:28:20] INFO:     127.0.0.1:46560 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:20] Prefill batch, #new-seq: 1, #new-token: 1283, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:20] Decode batch, #running-req: 1, #token: 1351, token usage: 0.00, cuda graph: True, gen throughput (token/s): 67.54, #queue-req: 0, 
[2025-12-15 18:28:20] INFO:     127.0.0.1:46564 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:20] Prefill batch, #new-seq: 1, #new-token: 46, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:21] Decode batch, #running-req: 1, #token: 274, token usage: 0.00, cuda graph: True, gen throughput (token/s): 75.97, #queue-req: 0, 
[2025-12-15 18:28:21] Decode batch, #running-req: 1, #token: 314, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.21, #queue-req: 0, 
[2025-12-15 18:28:22] Decode batch, #running-req: 1, #token: 354, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.13, #queue-req: 0, 
[2025-12-15 18:28:22] INFO:     127.0.0.1:46568 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:22] Prefill batch, #new-seq: 1, #new-token: 1354, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:22] Decode batch, #running-req: 1, #token: 1442, token usage: 0.00, cuda graph: True, gen throughput (token/s): 67.17, #queue-req: 0, 
[2025-12-15 18:28:22] INFO:     127.0.0.1:46572 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:22] Prefill batch, #new-seq: 1, #new-token: 56, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:23] Decode batch, #running-req: 1, #token: 297, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.18, #queue-req: 0, 
[2025-12-15 18:28:23] Decode batch, #running-req: 1, #token: 337, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:28:23] INFO:     127.0.0.1:46576 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:23] Prefill batch, #new-seq: 1, #new-token: 1328, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:24] Decode batch, #running-req: 1, #token: 1412, token usage: 0.00, cuda graph: True, gen throughput (token/s): 67.29, #queue-req: 0, 
[2025-12-15 18:28:24] INFO:     127.0.0.1:46580 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:24] Prefill batch, #new-seq: 1, #new-token: 30, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:24] Decode batch, #running-req: 1, #token: 279, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.33, #queue-req: 0, 
[2025-12-15 18:28:25] Decode batch, #running-req: 1, #token: 319, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:28:25] INFO:     127.0.0.1:46584 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:25] Prefill batch, #new-seq: 1, #new-token: 1097, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:25] Decode batch, #running-req: 1, #token: 1177, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.72, #queue-req: 0, 
[2025-12-15 18:28:26] INFO:     127.0.0.1:46588 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:26] Prefill batch, #new-seq: 1, #new-token: 54, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:26] Decode batch, #running-req: 1, #token: 287, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.13, #queue-req: 0, 
[2025-12-15 18:28:26] Decode batch, #running-req: 1, #token: 327, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:28:27] Decode batch, #running-req: 1, #token: 367, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.09, #queue-req: 0, 
[2025-12-15 18:28:27] INFO:     127.0.0.1:46592 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:27] Prefill batch, #new-seq: 1, #new-token: 1177, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:28] Decode batch, #running-req: 1, #token: 1267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.26, #queue-req: 0, 
[2025-12-15 18:28:28] INFO:     127.0.0.1:46596 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:28] Prefill batch, #new-seq: 1, #new-token: 43, #cached-token: 223, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:28] Decode batch, #running-req: 1, #token: 289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.21, #queue-req: 0, 
[2025-12-15 18:28:29] Decode batch, #running-req: 1, #token: 329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:28:29] INFO:     127.0.0.1:46600 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:29] Prefill batch, #new-seq: 1, #new-token: 1168, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:29] Decode batch, #running-req: 1, #token: 1252, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.32, #queue-req: 0, 
[2025-12-15 18:28:29] INFO:     127.0.0.1:46604 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:29] Prefill batch, #new-seq: 1, #new-token: 17, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:30] Decode batch, #running-req: 1, #token: 257, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.39, #queue-req: 0, 
[2025-12-15 18:28:30] Decode batch, #running-req: 1, #token: 297, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:28:30] INFO:     127.0.0.1:46608 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:31] Prefill batch, #new-seq: 1, #new-token: 1074, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:31] Decode batch, #running-req: 1, #token: 1146, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.87, #queue-req: 0, 
[2025-12-15 18:28:31] INFO:     127.0.0.1:46612 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:31] Prefill batch, #new-seq: 1, #new-token: 41, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:31] Decode batch, #running-req: 1, #token: 272, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.20, #queue-req: 0, 
[2025-12-15 18:28:32] Decode batch, #running-req: 1, #token: 312, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.21, #queue-req: 0, 
[2025-12-15 18:28:32] Decode batch, #running-req: 1, #token: 352, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.13, #queue-req: 0, 
[2025-12-15 18:28:32] INFO:     127.0.0.1:46618 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:32] Prefill batch, #new-seq: 1, #new-token: 1166, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:33] Decode batch, #running-req: 1, #token: 1259, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.26, #queue-req: 0, 
[2025-12-15 18:28:33] INFO:     127.0.0.1:46622 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:33] Prefill batch, #new-seq: 1, #new-token: 22, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:33] Decode batch, #running-req: 1, #token: 272, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.45, #queue-req: 0, 
[2025-12-15 18:28:34] Decode batch, #running-req: 1, #token: 312, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.21, #queue-req: 0, 
[2025-12-15 18:28:34] INFO:     127.0.0.1:46626 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:34] Prefill batch, #new-seq: 1, #new-token: 1177, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:34] Decode batch, #running-req: 1, #token: 1264, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.36, #queue-req: 0, 
[2025-12-15 18:28:35] INFO:     127.0.0.1:46630 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:35] Prefill batch, #new-seq: 1, #new-token: 48, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:35] Decode batch, #running-req: 1, #token: 299, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.28, #queue-req: 0, 
[2025-12-15 18:28:35] Decode batch, #running-req: 1, #token: 339, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:28:35] INFO:     127.0.0.1:46634 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:36] Prefill batch, #new-seq: 1, #new-token: 1214, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:36] Decode batch, #running-req: 1, #token: 1308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.12, #queue-req: 0, 
[2025-12-15 18:28:36] INFO:     127.0.0.1:46638 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:36] Prefill batch, #new-seq: 1, #new-token: 10, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:37] Decode batch, #running-req: 1, #token: 261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.45, #queue-req: 0, 
[2025-12-15 18:28:37] Decode batch, #running-req: 1, #token: 301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.21, #queue-req: 0, 
[2025-12-15 18:28:37] INFO:     127.0.0.1:46644 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:37] Prefill batch, #new-seq: 1, #new-token: 1053, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:38] Decode batch, #running-req: 1, #token: 1132, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.86, #queue-req: 0, 
[2025-12-15 18:28:38] INFO:     127.0.0.1:46648 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:38] Prefill batch, #new-seq: 1, #new-token: 70, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:38] Decode batch, #running-req: 1, #token: 307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.14, #queue-req: 0, 
[2025-12-15 18:28:39] Decode batch, #running-req: 1, #token: 347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.14, #queue-req: 0, 
[2025-12-15 18:28:39] INFO:     127.0.0.1:46652 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:39] Prefill batch, #new-seq: 1, #new-token: 1143, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:39] Decode batch, #running-req: 1, #token: 1235, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.50, #queue-req: 0, 
[2025-12-15 18:28:40] Decode batch, #running-req: 1, #token: 1275, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.70, #queue-req: 0, 
[2025-12-15 18:28:40] Decode batch, #running-req: 1, #token: 1315, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.69, #queue-req: 0, 
[2025-12-15 18:28:41] Decode batch, #running-req: 1, #token: 1355, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.70, #queue-req: 0, 
[2025-12-15 18:28:41] Decode batch, #running-req: 1, #token: 1395, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.68, #queue-req: 0, 
[2025-12-15 18:28:42] Decode batch, #running-req: 1, #token: 1435, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.68, #queue-req: 0, 
[2025-12-15 18:28:42] Decode batch, #running-req: 1, #token: 1475, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.68, #queue-req: 0, 
[2025-12-15 18:28:43] Decode batch, #running-req: 1, #token: 1515, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.67, #queue-req: 0, 
[2025-12-15 18:28:43] Decode batch, #running-req: 1, #token: 1555, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.64, #queue-req: 0, 
[2025-12-15 18:28:44] Decode batch, #running-req: 1, #token: 1595, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.65, #queue-req: 0, 
[2025-12-15 18:28:44] Decode batch, #running-req: 1, #token: 1635, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.63, #queue-req: 0, 
[2025-12-15 18:28:45] Decode batch, #running-req: 1, #token: 1675, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.60, #queue-req: 0, 
[2025-12-15 18:28:45] Decode batch, #running-req: 1, #token: 1715, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.58, #queue-req: 0, 
[2025-12-15 18:28:46] Decode batch, #running-req: 1, #token: 1755, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.58, #queue-req: 0, 
[2025-12-15 18:28:46] Decode batch, #running-req: 1, #token: 1795, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:28:47] Decode batch, #running-req: 1, #token: 1835, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.58, #queue-req: 0, 
[2025-12-15 18:28:47] Decode batch, #running-req: 1, #token: 1875, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:28:48] Decode batch, #running-req: 1, #token: 1915, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.55, #queue-req: 0, 
[2025-12-15 18:28:48] Decode batch, #running-req: 1, #token: 1955, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:28:49] Decode batch, #running-req: 1, #token: 1995, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.55, #queue-req: 0, 
[2025-12-15 18:28:49] Decode batch, #running-req: 1, #token: 2035, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.53, #queue-req: 0, 
[2025-12-15 18:28:50] Decode batch, #running-req: 1, #token: 2075, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.49, #queue-req: 0, 
[2025-12-15 18:28:50] Decode batch, #running-req: 1, #token: 2115, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.47, #queue-req: 0, 
[2025-12-15 18:28:51] Decode batch, #running-req: 1, #token: 2155, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.46, #queue-req: 0, 
[2025-12-15 18:28:51] Decode batch, #running-req: 1, #token: 2195, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.45, #queue-req: 0, 
[2025-12-15 18:28:51] INFO:     127.0.0.1:46656 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:51] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:52] Decode batch, #running-req: 1, #token: 261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.17, #queue-req: 0, 
[2025-12-15 18:28:52] INFO:     127.0.0.1:46662 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:52] Prefill batch, #new-seq: 1, #new-token: 836, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:52] Decode batch, #running-req: 1, #token: 901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 71.15, #queue-req: 0, 
[2025-12-15 18:28:53] INFO:     127.0.0.1:46666 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:53] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.59, #queue-req: 0, 
[2025-12-15 18:28:53] Prefill batch, #new-seq: 1, #new-token: 39, #cached-token: 231, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:53] Decode batch, #running-req: 1, #token: 311, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.69, #queue-req: 0, 
[2025-12-15 18:28:54] Decode batch, #running-req: 1, #token: 351, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.15, #queue-req: 0, 
[2025-12-15 18:28:54] INFO:     127.0.0.1:46670 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:54] Prefill batch, #new-seq: 1, #new-token: 1066, #cached-token: 61, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:55] Decode batch, #running-req: 1, #token: 1152, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.71, #queue-req: 0, 
[2025-12-15 18:28:55] INFO:     127.0.0.1:46674 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:55] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:55] Decode batch, #running-req: 1, #token: 253, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.39, #queue-req: 0, 
[2025-12-15 18:28:56] Decode batch, #running-req: 1, #token: 293, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.24, #queue-req: 0, 
[2025-12-15 18:28:56] INFO:     127.0.0.1:46678 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:56] Prefill batch, #new-seq: 1, #new-token: 909, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:56] Decode batch, #running-req: 1, #token: 998, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.89, #queue-req: 0, 
[2025-12-15 18:28:56] INFO:     127.0.0.1:46682 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:56] Prefill batch, #new-seq: 1, #new-token: 30, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:57] Decode batch, #running-req: 1, #token: 273, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.33, #queue-req: 0, 
[2025-12-15 18:28:57] INFO:     127.0.0.1:46686 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:57] Prefill batch, #new-seq: 1, #new-token: 956, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:57] Decode batch, #running-req: 1, #token: 1021, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.99, #queue-req: 0, 
[2025-12-15 18:28:58] INFO:     127.0.0.1:46690 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:58] Prefill batch, #new-seq: 1, #new-token: 15, #cached-token: 222, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:58] Decode batch, #running-req: 1, #token: 241, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.07, #queue-req: 0, 
[2025-12-15 18:28:58] Decode batch, #running-req: 1, #token: 281, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.31, #queue-req: 0, 
[2025-12-15 18:28:59] Decode batch, #running-req: 1, #token: 321, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:28:59] INFO:     127.0.0.1:46694 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:28:59] Prefill batch, #new-seq: 1, #new-token: 1121, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:28:59] Decode batch, #running-req: 1, #token: 1207, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.67, #queue-req: 0, 
[2025-12-15 18:29:00] Decode batch, #running-req: 1, #token: 1247, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.84, #queue-req: 0, 
[2025-12-15 18:29:00] Decode batch, #running-req: 1, #token: 1287, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.84, #queue-req: 0, 
[2025-12-15 18:29:01] Decode batch, #running-req: 1, #token: 1327, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.84, #queue-req: 0, 
[2025-12-15 18:29:01] Decode batch, #running-req: 1, #token: 1367, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.84, #queue-req: 0, 
[2025-12-15 18:29:02] Decode batch, #running-req: 1, #token: 1407, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.79, #queue-req: 0, 
[2025-12-15 18:29:02] Decode batch, #running-req: 1, #token: 1447, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.81, #queue-req: 0, 
[2025-12-15 18:29:02] INFO:     127.0.0.1:46698 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:02] Prefill batch, #new-seq: 1, #new-token: 43, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:03] Decode batch, #running-req: 1, #token: 295, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.32, #queue-req: 0, 
[2025-12-15 18:29:03] Decode batch, #running-req: 1, #token: 335, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:29:04] INFO:     127.0.0.1:46702 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:04] Prefill batch, #new-seq: 1, #new-token: 1207, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:04] Decode batch, #running-req: 1, #token: 1280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.30, #queue-req: 0, 
[2025-12-15 18:29:04] INFO:     127.0.0.1:46706 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:04] Prefill batch, #new-seq: 1, #new-token: 20, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:04] Decode batch, #running-req: 1, #token: 251, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.26, #queue-req: 0, 
[2025-12-15 18:29:05] INFO:     127.0.0.1:46710 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:05] Prefill batch, #new-seq: 1, #new-token: 1105, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:05] Decode batch, #running-req: 1, #token: 1179, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.86, #queue-req: 0, 
[2025-12-15 18:29:05] INFO:     127.0.0.1:46714 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:05] Prefill batch, #new-seq: 1, #new-token: 40, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:06] Decode batch, #running-req: 1, #token: 275, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.12, #queue-req: 0, 
[2025-12-15 18:29:06] Decode batch, #running-req: 1, #token: 315, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:29:06] INFO:     127.0.0.1:46718 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:06] Prefill batch, #new-seq: 1, #new-token: 1122, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:07] Decode batch, #running-req: 1, #token: 1194, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.69, #queue-req: 0, 
[2025-12-15 18:29:07] INFO:     127.0.0.1:46722 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:07] Prefill batch, #new-seq: 1, #new-token: 15, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:07] Decode batch, #running-req: 1, #token: 239, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.05, #queue-req: 0, 
[2025-12-15 18:29:08] Decode batch, #running-req: 1, #token: 279, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.32, #queue-req: 0, 
[2025-12-15 18:29:08] Decode batch, #running-req: 1, #token: 319, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:29:08] INFO:     127.0.0.1:46726 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:08] Prefill batch, #new-seq: 1, #new-token: 996, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:09] Decode batch, #running-req: 1, #token: 1092, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.64, #queue-req: 0, 
[2025-12-15 18:29:09] INFO:     127.0.0.1:46730 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:09] Prefill batch, #new-seq: 1, #new-token: 15, #cached-token: 223, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:09] Decode batch, #running-req: 1, #token: 273, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.53, #queue-req: 0, 
[2025-12-15 18:29:09] INFO:     127.0.0.1:46734 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:09] Prefill batch, #new-seq: 1, #new-token: 1020, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:10] Decode batch, #running-req: 1, #token: 1108, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.66, #queue-req: 0, 
[2025-12-15 18:29:10] INFO:     127.0.0.1:46738 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:10] Prefill batch, #new-seq: 1, #new-token: 45, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:10] Decode batch, #running-req: 1, #token: 288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.28, #queue-req: 0, 
[2025-12-15 18:29:11] Decode batch, #running-req: 1, #token: 328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:29:11] INFO:     127.0.0.1:46742 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:11] Prefill batch, #new-seq: 1, #new-token: 948, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:11] Decode batch, #running-req: 1, #token: 1026, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.83, #queue-req: 0, 
[2025-12-15 18:29:12] INFO:     127.0.0.1:46746 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:12] Prefill batch, #new-seq: 1, #new-token: 33, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:12] Decode batch, #running-req: 1, #token: 265, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.21, #queue-req: 0, 
[2025-12-15 18:29:12] Decode batch, #running-req: 1, #token: 305, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:29:13] INFO:     127.0.0.1:46750 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:13] Prefill batch, #new-seq: 1, #new-token: 1035, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:13] Decode batch, #running-req: 1, #token: 1103, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.88, #queue-req: 0, 
[2025-12-15 18:29:13] INFO:     127.0.0.1:46754 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:14] Prefill batch, #new-seq: 1, #new-token: 28, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:14] Decode batch, #running-req: 1, #token: 252, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.02, #queue-req: 0, 
[2025-12-15 18:29:14] Decode batch, #running-req: 1, #token: 292, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:29:15] Decode batch, #running-req: 1, #token: 332, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.17, #queue-req: 0, 
[2025-12-15 18:29:15] INFO:     127.0.0.1:46758 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:15] Prefill batch, #new-seq: 1, #new-token: 941, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:15] Decode batch, #running-req: 1, #token: 1007, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.92, #queue-req: 0, 
[2025-12-15 18:29:16] Decode batch, #running-req: 1, #token: 1047, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.76, #queue-req: 0, 
[2025-12-15 18:29:16] INFO:     127.0.0.1:46762 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:16] Prefill batch, #new-seq: 1, #new-token: 55, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:16] Decode batch, #running-req: 1, #token: 314, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.40, #queue-req: 0, 
[2025-12-15 18:29:17] Decode batch, #running-req: 1, #token: 354, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.13, #queue-req: 0, 
[2025-12-15 18:29:17] INFO:     127.0.0.1:46766 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:17] Prefill batch, #new-seq: 1, #new-token: 1204, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:17] Decode batch, #running-req: 1, #token: 1286, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.23, #queue-req: 0, 
[2025-12-15 18:29:18] INFO:     127.0.0.1:46772 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:18] Prefill batch, #new-seq: 1, #new-token: 12, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:18] Decode batch, #running-req: 1, #token: 249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.23, #queue-req: 0, 
[2025-12-15 18:29:18] INFO:     127.0.0.1:46776 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:18] Prefill batch, #new-seq: 1, #new-token: 955, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:18] Decode batch, #running-req: 1, #token: 1016, token usage: 0.00, cuda graph: True, gen throughput (token/s): 70.10, #queue-req: 0, 
[2025-12-15 18:29:19] INFO:     127.0.0.1:46780 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:19] Prefill batch, #new-seq: 1, #new-token: 39, #cached-token: 226, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:19] Decode batch, #running-req: 1, #token: 269, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.06, #queue-req: 0, 
[2025-12-15 18:29:19] Decode batch, #running-req: 1, #token: 309, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:29:20] Decode batch, #running-req: 1, #token: 349, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.13, #queue-req: 0, 
[2025-12-15 18:29:20] INFO:     127.0.0.1:46784 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:20] Prefill batch, #new-seq: 1, #new-token: 1236, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:20] Decode batch, #running-req: 1, #token: 1320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.21, #queue-req: 0, 
[2025-12-15 18:29:21] INFO:     127.0.0.1:46788 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:21] Prefill batch, #new-seq: 1, #new-token: 62, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:21] Decode batch, #running-req: 1, #token: 316, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.26, #queue-req: 0, 
[2025-12-15 18:29:21] Decode batch, #running-req: 1, #token: 356, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.12, #queue-req: 0, 
[2025-12-15 18:29:22] INFO:     127.0.0.1:46792 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:22] Prefill batch, #new-seq: 1, #new-token: 1188, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:22] Decode batch, #running-req: 1, #token: 1264, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.32, #queue-req: 0, 
[2025-12-15 18:29:22] INFO:     127.0.0.1:46796 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:22] Prefill batch, #new-seq: 1, #new-token: 52, #cached-token: 223, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:23] Decode batch, #running-req: 1, #token: 284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.04, #queue-req: 0, 
[2025-12-15 18:29:23] Decode batch, #running-req: 1, #token: 324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:29:24] Decode batch, #running-req: 1, #token: 364, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.12, #queue-req: 0, 
[2025-12-15 18:29:24] INFO:     127.0.0.1:46800 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:24] Prefill batch, #new-seq: 1, #new-token: 1138, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:24] Decode batch, #running-req: 1, #token: 1208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.64, #queue-req: 0, 
[2025-12-15 18:29:25] INFO:     127.0.0.1:46804 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:25] Prefill batch, #new-seq: 1, #new-token: 11, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:25] Decode batch, #running-req: 1, #token: 243, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.17, #queue-req: 0, 
[2025-12-15 18:29:25] Decode batch, #running-req: 1, #token: 283, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.30, #queue-req: 0, 
[2025-12-15 18:29:26] INFO:     127.0.0.1:46808 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:26] Prefill batch, #new-seq: 1, #new-token: 1055, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:26] Decode batch, #running-req: 1, #token: 1119, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.94, #queue-req: 0, 
[2025-12-15 18:29:26] Decode batch, #running-req: 1, #token: 1159, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.70, #queue-req: 0, 
[2025-12-15 18:29:26] INFO:     127.0.0.1:46812 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:26] Prefill batch, #new-seq: 1, #new-token: 25, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:27] Decode batch, #running-req: 1, #token: 278, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.46, #queue-req: 0, 
[2025-12-15 18:29:27] INFO:     127.0.0.1:46816 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:27] Prefill batch, #new-seq: 1, #new-token: 1232, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:27] Decode batch, #running-req: 1, #token: 1309, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.27, #queue-req: 0, 
[2025-12-15 18:29:28] INFO:     127.0.0.1:46820 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:28] Prefill batch, #new-seq: 1, #new-token: 34, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:28] Decode batch, #running-req: 1, #token: 270, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.21, #queue-req: 0, 
[2025-12-15 18:29:28] Decode batch, #running-req: 1, #token: 310, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.17, #queue-req: 0, 
[2025-12-15 18:29:29] INFO:     127.0.0.1:46824 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:29] Prefill batch, #new-seq: 1, #new-token: 1119, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:29] Decode batch, #running-req: 1, #token: 1206, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.64, #queue-req: 0, 
[2025-12-15 18:29:29] INFO:     127.0.0.1:46830 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:29] Prefill batch, #new-seq: 1, #new-token: 59, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:30] Decode batch, #running-req: 1, #token: 307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.27, #queue-req: 0, 
[2025-12-15 18:29:30] Decode batch, #running-req: 1, #token: 347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.16, #queue-req: 0, 
[2025-12-15 18:29:30] INFO:     127.0.0.1:46834 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:30] Prefill batch, #new-seq: 1, #new-token: 1168, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:31] Decode batch, #running-req: 1, #token: 1230, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.48, #queue-req: 0, 
[2025-12-15 18:29:31] INFO:     127.0.0.1:46838 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:31] Prefill batch, #new-seq: 1, #new-token: 28, #cached-token: 222, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:31] Decode batch, #running-req: 1, #token: 257, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.00, #queue-req: 0, 
[2025-12-15 18:29:32] Decode batch, #running-req: 1, #token: 297, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:29:32] Decode batch, #running-req: 1, #token: 337, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:29:32] INFO:     127.0.0.1:46842 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:32] Prefill batch, #new-seq: 1, #new-token: 1196, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:33] Decode batch, #running-req: 1, #token: 1276, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.24, #queue-req: 0, 
[2025-12-15 18:29:33] INFO:     127.0.0.1:46846 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:33] Prefill batch, #new-seq: 1, #new-token: 42, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:33] Decode batch, #running-req: 1, #token: 275, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.14, #queue-req: 0, 
[2025-12-15 18:29:34] Decode batch, #running-req: 1, #token: 315, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:29:34] INFO:     127.0.0.1:46850 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:34] Prefill batch, #new-seq: 1, #new-token: 1187, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:34] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.21, #queue-req: 0, 
[2025-12-15 18:29:34] INFO:     127.0.0.1:46854 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:34] Prefill batch, #new-seq: 1, #new-token: 47, #cached-token: 223, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:35] Decode batch, #running-req: 1, #token: 310, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.37, #queue-req: 0, 
[2025-12-15 18:29:35] Decode batch, #running-req: 1, #token: 350, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.17, #queue-req: 0, 
[2025-12-15 18:29:36] INFO:     127.0.0.1:46858 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:36] Prefill batch, #new-seq: 1, #new-token: 1201, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:36] Decode batch, #running-req: 1, #token: 1273, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.27, #queue-req: 0, 
[2025-12-15 18:29:36] INFO:     127.0.0.1:46864 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:36] Prefill batch, #new-seq: 1, #new-token: 9, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:36] Decode batch, #running-req: 1, #token: 235, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.02, #queue-req: 0, 
[2025-12-15 18:29:37] Decode batch, #running-req: 1, #token: 275, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.37, #queue-req: 0, 
[2025-12-15 18:29:37] Decode batch, #running-req: 1, #token: 315, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:29:38] INFO:     127.0.0.1:46868 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:38] Prefill batch, #new-seq: 1, #new-token: 956, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:38] Decode batch, #running-req: 1, #token: 1050, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.74, #queue-req: 0, 
[2025-12-15 18:29:38] INFO:     127.0.0.1:46872 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:38] Prefill batch, #new-seq: 1, #new-token: 13, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:39] Decode batch, #running-req: 1, #token: 257, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.42, #queue-req: 0, 
[2025-12-15 18:29:39] INFO:     127.0.0.1:46876 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:39] Prefill batch, #new-seq: 1, #new-token: 899, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:39] Decode batch, #running-req: 1, #token: 986, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.94, #queue-req: 0, 
[2025-12-15 18:29:39] INFO:     127.0.0.1:46880 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:39] Prefill batch, #new-seq: 1, #new-token: 76, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:40] Decode batch, #running-req: 1, #token: 327, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.32, #queue-req: 0, 
[2025-12-15 18:29:40] Decode batch, #running-req: 1, #token: 367, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.09, #queue-req: 0, 
[2025-12-15 18:29:40] INFO:     127.0.0.1:46884 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:40] Prefill batch, #new-seq: 1, #new-token: 1314, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:41] Decode batch, #running-req: 1, #token: 1407, token usage: 0.00, cuda graph: True, gen throughput (token/s): 67.25, #queue-req: 0, 
[2025-12-15 18:29:41] INFO:     127.0.0.1:46888 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:41] Prefill batch, #new-seq: 1, #new-token: 34, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:41] Decode batch, #running-req: 1, #token: 287, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.31, #queue-req: 0, 
[2025-12-15 18:29:42] INFO:     127.0.0.1:46892 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:42] Prefill batch, #new-seq: 1, #new-token: 1164, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:42] Decode batch, #running-req: 1, #token: 1231, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.52, #queue-req: 0, 
[2025-12-15 18:29:42] INFO:     127.0.0.1:46896 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:42] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.45, #queue-req: 0, 
[2025-12-15 18:29:42] Prefill batch, #new-seq: 1, #new-token: 32, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:43] Decode batch, #running-req: 1, #token: 289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.72, #queue-req: 0, 
[2025-12-15 18:29:43] Decode batch, #running-req: 1, #token: 329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:29:43] INFO:     127.0.0.1:46900 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:43] Prefill batch, #new-seq: 1, #new-token: 1153, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:44] Decode batch, #running-req: 1, #token: 1245, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.21, #queue-req: 0, 
[2025-12-15 18:29:44] INFO:     127.0.0.1:46904 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:44] Prefill batch, #new-seq: 1, #new-token: 22, #cached-token: 227, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:44] Decode batch, #running-req: 1, #token: 280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.40, #queue-req: 0, 
[2025-12-15 18:29:45] Decode batch, #running-req: 1, #token: 320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:29:45] INFO:     127.0.0.1:46908 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:45] Prefill batch, #new-seq: 1, #new-token: 1060, #cached-token: 61, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:46] Decode batch, #running-req: 1, #token: 1137, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.82, #queue-req: 0, 
[2025-12-15 18:29:46] Decode batch, #running-req: 1, #token: 1177, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.78, #queue-req: 0, 
[2025-12-15 18:29:47] Decode batch, #running-req: 1, #token: 1217, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.83, #queue-req: 0, 
[2025-12-15 18:29:47] Decode batch, #running-req: 1, #token: 1257, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.81, #queue-req: 0, 
[2025-12-15 18:29:48] Decode batch, #running-req: 1, #token: 1297, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.78, #queue-req: 0, 
[2025-12-15 18:29:48] Decode batch, #running-req: 1, #token: 1337, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.78, #queue-req: 0, 
[2025-12-15 18:29:49] Decode batch, #running-req: 1, #token: 1377, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.78, #queue-req: 0, 
[2025-12-15 18:29:49] INFO:     127.0.0.1:46912 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:49] Prefill batch, #new-seq: 1, #new-token: 16, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:49] Decode batch, #running-req: 1, #token: 240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 75.83, #queue-req: 0, 
[2025-12-15 18:29:50] Decode batch, #running-req: 1, #token: 280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.32, #queue-req: 0, 
[2025-12-15 18:29:50] Decode batch, #running-req: 1, #token: 320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:29:50] INFO:     127.0.0.1:46916 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:50] Prefill batch, #new-seq: 1, #new-token: 1063, #cached-token: 61, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:51] Decode batch, #running-req: 1, #token: 1160, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.66, #queue-req: 0, 
[2025-12-15 18:29:51] INFO:     127.0.0.1:46920 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:51] Prefill batch, #new-seq: 1, #new-token: 37, #cached-token: 223, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:51] Decode batch, #running-req: 1, #token: 295, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.35, #queue-req: 0, 
[2025-12-15 18:29:52] Decode batch, #running-req: 1, #token: 335, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:29:52] INFO:     127.0.0.1:46924 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:52] Prefill batch, #new-seq: 1, #new-token: 1090, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:52] Decode batch, #running-req: 1, #token: 1167, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.65, #queue-req: 0, 
[2025-12-15 18:29:53] INFO:     127.0.0.1:46928 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:53] Prefill batch, #new-seq: 1, #new-token: 38, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:53] Decode batch, #running-req: 1, #token: 277, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.23, #queue-req: 0, 
[2025-12-15 18:29:53] Decode batch, #running-req: 1, #token: 317, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:29:54] INFO:     127.0.0.1:46932 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:54] Prefill batch, #new-seq: 1, #new-token: 1220, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:54] Decode batch, #running-req: 1, #token: 1289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.31, #queue-req: 0, 
[2025-12-15 18:29:54] INFO:     127.0.0.1:46936 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:54] Prefill batch, #new-seq: 1, #new-token: 52, #cached-token: 222, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:54] Decode batch, #running-req: 1, #token: 288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.12, #queue-req: 0, 
[2025-12-15 18:29:55] Decode batch, #running-req: 1, #token: 328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:29:55] INFO:     127.0.0.1:46940 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:55] Prefill batch, #new-seq: 1, #new-token: 1081, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:55] Decode batch, #running-req: 1, #token: 1141, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.80, #queue-req: 0, 
[2025-12-15 18:29:56] INFO:     127.0.0.1:46944 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:56] Prefill batch, #new-seq: 1, #new-token: 30, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:56] Decode batch, #running-req: 1, #token: 251, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.01, #queue-req: 0, 
[2025-12-15 18:29:57] Decode batch, #running-req: 1, #token: 291, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:29:57] Decode batch, #running-req: 1, #token: 331, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.21, #queue-req: 0, 
[2025-12-15 18:29:57] INFO:     127.0.0.1:46948 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:57] Prefill batch, #new-seq: 1, #new-token: 1163, #cached-token: 63, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:58] Decode batch, #running-req: 1, #token: 1241, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.36, #queue-req: 0, 
[2025-12-15 18:29:58] INFO:     127.0.0.1:46952 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:58] Prefill batch, #new-seq: 1, #new-token: 22, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:58] Decode batch, #running-req: 1, #token: 257, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.31, #queue-req: 0, 
[2025-12-15 18:29:59] Decode batch, #running-req: 1, #token: 297, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.21, #queue-req: 0, 
[2025-12-15 18:29:59] INFO:     127.0.0.1:46956 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:29:59] Prefill batch, #new-seq: 1, #new-token: 1114, #cached-token: 61, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:29:59] Decode batch, #running-req: 1, #token: 1183, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.86, #queue-req: 0, 
[2025-12-15 18:30:00] INFO:     127.0.0.1:46962 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:00] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.47, #queue-req: 0, 
[2025-12-15 18:30:00] Prefill batch, #new-seq: 1, #new-token: 40, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:00] Decode batch, #running-req: 1, #token: 297, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.66, #queue-req: 0, 
[2025-12-15 18:30:00] INFO:     127.0.0.1:46966 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:00] Prefill batch, #new-seq: 1, #new-token: 1121, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:01] Decode batch, #running-req: 1, #token: 1208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.66, #queue-req: 0, 
[2025-12-15 18:30:01] INFO:     127.0.0.1:46970 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:01] Prefill batch, #new-seq: 1, #new-token: 13, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:01] Decode batch, #running-req: 1, #token: 263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.52, #queue-req: 0, 
[2025-12-15 18:30:02] Decode batch, #running-req: 1, #token: 303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:30:02] INFO:     127.0.0.1:46974 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:02] Prefill batch, #new-seq: 1, #new-token: 1098, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:02] Decode batch, #running-req: 1, #token: 1182, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.66, #queue-req: 0, 
[2025-12-15 18:30:03] Decode batch, #running-req: 1, #token: 1222, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.84, #queue-req: 0, 
[2025-12-15 18:30:03] Decode batch, #running-req: 1, #token: 1262, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.80, #queue-req: 0, 
[2025-12-15 18:30:04] Decode batch, #running-req: 1, #token: 1302, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.81, #queue-req: 0, 
[2025-12-15 18:30:04] Decode batch, #running-req: 1, #token: 1342, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.81, #queue-req: 0, 
[2025-12-15 18:30:05] Decode batch, #running-req: 1, #token: 1382, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.80, #queue-req: 0, 
[2025-12-15 18:30:05] Decode batch, #running-req: 1, #token: 1422, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.79, #queue-req: 0, 
[2025-12-15 18:30:06] Decode batch, #running-req: 1, #token: 1462, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.76, #queue-req: 0, 
[2025-12-15 18:30:06] Decode batch, #running-req: 1, #token: 1502, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.73, #queue-req: 0, 
[2025-12-15 18:30:07] INFO:     127.0.0.1:46978 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:07] Prefill batch, #new-seq: 1, #new-token: 14, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:07] Decode batch, #running-req: 1, #token: 244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.08, #queue-req: 0, 
[2025-12-15 18:30:07] Decode batch, #running-req: 1, #token: 284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.33, #queue-req: 0, 
[2025-12-15 18:30:07] INFO:     127.0.0.1:46984 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:07] Prefill batch, #new-seq: 1, #new-token: 949, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:08] Decode batch, #running-req: 1, #token: 1044, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.76, #queue-req: 0, 
[2025-12-15 18:30:08] INFO:     127.0.0.1:46988 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:08] Prefill batch, #new-seq: 1, #new-token: 15, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:09] Decode batch, #running-req: 1, #token: 271, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.62, #queue-req: 0, 
[2025-12-15 18:30:09] Decode batch, #running-req: 1, #token: 311, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:30:09] INFO:     127.0.0.1:46992 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:09] Prefill batch, #new-seq: 1, #new-token: 960, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:10] Decode batch, #running-req: 1, #token: 1056, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.75, #queue-req: 0, 
[2025-12-15 18:30:10] INFO:     127.0.0.1:46996 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:10] Prefill batch, #new-seq: 1, #new-token: 29, #cached-token: 233, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:10] Decode batch, #running-req: 1, #token: 298, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.43, #queue-req: 0, 
[2025-12-15 18:30:11] Decode batch, #running-req: 1, #token: 338, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:30:11] Decode batch, #running-req: 1, #token: 378, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.06, #queue-req: 0, 
[2025-12-15 18:30:11] INFO:     127.0.0.1:47000 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:11] Prefill batch, #new-seq: 1, #new-token: 1253, #cached-token: 61, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:12] Decode batch, #running-req: 1, #token: 1345, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.20, #queue-req: 0, 
[2025-12-15 18:30:12] INFO:     127.0.0.1:47004 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:12] Prefill batch, #new-seq: 1, #new-token: 20, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:12] Decode batch, #running-req: 1, #token: 271, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.39, #queue-req: 0, 
[2025-12-15 18:30:12] INFO:     127.0.0.1:47008 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:12] Prefill batch, #new-seq: 1, #new-token: 998, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:13] Decode batch, #running-req: 1, #token: 1083, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.71, #queue-req: 0, 
[2025-12-15 18:30:13] INFO:     127.0.0.1:47012 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:13] Prefill batch, #new-seq: 1, #new-token: 47, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:13] Decode batch, #running-req: 1, #token: 283, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.27, #queue-req: 0, 
[2025-12-15 18:30:14] Decode batch, #running-req: 1, #token: 323, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.21, #queue-req: 0, 
[2025-12-15 18:30:14] INFO:     127.0.0.1:47016 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:14] Prefill batch, #new-seq: 1, #new-token: 1125, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:14] Decode batch, #running-req: 1, #token: 1190, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.79, #queue-req: 0, 
[2025-12-15 18:30:15] Decode batch, #running-req: 1, #token: 1230, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.69, #queue-req: 0, 
[2025-12-15 18:30:15] INFO:     127.0.0.1:47020 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:15] Prefill batch, #new-seq: 1, #new-token: 9, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:15] Decode batch, #running-req: 1, #token: 267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.61, #queue-req: 0, 
[2025-12-15 18:30:16] Decode batch, #running-req: 1, #token: 307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:30:16] INFO:     127.0.0.1:47024 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:16] Prefill batch, #new-seq: 1, #new-token: 978, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:17] Decode batch, #running-req: 1, #token: 1069, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.63, #queue-req: 0, 
[2025-12-15 18:30:17] INFO:     127.0.0.1:47028 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:17] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:17] Decode batch, #running-req: 1, #token: 261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.53, #queue-req: 0, 
[2025-12-15 18:30:18] Decode batch, #running-req: 1, #token: 301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.24, #queue-req: 0, 
[2025-12-15 18:30:18] INFO:     127.0.0.1:47032 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:18] Prefill batch, #new-seq: 1, #new-token: 1025, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:18] Decode batch, #running-req: 1, #token: 1094, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.94, #queue-req: 0, 
[2025-12-15 18:30:19] INFO:     127.0.0.1:47036 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:19] Prefill batch, #new-seq: 1, #new-token: 10, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:19] Decode batch, #running-req: 1, #token: 239, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.09, #queue-req: 0, 
[2025-12-15 18:30:19] INFO:     127.0.0.1:47040 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:19] Prefill batch, #new-seq: 1, #new-token: 908, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:19] Decode batch, #running-req: 1, #token: 969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 70.23, #queue-req: 0, 
[2025-12-15 18:30:20] INFO:     127.0.0.1:47044 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:20] Prefill batch, #new-seq: 1, #new-token: 16, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:20] Decode batch, #running-req: 1, #token: 239, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.18, #queue-req: 0, 
[2025-12-15 18:30:20] Decode batch, #running-req: 1, #token: 279, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.36, #queue-req: 0, 
[2025-12-15 18:30:21] INFO:     127.0.0.1:47048 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:21] Prefill batch, #new-seq: 1, #new-token: 1028, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:21] Decode batch, #running-req: 1, #token: 1101, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.95, #queue-req: 0, 
[2025-12-15 18:30:21] Decode batch, #running-req: 1, #token: 1141, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.81, #queue-req: 0, 
[2025-12-15 18:30:22] Decode batch, #running-req: 1, #token: 1181, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.83, #queue-req: 0, 
[2025-12-15 18:30:22] Decode batch, #running-req: 1, #token: 1221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.78, #queue-req: 0, 
[2025-12-15 18:30:23] Decode batch, #running-req: 1, #token: 1261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.77, #queue-req: 0, 
[2025-12-15 18:30:23] Decode batch, #running-req: 1, #token: 1301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.78, #queue-req: 0, 
[2025-12-15 18:30:23] INFO:     127.0.0.1:47052 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:23] Prefill batch, #new-seq: 1, #new-token: 83, #cached-token: 223, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:24] Decode batch, #running-req: 1, #token: 334, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.25, #queue-req: 0, 
[2025-12-15 18:30:24] Decode batch, #running-req: 1, #token: 374, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.05, #queue-req: 0, 
[2025-12-15 18:30:24] INFO:     127.0.0.1:47056 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:24] Prefill batch, #new-seq: 1, #new-token: 1147, #cached-token: 61, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:25] Decode batch, #running-req: 1, #token: 1240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.50, #queue-req: 0, 
[2025-12-15 18:30:25] INFO:     127.0.0.1:47060 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:25] Prefill batch, #new-seq: 1, #new-token: 25, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:25] Decode batch, #running-req: 1, #token: 275, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.38, #queue-req: 0, 
[2025-12-15 18:30:26] Decode batch, #running-req: 1, #token: 315, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:30:26] INFO:     127.0.0.1:47064 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:26] Prefill batch, #new-seq: 1, #new-token: 1108, #cached-token: 58, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-12-15 18:30:27] Decode batch, #running-req: 1, #token: 1175, token usage: 0.00, cuda graph: True, gen throughput (token/s): 60.90, #queue-req: 0, 
[2025-12-15 18:30:27] INFO:     127.0.0.1:47068 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:27] Prefill batch, #new-seq: 1, #new-token: 40, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:27] Decode batch, #running-req: 1, #token: 262, token usage: 0.00, cuda graph: True, gen throughput (token/s): 75.95, #queue-req: 0, 
[2025-12-15 18:30:28] Decode batch, #running-req: 1, #token: 302, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.15, #queue-req: 0, 
[2025-12-15 18:30:28] INFO:     127.0.0.1:47072 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:28] Prefill batch, #new-seq: 1, #new-token: 1057, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:28] Decode batch, #running-req: 1, #token: 1118, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.94, #queue-req: 0, 
[2025-12-15 18:30:29] INFO:     127.0.0.1:47076 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:29] Prefill batch, #new-seq: 1, #new-token: 60, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:29] Decode batch, #running-req: 1, #token: 288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.05, #queue-req: 0, 
[2025-12-15 18:30:29] Decode batch, #running-req: 1, #token: 328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.16, #queue-req: 0, 
[2025-12-15 18:30:30] Decode batch, #running-req: 1, #token: 368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.10, #queue-req: 0, 
[2025-12-15 18:30:30] INFO:     127.0.0.1:47080 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:30] Prefill batch, #new-seq: 1, #new-token: 1097, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:30] Decode batch, #running-req: 1, #token: 1181, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.62, #queue-req: 0, 
[2025-12-15 18:30:31] INFO:     127.0.0.1:47084 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:31] Prefill batch, #new-seq: 1, #new-token: 44, #cached-token: 223, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:31] Decode batch, #running-req: 1, #token: 288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.14, #queue-req: 0, 
[2025-12-15 18:30:31] Decode batch, #running-req: 1, #token: 328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:30:32] Decode batch, #running-req: 1, #token: 368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.12, #queue-req: 0, 
[2025-12-15 18:30:32] INFO:     127.0.0.1:47090 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:32] Prefill batch, #new-seq: 1, #new-token: 1140, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:32] Decode batch, #running-req: 1, #token: 1204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 64.51, #queue-req: 0, 
[2025-12-15 18:30:33] Decode batch, #running-req: 1, #token: 1244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.67, #queue-req: 0, 
[2025-12-15 18:30:33] INFO:     127.0.0.1:47096 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:33] Prefill batch, #new-seq: 1, #new-token: 40, #cached-token: 227, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:33] Decode batch, #running-req: 1, #token: 305, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.29, #queue-req: 0, 
[2025-12-15 18:30:34] Decode batch, #running-req: 1, #token: 345, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:30:34] Decode batch, #running-req: 1, #token: 385, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.09, #queue-req: 0, 
[2025-12-15 18:30:35] INFO:     127.0.0.1:47100 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:35] Prefill batch, #new-seq: 1, #new-token: 1316, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:35] Decode batch, #running-req: 1, #token: 1405, token usage: 0.00, cuda graph: True, gen throughput (token/s): 67.28, #queue-req: 0, 
[2025-12-15 18:30:35] INFO:     127.0.0.1:47104 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:35] Prefill batch, #new-seq: 1, #new-token: 43, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:36] Decode batch, #running-req: 1, #token: 288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.18, #queue-req: 0, 
[2025-12-15 18:30:36] Decode batch, #running-req: 1, #token: 328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:30:37] Decode batch, #running-req: 1, #token: 368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.09, #queue-req: 0, 
[2025-12-15 18:30:37] INFO:     127.0.0.1:47108 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:37] Prefill batch, #new-seq: 1, #new-token: 1228, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:37] Decode batch, #running-req: 1, #token: 1324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.13, #queue-req: 0, 
[2025-12-15 18:30:37] INFO:     127.0.0.1:47112 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:37] Prefill batch, #new-seq: 1, #new-token: 32, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:38] Decode batch, #running-req: 1, #token: 292, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.33, #queue-req: 0, 
[2025-12-15 18:30:38] INFO:     127.0.0.1:47116 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:38] Prefill batch, #new-seq: 1, #new-token: 1086, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:38] Decode batch, #running-req: 1, #token: 1170, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.72, #queue-req: 0, 
[2025-12-15 18:30:39] Decode batch, #running-req: 1, #token: 1210, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.82, #queue-req: 0, 
[2025-12-15 18:30:39] Decode batch, #running-req: 1, #token: 1250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.81, #queue-req: 0, 
[2025-12-15 18:30:40] Decode batch, #running-req: 1, #token: 1290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.78, #queue-req: 0, 
[2025-12-15 18:30:40] Decode batch, #running-req: 1, #token: 1330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.76, #queue-req: 0, 
[2025-12-15 18:30:41] Decode batch, #running-req: 1, #token: 1370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.78, #queue-req: 0, 
[2025-12-15 18:30:41] INFO:     127.0.0.1:47120 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:41] Prefill batch, #new-seq: 1, #new-token: 25, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:41] Decode batch, #running-req: 1, #token: 272, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.36, #queue-req: 0, 
[2025-12-15 18:30:42] Decode batch, #running-req: 1, #token: 312, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.21, #queue-req: 0, 
[2025-12-15 18:30:42] INFO:     127.0.0.1:47124 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:42] Prefill batch, #new-seq: 1, #new-token: 1105, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:42] Decode batch, #running-req: 1, #token: 1193, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.63, #queue-req: 0, 
[2025-12-15 18:30:43] Decode batch, #running-req: 1, #token: 1233, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.72, #queue-req: 0, 
[2025-12-15 18:30:43] Decode batch, #running-req: 1, #token: 1273, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.70, #queue-req: 0, 
[2025-12-15 18:30:44] Decode batch, #running-req: 1, #token: 1313, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.69, #queue-req: 0, 
[2025-12-15 18:30:44] Decode batch, #running-req: 1, #token: 1353, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.72, #queue-req: 0, 
[2025-12-15 18:30:45] Decode batch, #running-req: 1, #token: 1393, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.68, #queue-req: 0, 
[2025-12-15 18:30:45] Decode batch, #running-req: 1, #token: 1433, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.68, #queue-req: 0, 
[2025-12-15 18:30:46] Decode batch, #running-req: 1, #token: 1473, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.68, #queue-req: 0, 
[2025-12-15 18:30:46] Decode batch, #running-req: 1, #token: 1513, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.68, #queue-req: 0, 
[2025-12-15 18:30:47] Decode batch, #running-req: 1, #token: 1553, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.66, #queue-req: 0, 
[2025-12-15 18:30:47] Decode batch, #running-req: 1, #token: 1593, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.64, #queue-req: 0, 
[2025-12-15 18:30:48] Decode batch, #running-req: 1, #token: 1633, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.62, #queue-req: 0, 
[2025-12-15 18:30:48] Decode batch, #running-req: 1, #token: 1673, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.61, #queue-req: 0, 
[2025-12-15 18:30:49] Decode batch, #running-req: 1, #token: 1713, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.58, #queue-req: 0, 
[2025-12-15 18:30:49] Decode batch, #running-req: 1, #token: 1753, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:30:50] Decode batch, #running-req: 1, #token: 1793, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:30:50] Decode batch, #running-req: 1, #token: 1833, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:30:51] Decode batch, #running-req: 1, #token: 1873, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:30:51] Decode batch, #running-req: 1, #token: 1913, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.54, #queue-req: 0, 
[2025-12-15 18:30:52] Decode batch, #running-req: 1, #token: 1953, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.55, #queue-req: 0, 
[2025-12-15 18:30:52] Decode batch, #running-req: 1, #token: 1993, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.55, #queue-req: 0, 
[2025-12-15 18:30:53] Decode batch, #running-req: 1, #token: 2033, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.53, #queue-req: 0, 
[2025-12-15 18:30:53] Decode batch, #running-req: 1, #token: 2073, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.47, #queue-req: 0, 
[2025-12-15 18:30:54] Decode batch, #running-req: 1, #token: 2113, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.46, #queue-req: 0, 
[2025-12-15 18:30:54] Decode batch, #running-req: 1, #token: 2153, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.44, #queue-req: 0, 
[2025-12-15 18:30:55] INFO:     127.0.0.1:47128 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:55] Prefill batch, #new-seq: 1, #new-token: 30, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:55] Decode batch, #running-req: 1, #token: 282, token usage: 0.00, cuda graph: True, gen throughput (token/s): 75.92, #queue-req: 0, 
[2025-12-15 18:30:55] Decode batch, #running-req: 1, #token: 322, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:30:56] INFO:     127.0.0.1:47132 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:56] Prefill batch, #new-seq: 1, #new-token: 1092, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:56] Decode batch, #running-req: 1, #token: 1169, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.61, #queue-req: 0, 
[2025-12-15 18:30:56] INFO:     127.0.0.1:47136 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:56] Prefill batch, #new-seq: 1, #new-token: 27, #cached-token: 229, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:57] Decode batch, #running-req: 1, #token: 273, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.20, #queue-req: 0, 
[2025-12-15 18:30:57] Decode batch, #running-req: 1, #token: 313, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:30:57] INFO:     127.0.0.1:47142 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:58] Prefill batch, #new-seq: 1, #new-token: 1125, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:58] Decode batch, #running-req: 1, #token: 1193, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.76, #queue-req: 0, 
[2025-12-15 18:30:58] INFO:     127.0.0.1:47146 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:58] Prefill batch, #new-seq: 1, #new-token: 20, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:58] Decode batch, #running-req: 1, #token: 253, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.24, #queue-req: 0, 
[2025-12-15 18:30:59] INFO:     127.0.0.1:47150 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:59] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.00, #queue-req: 0, 
[2025-12-15 18:30:59] Prefill batch, #new-seq: 1, #new-token: 1073, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:30:59] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.75, #queue-req: 0, 
[2025-12-15 18:30:59] INFO:     127.0.0.1:47154 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:30:59] Prefill batch, #new-seq: 1, #new-token: 25, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:00] Decode batch, #running-req: 1, #token: 285, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.60, #queue-req: 0, 
[2025-12-15 18:31:00] INFO:     127.0.0.1:47158 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:00] Prefill batch, #new-seq: 1, #new-token: 1192, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:00] Decode batch, #running-req: 1, #token: 1266, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.38, #queue-req: 0, 
[2025-12-15 18:31:01] Decode batch, #running-req: 1, #token: 1306, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.76, #queue-req: 0, 
[2025-12-15 18:31:01] Decode batch, #running-req: 1, #token: 1346, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.80, #queue-req: 0, 
[2025-12-15 18:31:02] Decode batch, #running-req: 1, #token: 1386, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.80, #queue-req: 0, 
[2025-12-15 18:31:02] Decode batch, #running-req: 1, #token: 1426, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.78, #queue-req: 0, 
[2025-12-15 18:31:03] Decode batch, #running-req: 1, #token: 1466, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.77, #queue-req: 0, 
[2025-12-15 18:31:03] Decode batch, #running-req: 1, #token: 1506, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.74, #queue-req: 0, 
[2025-12-15 18:31:04] Decode batch, #running-req: 1, #token: 1546, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.65, #queue-req: 0, 
[2025-12-15 18:31:04] Decode batch, #running-req: 1, #token: 1586, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.65, #queue-req: 0, 
[2025-12-15 18:31:05] Decode batch, #running-req: 1, #token: 1626, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.64, #queue-req: 0, 
[2025-12-15 18:31:05] Decode batch, #running-req: 1, #token: 1666, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.63, #queue-req: 0, 
[2025-12-15 18:31:06] Decode batch, #running-req: 1, #token: 1706, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.61, #queue-req: 0, 
[2025-12-15 18:31:06] Decode batch, #running-req: 1, #token: 1746, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.60, #queue-req: 0, 
[2025-12-15 18:31:07] Decode batch, #running-req: 1, #token: 1786, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:31:07] Decode batch, #running-req: 1, #token: 1826, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.58, #queue-req: 0, 
[2025-12-15 18:31:08] Decode batch, #running-req: 1, #token: 1866, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.59, #queue-req: 0, 
[2025-12-15 18:31:08] Decode batch, #running-req: 1, #token: 1906, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:31:09] Decode batch, #running-req: 1, #token: 1946, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.55, #queue-req: 0, 
[2025-12-15 18:31:09] Decode batch, #running-req: 1, #token: 1986, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:31:10] Decode batch, #running-req: 1, #token: 2026, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.55, #queue-req: 0, 
[2025-12-15 18:31:10] Decode batch, #running-req: 1, #token: 2066, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.50, #queue-req: 0, 
[2025-12-15 18:31:11] Decode batch, #running-req: 1, #token: 2106, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.47, #queue-req: 0, 
[2025-12-15 18:31:11] Decode batch, #running-req: 1, #token: 2146, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.47, #queue-req: 0, 
[2025-12-15 18:31:12] Decode batch, #running-req: 1, #token: 2186, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.45, #queue-req: 0, 
[2025-12-15 18:31:12] Decode batch, #running-req: 1, #token: 2226, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.42, #queue-req: 0, 
[2025-12-15 18:31:13] INFO:     127.0.0.1:47162 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:13] Prefill batch, #new-seq: 1, #new-token: 25, #cached-token: 223, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:13] Decode batch, #running-req: 1, #token: 264, token usage: 0.00, cuda graph: True, gen throughput (token/s): 75.49, #queue-req: 0, 
[2025-12-15 18:31:13] Decode batch, #running-req: 1, #token: 304, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:31:14] INFO:     127.0.0.1:47166 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:14] Prefill batch, #new-seq: 1, #new-token: 1116, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:14] Decode batch, #running-req: 1, #token: 1187, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.76, #queue-req: 0, 
[2025-12-15 18:31:14] INFO:     127.0.0.1:47170 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:15] Prefill batch, #new-seq: 1, #new-token: 40, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:15] Decode batch, #running-req: 1, #token: 266, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.04, #queue-req: 0, 
[2025-12-15 18:31:15] Decode batch, #running-req: 1, #token: 306, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.21, #queue-req: 0, 
[2025-12-15 18:31:15] INFO:     127.0.0.1:47174 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:15] Prefill batch, #new-seq: 1, #new-token: 1238, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:16] Decode batch, #running-req: 1, #token: 1321, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.24, #queue-req: 0, 
[2025-12-15 18:31:16] INFO:     127.0.0.1:47178 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:16] Prefill batch, #new-seq: 1, #new-token: 22, #cached-token: 223, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:16] Decode batch, #running-req: 1, #token: 268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.37, #queue-req: 0, 
[2025-12-15 18:31:17] Decode batch, #running-req: 1, #token: 308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:31:17] Decode batch, #running-req: 1, #token: 348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.15, #queue-req: 0, 
[2025-12-15 18:31:17] INFO:     127.0.0.1:47182 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:17] Prefill batch, #new-seq: 1, #new-token: 1177, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:18] Decode batch, #running-req: 1, #token: 1269, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.20, #queue-req: 0, 
[2025-12-15 18:31:18] INFO:     127.0.0.1:47186 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:18] Prefill batch, #new-seq: 1, #new-token: 12, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:18] Decode batch, #running-req: 1, #token: 267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.47, #queue-req: 0, 
[2025-12-15 18:31:19] Decode batch, #running-req: 1, #token: 307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:31:19] INFO:     127.0.0.1:47190 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:19] Prefill batch, #new-seq: 1, #new-token: 895, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:19] Decode batch, #running-req: 1, #token: 976, token usage: 0.00, cuda graph: True, gen throughput (token/s): 70.94, #queue-req: 0, 
[2025-12-15 18:31:20] INFO:     127.0.0.1:47194 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:20] Prefill batch, #new-seq: 1, #new-token: 13, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:20] Decode batch, #running-req: 1, #token: 249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.36, #queue-req: 0, 
[2025-12-15 18:31:20] INFO:     127.0.0.1:47198 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:20] Prefill batch, #new-seq: 1, #new-token: 989, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:20] Decode batch, #running-req: 1, #token: 1071, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.76, #queue-req: 0, 
[2025-12-15 18:31:21] INFO:     127.0.0.1:47202 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:21] Prefill batch, #new-seq: 1, #new-token: 57, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:21] Decode batch, #running-req: 1, #token: 297, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.20, #queue-req: 0, 
[2025-12-15 18:31:21] Decode batch, #running-req: 1, #token: 337, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.17, #queue-req: 0, 
[2025-12-15 18:31:22] INFO:     127.0.0.1:47206 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:22] Prefill batch, #new-seq: 1, #new-token: 1020, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:22] Decode batch, #running-req: 1, #token: 1086, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.68, #queue-req: 0, 
[2025-12-15 18:31:22] INFO:     127.0.0.1:47210 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:22] Prefill batch, #new-seq: 1, #new-token: 49, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:23] Decode batch, #running-req: 1, #token: 282, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.05, #queue-req: 0, 
[2025-12-15 18:31:23] Decode batch, #running-req: 1, #token: 322, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:31:24] Decode batch, #running-req: 1, #token: 362, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.11, #queue-req: 0, 
[2025-12-15 18:31:24] INFO:     127.0.0.1:47214 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:24] Prefill batch, #new-seq: 1, #new-token: 1186, #cached-token: 61, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:24] Decode batch, #running-req: 1, #token: 1275, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.25, #queue-req: 0, 
[2025-12-15 18:31:24] INFO:     127.0.0.1:47218 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:24] Prefill batch, #new-seq: 1, #new-token: 9, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:25] Decode batch, #running-req: 1, #token: 251, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.29, #queue-req: 0, 
[2025-12-15 18:31:25] Decode batch, #running-req: 1, #token: 291, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.25, #queue-req: 0, 
[2025-12-15 18:31:25] INFO:     127.0.0.1:47224 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:25] Prefill batch, #new-seq: 1, #new-token: 954, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:26] Decode batch, #running-req: 1, #token: 1038, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.80, #queue-req: 0, 
[2025-12-15 18:31:26] INFO:     127.0.0.1:47228 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:26] Prefill batch, #new-seq: 1, #new-token: 25, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:26] Decode batch, #running-req: 1, #token: 278, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.46, #queue-req: 0, 
[2025-12-15 18:31:27] Decode batch, #running-req: 1, #token: 318, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:31:27] INFO:     127.0.0.1:47232 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:27] Prefill batch, #new-seq: 1, #new-token: 1095, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:27] Decode batch, #running-req: 1, #token: 1184, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.62, #queue-req: 0, 
[2025-12-15 18:31:27] INFO:     127.0.0.1:47236 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:28] Prefill batch, #new-seq: 1, #new-token: 26, #cached-token: 226, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:28] Decode batch, #running-req: 1, #token: 283, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.33, #queue-req: 0, 
[2025-12-15 18:31:28] Decode batch, #running-req: 1, #token: 323, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:31:28] INFO:     127.0.0.1:47240 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:28] Prefill batch, #new-seq: 1, #new-token: 1014, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:29] Decode batch, #running-req: 1, #token: 1108, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.55, #queue-req: 0, 
[2025-12-15 18:31:29] INFO:     127.0.0.1:47244 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:29] Prefill batch, #new-seq: 1, #new-token: 29, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:30] Decode batch, #running-req: 1, #token: 284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.51, #queue-req: 0, 
[2025-12-15 18:31:30] Decode batch, #running-req: 1, #token: 324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:31:30] INFO:     127.0.0.1:47248 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:30] Prefill batch, #new-seq: 1, #new-token: 1090, #cached-token: 62, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:31] Decode batch, #running-req: 1, #token: 1163, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.74, #queue-req: 0, 
[2025-12-15 18:31:31] INFO:     127.0.0.1:47254 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:31] Prefill batch, #new-seq: 1, #new-token: 43, #cached-token: 222, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:31] Decode batch, #running-req: 1, #token: 277, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.06, #queue-req: 0, 
[2025-12-15 18:31:31] INFO:     127.0.0.1:47258 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:31] Prefill batch, #new-seq: 1, #new-token: 967, #cached-token: 69, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:32] Decode batch, #running-req: 1, #token: 1074, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.69, #queue-req: 0, 
[2025-12-15 18:31:32] INFO:     127.0.0.1:47262 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:32] Prefill batch, #new-seq: 1, #new-token: 31, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:32] Decode batch, #running-req: 1, #token: 289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.50, #queue-req: 0, 
[2025-12-15 18:31:33] Decode batch, #running-req: 1, #token: 329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:31:33] INFO:     127.0.0.1:47266 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:33] Prefill batch, #new-seq: 1, #new-token: 1117, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:33] Decode batch, #running-req: 1, #token: 1202, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.56, #queue-req: 0, 
[2025-12-15 18:31:34] INFO:     127.0.0.1:47270 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:34] Prefill batch, #new-seq: 1, #new-token: 30, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:34] Decode batch, #running-req: 1, #token: 269, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.16, #queue-req: 0, 
[2025-12-15 18:31:34] Decode batch, #running-req: 1, #token: 309, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.17, #queue-req: 0, 
[2025-12-15 18:31:34] INFO:     127.0.0.1:47274 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:35] Prefill batch, #new-seq: 1, #new-token: 1077, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:35] Decode batch, #running-req: 1, #token: 1159, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.75, #queue-req: 0, 
[2025-12-15 18:31:35] INFO:     127.0.0.1:47278 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:35] Prefill batch, #new-seq: 1, #new-token: 65, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:35] Decode batch, #running-req: 1, #token: 313, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.35, #queue-req: 0, 
[2025-12-15 18:31:36] INFO:     127.0.0.1:47282 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:36] Prefill batch, #new-seq: 1, #new-token: 1238, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:36] Decode batch, #running-req: 1, #token: 1302, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.33, #queue-req: 0, 
[2025-12-15 18:31:36] INFO:     127.0.0.1:47286 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:36] Prefill batch, #new-seq: 1, #new-token: 43, #cached-token: 223, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:37] Decode batch, #running-req: 1, #token: 273, token usage: 0.00, cuda graph: True, gen throughput (token/s): 75.97, #queue-req: 0, 
[2025-12-15 18:31:37] Decode batch, #running-req: 1, #token: 313, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:31:38] Decode batch, #running-req: 1, #token: 353, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.16, #queue-req: 0, 
[2025-12-15 18:31:38] INFO:     127.0.0.1:47290 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:38] Prefill batch, #new-seq: 1, #new-token: 1223, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:38] Decode batch, #running-req: 1, #token: 1305, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.08, #queue-req: 0, 
[2025-12-15 18:31:38] INFO:     127.0.0.1:47294 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:38] Prefill batch, #new-seq: 1, #new-token: 19, #cached-token: 222, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:39] Decode batch, #running-req: 1, #token: 265, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.29, #queue-req: 0, 
[2025-12-15 18:31:39] Decode batch, #running-req: 1, #token: 305, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.24, #queue-req: 0, 
[2025-12-15 18:31:40] INFO:     127.0.0.1:47298 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:40] Prefill batch, #new-seq: 1, #new-token: 1256, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:40] Decode batch, #running-req: 1, #token: 1320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.38, #queue-req: 0, 
[2025-12-15 18:31:40] Decode batch, #running-req: 1, #token: 1360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.69, #queue-req: 0, 
[2025-12-15 18:31:40] INFO:     127.0.0.1:47302 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:40] Prefill batch, #new-seq: 1, #new-token: 55, #cached-token: 223, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:41] Decode batch, #running-req: 1, #token: 312, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.30, #queue-req: 0, 
[2025-12-15 18:31:41] Decode batch, #running-req: 1, #token: 352, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.14, #queue-req: 0, 
[2025-12-15 18:31:41] INFO:     127.0.0.1:47306 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:41] Prefill batch, #new-seq: 1, #new-token: 1133, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:42] Decode batch, #running-req: 1, #token: 1218, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.64, #queue-req: 0, 
[2025-12-15 18:31:42] INFO:     127.0.0.1:47310 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:42] Prefill batch, #new-seq: 1, #new-token: 20, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:42] Decode batch, #running-req: 1, #token: 258, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.29, #queue-req: 0, 
[2025-12-15 18:31:43] Decode batch, #running-req: 1, #token: 298, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.21, #queue-req: 0, 
[2025-12-15 18:31:43] Decode batch, #running-req: 1, #token: 338, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.17, #queue-req: 0, 
[2025-12-15 18:31:44] INFO:     127.0.0.1:47314 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:44] Prefill batch, #new-seq: 1, #new-token: 1213, #cached-token: 61, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:44] Decode batch, #running-req: 1, #token: 1296, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.22, #queue-req: 0, 
[2025-12-15 18:31:44] INFO:     127.0.0.1:47318 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:44] Prefill batch, #new-seq: 1, #new-token: 36, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:44] Decode batch, #running-req: 1, #token: 280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.30, #queue-req: 0, 
[2025-12-15 18:31:45] Decode batch, #running-req: 1, #token: 320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:31:45] INFO:     127.0.0.1:47322 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:45] Prefill batch, #new-seq: 1, #new-token: 1213, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:46] Decode batch, #running-req: 1, #token: 1282, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.30, #queue-req: 0, 
[2025-12-15 18:31:46] INFO:     127.0.0.1:47326 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:46] Prefill batch, #new-seq: 1, #new-token: 18, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:46] Decode batch, #running-req: 1, #token: 243, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.05, #queue-req: 0, 
[2025-12-15 18:31:47] Decode batch, #running-req: 1, #token: 283, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.32, #queue-req: 0, 
[2025-12-15 18:31:47] INFO:     127.0.0.1:47330 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:47] Prefill batch, #new-seq: 1, #new-token: 1127, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:47] Decode batch, #running-req: 1, #token: 1209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.68, #queue-req: 0, 
[2025-12-15 18:31:47] INFO:     127.0.0.1:47336 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:47] Prefill batch, #new-seq: 1, #new-token: 44, #cached-token: 222, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:48] INFO:     127.0.0.1:47340 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:48] Prefill batch, #new-seq: 1, #new-token: 1029, #cached-token: 65, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:48] Decode batch, #running-req: 1, #token: 1106, token usage: 0.00, cuda graph: True, gen throughput (token/s): 65.99, #queue-req: 0, 
[2025-12-15 18:31:48] INFO:     127.0.0.1:47344 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:48] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 222, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:48] Decode batch, #running-req: 1, #token: 234, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.06, #queue-req: 0, 
[2025-12-15 18:31:49] Decode batch, #running-req: 1, #token: 274, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.38, #queue-req: 0, 
[2025-12-15 18:31:49] INFO:     127.0.0.1:47348 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:49] Prefill batch, #new-seq: 1, #new-token: 897, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:49] Decode batch, #running-req: 1, #token: 983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.85, #queue-req: 0, 
[2025-12-15 18:31:49] INFO:     127.0.0.1:47352 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:49] Prefill batch, #new-seq: 1, #new-token: 36, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:50] Decode batch, #running-req: 1, #token: 285, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.43, #queue-req: 0, 
[2025-12-15 18:31:50] Decode batch, #running-req: 1, #token: 325, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:31:51] INFO:     127.0.0.1:47356 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:51] Prefill batch, #new-seq: 1, #new-token: 1205, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:51] Decode batch, #running-req: 1, #token: 1287, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.26, #queue-req: 0, 
[2025-12-15 18:31:51] INFO:     127.0.0.1:47360 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:51] Prefill batch, #new-seq: 1, #new-token: 15, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:51] Decode batch, #running-req: 1, #token: 253, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.20, #queue-req: 0, 
[2025-12-15 18:31:52] Decode batch, #running-req: 1, #token: 293, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.24, #queue-req: 0, 
[2025-12-15 18:31:52] INFO:     127.0.0.1:47364 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:52] Prefill batch, #new-seq: 1, #new-token: 962, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:53] Decode batch, #running-req: 1, #token: 1035, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.82, #queue-req: 0, 
[2025-12-15 18:31:53] INFO:     127.0.0.1:47368 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:53] Prefill batch, #new-seq: 1, #new-token: 47, #cached-token: 223, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:53] Decode batch, #running-req: 1, #token: 279, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.11, #queue-req: 0, 
[2025-12-15 18:31:54] Decode batch, #running-req: 1, #token: 319, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.16, #queue-req: 0, 
[2025-12-15 18:31:54] Decode batch, #running-req: 1, #token: 359, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.12, #queue-req: 0, 
[2025-12-15 18:31:54] INFO:     127.0.0.1:47372 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:54] Prefill batch, #new-seq: 1, #new-token: 1155, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:55] Decode batch, #running-req: 1, #token: 1242, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.25, #queue-req: 0, 
[2025-12-15 18:31:55] INFO:     127.0.0.1:47376 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:55] Prefill batch, #new-seq: 1, #new-token: 8, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:55] Decode batch, #running-req: 1, #token: 251, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.32, #queue-req: 0, 
[2025-12-15 18:31:56] Decode batch, #running-req: 1, #token: 291, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.25, #queue-req: 0, 
[2025-12-15 18:31:56] INFO:     127.0.0.1:47380 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:56] Prefill batch, #new-seq: 1, #new-token: 938, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:56] Decode batch, #running-req: 1, #token: 1014, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.87, #queue-req: 0, 
[2025-12-15 18:31:57] INFO:     127.0.0.1:47384 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:57] Prefill batch, #new-seq: 1, #new-token: 48, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:57] Decode batch, #running-req: 1, #token: 283, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.16, #queue-req: 0, 
[2025-12-15 18:31:57] Decode batch, #running-req: 1, #token: 323, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:31:58] INFO:     127.0.0.1:47388 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:58] Prefill batch, #new-seq: 1, #new-token: 1204, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:58] Decode batch, #running-req: 1, #token: 1266, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.44, #queue-req: 0, 
[2025-12-15 18:31:58] INFO:     127.0.0.1:47392 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:58] Prefill batch, #new-seq: 1, #new-token: 23, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:58] Decode batch, #running-req: 1, #token: 246, token usage: 0.00, cuda graph: True, gen throughput (token/s): 75.89, #queue-req: 0, 
[2025-12-15 18:31:59] Decode batch, #running-req: 1, #token: 286, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.28, #queue-req: 0, 
[2025-12-15 18:31:59] INFO:     127.0.0.1:47396 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:31:59] Prefill batch, #new-seq: 1, #new-token: 1082, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:31:59] Decode batch, #running-req: 1, #token: 1170, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.61, #queue-req: 0, 
[2025-12-15 18:32:00] INFO:     127.0.0.1:47400 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:00] Prefill batch, #new-seq: 1, #new-token: 47, #cached-token: 225, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:00] Decode batch, #running-req: 1, #token: 302, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.35, #queue-req: 0, 
[2025-12-15 18:32:00] INFO:     127.0.0.1:47404 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:00] Prefill batch, #new-seq: 1, #new-token: 1098, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:01] Decode batch, #running-req: 1, #token: 1164, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.84, #queue-req: 0, 
[2025-12-15 18:32:01] INFO:     127.0.0.1:47408 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:01] Prefill batch, #new-seq: 1, #new-token: 22, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:01] Decode batch, #running-req: 1, #token: 248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.00, #queue-req: 0, 
[2025-12-15 18:32:02] Decode batch, #running-req: 1, #token: 288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.27, #queue-req: 0, 
[2025-12-15 18:32:02] INFO:     127.0.0.1:47412 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:02] Prefill batch, #new-seq: 1, #new-token: 1131, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:02] Decode batch, #running-req: 1, #token: 1217, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.62, #queue-req: 0, 
[2025-12-15 18:32:02] INFO:     127.0.0.1:47416 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:02] Prefill batch, #new-seq: 1, #new-token: 39, #cached-token: 223, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:03] Decode batch, #running-req: 1, #token: 294, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.36, #queue-req: 0, 
[2025-12-15 18:32:03] INFO:     127.0.0.1:47420 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:03] Prefill batch, #new-seq: 1, #new-token: 1190, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:03] Decode batch, #running-req: 1, #token: 1256, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.43, #queue-req: 0, 
[2025-12-15 18:32:04] INFO:     127.0.0.1:47424 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:04] Prefill batch, #new-seq: 1, #new-token: 10, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:04] Decode batch, #running-req: 1, #token: 241, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.16, #queue-req: 0, 
[2025-12-15 18:32:04] Decode batch, #running-req: 1, #token: 281, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.33, #queue-req: 0, 
[2025-12-15 18:32:05] Decode batch, #running-req: 1, #token: 321, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:32:05] INFO:     127.0.0.1:47430 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:05] Prefill batch, #new-seq: 1, #new-token: 942, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:05] INFO:     127.0.0.1:47434 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:05] Prefill batch, #new-seq: 1, #new-token: 34, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:05] Decode batch, #running-req: 1, #token: 255, token usage: 0.00, cuda graph: True, gen throughput (token/s): 66.97, #queue-req: 0, 
[2025-12-15 18:32:06] Decode batch, #running-req: 1, #token: 295, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.21, #queue-req: 0, 
[2025-12-15 18:32:06] INFO:     127.0.0.1:47438 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:06] Prefill batch, #new-seq: 1, #new-token: 1125, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:07] Decode batch, #running-req: 1, #token: 1207, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.60, #queue-req: 0, 
[2025-12-15 18:32:07] INFO:     127.0.0.1:47442 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:07] Prefill batch, #new-seq: 1, #new-token: 27, #cached-token: 227, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:07] Decode batch, #running-req: 1, #token: 278, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.27, #queue-req: 0, 
[2025-12-15 18:32:08] Decode batch, #running-req: 1, #token: 318, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:32:08] INFO:     127.0.0.1:47446 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:08] Prefill batch, #new-seq: 1, #new-token: 1070, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:08] Decode batch, #running-req: 1, #token: 1162, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.68, #queue-req: 0, 
[2025-12-15 18:32:09] Decode batch, #running-req: 1, #token: 1202, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.75, #queue-req: 0, 
[2025-12-15 18:32:09] Decode batch, #running-req: 1, #token: 1242, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.81, #queue-req: 0, 
[2025-12-15 18:32:10] Decode batch, #running-req: 1, #token: 1282, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.77, #queue-req: 0, 
[2025-12-15 18:32:10] Decode batch, #running-req: 1, #token: 1322, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.79, #queue-req: 0, 
[2025-12-15 18:32:11] Decode batch, #running-req: 1, #token: 1362, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.75, #queue-req: 0, 
[2025-12-15 18:32:11] Decode batch, #running-req: 1, #token: 1402, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.77, #queue-req: 0, 
[2025-12-15 18:32:12] Decode batch, #running-req: 1, #token: 1442, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.77, #queue-req: 0, 
[2025-12-15 18:32:12] Decode batch, #running-req: 1, #token: 1482, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.76, #queue-req: 0, 
[2025-12-15 18:32:12] INFO:     127.0.0.1:47450 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:12] Prefill batch, #new-seq: 1, #new-token: 6, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:13] Decode batch, #running-req: 1, #token: 265, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.44, #queue-req: 0, 
[2025-12-15 18:32:13] INFO:     127.0.0.1:47454 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:13] Prefill batch, #new-seq: 1, #new-token: 954, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:13] Decode batch, #running-req: 1, #token: 1018, token usage: 0.00, cuda graph: True, gen throughput (token/s): 70.00, #queue-req: 0, 
[2025-12-15 18:32:14] Decode batch, #running-req: 1, #token: 1058, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.75, #queue-req: 0, 
[2025-12-15 18:32:14] INFO:     127.0.0.1:47458 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:14] Prefill batch, #new-seq: 1, #new-token: 20, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:14] Decode batch, #running-req: 1, #token: 277, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.58, #queue-req: 0, 
[2025-12-15 18:32:15] INFO:     127.0.0.1:47462 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:15] Prefill batch, #new-seq: 1, #new-token: 1167, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:15] Decode batch, #running-req: 1, #token: 1234, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.46, #queue-req: 0, 
[2025-12-15 18:32:15] INFO:     127.0.0.1:47466 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:15] Prefill batch, #new-seq: 1, #new-token: 20, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:15] Decode batch, #running-req: 1, #token: 248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.13, #queue-req: 0, 
[2025-12-15 18:32:16] Decode batch, #running-req: 1, #token: 288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.26, #queue-req: 0, 
[2025-12-15 18:32:16] INFO:     127.0.0.1:47470 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:16] Prefill batch, #new-seq: 1, #new-token: 1039, #cached-token: 61, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:16] Decode batch, #running-req: 1, #token: 1130, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.76, #queue-req: 0, 
[2025-12-15 18:32:17] INFO:     127.0.0.1:47474 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:17] Prefill batch, #new-seq: 1, #new-token: 19, #cached-token: 223, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:17] Decode batch, #running-req: 1, #token: 271, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.39, #queue-req: 0, 
[2025-12-15 18:32:17] Decode batch, #running-req: 1, #token: 311, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:32:18] Decode batch, #running-req: 1, #token: 351, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.13, #queue-req: 0, 
[2025-12-15 18:32:18] INFO:     127.0.0.1:47478 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:18] Prefill batch, #new-seq: 1, #new-token: 1165, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:19] Decode batch, #running-req: 1, #token: 1247, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.26, #queue-req: 0, 
[2025-12-15 18:32:19] INFO:     127.0.0.1:47484 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:19] Prefill batch, #new-seq: 1, #new-token: 32, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:19] Decode batch, #running-req: 1, #token: 276, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.19, #queue-req: 0, 
[2025-12-15 18:32:19] INFO:     127.0.0.1:47488 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:20] Prefill batch, #new-seq: 1, #new-token: 1104, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:20] Decode batch, #running-req: 1, #token: 1164, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.85, #queue-req: 0, 
[2025-12-15 18:32:20] Decode batch, #running-req: 1, #token: 1204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.64, #queue-req: 0, 
[2025-12-15 18:32:20] INFO:     127.0.0.1:47492 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:20] Prefill batch, #new-seq: 1, #new-token: 15, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:21] Decode batch, #running-req: 1, #token: 271, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.46, #queue-req: 0, 
[2025-12-15 18:32:21] INFO:     127.0.0.1:47496 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:21] Prefill batch, #new-seq: 1, #new-token: 897, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:21] Decode batch, #running-req: 1, #token: 969, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.92, #queue-req: 0, 
[2025-12-15 18:32:22] Decode batch, #running-req: 1, #token: 1009, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.80, #queue-req: 0, 
[2025-12-15 18:32:22] Decode batch, #running-req: 1, #token: 1049, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.84, #queue-req: 0, 
[2025-12-15 18:32:23] Decode batch, #running-req: 1, #token: 1089, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.81, #queue-req: 0, 
[2025-12-15 18:32:23] Decode batch, #running-req: 1, #token: 1129, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.79, #queue-req: 0, 
[2025-12-15 18:32:24] INFO:     127.0.0.1:48384 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:24] Prefill batch, #new-seq: 1, #new-token: 55, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:24] Decode batch, #running-req: 1, #token: 295, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.18, #queue-req: 0, 
[2025-12-15 18:32:24] Decode batch, #running-req: 1, #token: 335, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:32:25] Decode batch, #running-req: 1, #token: 375, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.10, #queue-req: 0, 
[2025-12-15 18:32:25] INFO:     127.0.0.1:48388 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:25] Prefill batch, #new-seq: 1, #new-token: 1175, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:25] Decode batch, #running-req: 1, #token: 1269, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.15, #queue-req: 0, 
[2025-12-15 18:32:25] INFO:     127.0.0.1:48392 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:25] Prefill batch, #new-seq: 1, #new-token: 71, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:26] Decode batch, #running-req: 1, #token: 327, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.33, #queue-req: 0, 
[2025-12-15 18:32:26] INFO:     127.0.0.1:48396 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:26] Prefill batch, #new-seq: 1, #new-token: 1210, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:26] Decode batch, #running-req: 1, #token: 1297, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.08, #queue-req: 0, 
[2025-12-15 18:32:27] INFO:     127.0.0.1:48400 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:27] Prefill batch, #new-seq: 1, #new-token: 43, #cached-token: 222, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:27] Decode batch, #running-req: 1, #token: 290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.23, #queue-req: 0, 
[2025-12-15 18:32:27] Decode batch, #running-req: 1, #token: 330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.15, #queue-req: 0, 
[2025-12-15 18:32:28] INFO:     127.0.0.1:48404 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:28] Prefill batch, #new-seq: 1, #new-token: 1210, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:28] Decode batch, #running-req: 1, #token: 1289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.21, #queue-req: 0, 
[2025-12-15 18:32:28] INFO:     127.0.0.1:48410 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:28] Prefill batch, #new-seq: 1, #new-token: 53, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:29] Decode batch, #running-req: 1, #token: 293, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.09, #queue-req: 0, 
[2025-12-15 18:32:29] INFO:     127.0.0.1:48414 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:29] Prefill batch, #new-seq: 1, #new-token: 1091, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:29] Decode batch, #running-req: 1, #token: 1154, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.81, #queue-req: 0, 
[2025-12-15 18:32:30] Decode batch, #running-req: 1, #token: 1194, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.67, #queue-req: 0, 
[2025-12-15 18:32:30] INFO:     127.0.0.1:48418 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:30] Prefill batch, #new-seq: 1, #new-token: 11, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:30] Decode batch, #running-req: 1, #token: 264, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.50, #queue-req: 0, 
[2025-12-15 18:32:30] INFO:     127.0.0.1:48422 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:30] Prefill batch, #new-seq: 1, #new-token: 965, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:31] Decode batch, #running-req: 1, #token: 1056, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.71, #queue-req: 0, 
[2025-12-15 18:32:31] INFO:     127.0.0.1:48426 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:31] Prefill batch, #new-seq: 1, #new-token: 43, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:31] Decode batch, #running-req: 1, #token: 292, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.33, #queue-req: 0, 
[2025-12-15 18:32:32] Decode batch, #running-req: 1, #token: 332, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:32:32] INFO:     127.0.0.1:48430 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:32] Prefill batch, #new-seq: 1, #new-token: 1300, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:32] Decode batch, #running-req: 1, #token: 1385, token usage: 0.00, cuda graph: True, gen throughput (token/s): 67.26, #queue-req: 0, 
[2025-12-15 18:32:33] INFO:     127.0.0.1:48434 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:33] Prefill batch, #new-seq: 1, #new-token: 24, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:33] Decode batch, #running-req: 1, #token: 264, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.24, #queue-req: 0, 
[2025-12-15 18:32:33] Decode batch, #running-req: 1, #token: 304, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.26, #queue-req: 0, 
[2025-12-15 18:32:34] INFO:     127.0.0.1:48438 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:34] Prefill batch, #new-seq: 1, #new-token: 1271, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:34] Decode batch, #running-req: 1, #token: 1334, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.24, #queue-req: 0, 
[2025-12-15 18:32:35] Decode batch, #running-req: 1, #token: 1374, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.65, #queue-req: 0, 
[2025-12-15 18:32:35] Decode batch, #running-req: 1, #token: 1414, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.66, #queue-req: 0, 
[2025-12-15 18:32:36] Decode batch, #running-req: 1, #token: 1454, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.66, #queue-req: 0, 
[2025-12-15 18:32:36] Decode batch, #running-req: 1, #token: 1494, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.66, #queue-req: 0, 
[2025-12-15 18:32:37] Decode batch, #running-req: 1, #token: 1534, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.65, #queue-req: 0, 
[2025-12-15 18:32:37] Decode batch, #running-req: 1, #token: 1574, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.63, #queue-req: 0, 
[2025-12-15 18:32:38] Decode batch, #running-req: 1, #token: 1614, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.61, #queue-req: 0, 
[2025-12-15 18:32:38] Decode batch, #running-req: 1, #token: 1654, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.61, #queue-req: 0, 
[2025-12-15 18:32:39] Decode batch, #running-req: 1, #token: 1694, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.58, #queue-req: 0, 
[2025-12-15 18:32:39] Decode batch, #running-req: 1, #token: 1734, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.58, #queue-req: 0, 
[2025-12-15 18:32:40] Decode batch, #running-req: 1, #token: 1774, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:32:40] Decode batch, #running-req: 1, #token: 1814, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:32:41] Decode batch, #running-req: 1, #token: 1854, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:32:41] Decode batch, #running-req: 1, #token: 1894, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.52, #queue-req: 0, 
[2025-12-15 18:32:42] Decode batch, #running-req: 1, #token: 1934, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.52, #queue-req: 0, 
[2025-12-15 18:32:42] Decode batch, #running-req: 1, #token: 1974, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.53, #queue-req: 0, 
[2025-12-15 18:32:43] Decode batch, #running-req: 1, #token: 2014, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.51, #queue-req: 0, 
[2025-12-15 18:32:43] Decode batch, #running-req: 1, #token: 2054, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.49, #queue-req: 0, 
[2025-12-15 18:32:44] Decode batch, #running-req: 1, #token: 2094, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.45, #queue-req: 0, 
[2025-12-15 18:32:44] Decode batch, #running-req: 1, #token: 2134, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.44, #queue-req: 0, 
[2025-12-15 18:32:45] Decode batch, #running-req: 1, #token: 2174, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.42, #queue-req: 0, 
[2025-12-15 18:32:45] Decode batch, #running-req: 1, #token: 2214, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.39, #queue-req: 0, 
[2025-12-15 18:32:46] Decode batch, #running-req: 1, #token: 2254, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.39, #queue-req: 0, 
[2025-12-15 18:32:46] Decode batch, #running-req: 1, #token: 2294, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.37, #queue-req: 0, 
[2025-12-15 18:32:46] INFO:     127.0.0.1:48442 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:47] Prefill batch, #new-seq: 1, #new-token: 45, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:47] Decode batch, #running-req: 1, #token: 270, token usage: 0.00, cuda graph: True, gen throughput (token/s): 75.45, #queue-req: 0, 
[2025-12-15 18:32:47] Decode batch, #running-req: 1, #token: 310, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.21, #queue-req: 0, 
[2025-12-15 18:32:47] INFO:     127.0.0.1:48446 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:47] Prefill batch, #new-seq: 1, #new-token: 1247, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:48] Decode batch, #running-req: 1, #token: 1326, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.26, #queue-req: 0, 
[2025-12-15 18:32:48] INFO:     127.0.0.1:48450 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:48] Prefill batch, #new-seq: 1, #new-token: 24, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:48] Decode batch, #running-req: 1, #token: 266, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.31, #queue-req: 0, 
[2025-12-15 18:32:49] Decode batch, #running-req: 1, #token: 306, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.21, #queue-req: 0, 
[2025-12-15 18:32:49] INFO:     127.0.0.1:48454 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:49] Prefill batch, #new-seq: 1, #new-token: 1162, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:49] Decode batch, #running-req: 1, #token: 1237, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.35, #queue-req: 0, 
[2025-12-15 18:32:50] INFO:     127.0.0.1:48458 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:50] Prefill batch, #new-seq: 1, #new-token: 34, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:50] Decode batch, #running-req: 1, #token: 258, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.13, #queue-req: 0, 
[2025-12-15 18:32:50] INFO:     127.0.0.1:48464 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:50] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.95, #queue-req: 0, 
[2025-12-15 18:32:50] Prefill batch, #new-seq: 1, #new-token: 1154, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:51] Decode batch, #running-req: 1, #token: 1252, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.35, #queue-req: 0, 
[2025-12-15 18:32:51] INFO:     127.0.0.1:48468 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:51] Prefill batch, #new-seq: 1, #new-token: 47, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:51] Decode batch, #running-req: 1, #token: 305, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.33, #queue-req: 0, 
[2025-12-15 18:32:52] Decode batch, #running-req: 1, #token: 345, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.17, #queue-req: 0, 
[2025-12-15 18:32:52] INFO:     127.0.0.1:48472 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:52] Prefill batch, #new-seq: 1, #new-token: 1113, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:53] Decode batch, #running-req: 1, #token: 1179, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.71, #queue-req: 0, 
[2025-12-15 18:32:53] INFO:     127.0.0.1:48476 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:53] Prefill batch, #new-seq: 1, #new-token: 6, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:53] Decode batch, #running-req: 1, #token: 236, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.07, #queue-req: 0, 
[2025-12-15 18:32:54] Decode batch, #running-req: 1, #token: 276, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.35, #queue-req: 0, 
[2025-12-15 18:32:54] INFO:     127.0.0.1:48480 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:54] Prefill batch, #new-seq: 1, #new-token: 1063, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:54] Decode batch, #running-req: 1, #token: 1137, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.86, #queue-req: 0, 
[2025-12-15 18:32:54] INFO:     127.0.0.1:48484 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:54] Prefill batch, #new-seq: 1, #new-token: 25, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:55] Decode batch, #running-req: 1, #token: 259, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.27, #queue-req: 0, 
[2025-12-15 18:32:55] INFO:     127.0.0.1:48488 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:55] Prefill batch, #new-seq: 1, #new-token: 1013, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:55] Decode batch, #running-req: 1, #token: 1075, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.85, #queue-req: 0, 
[2025-12-15 18:32:56] INFO:     127.0.0.1:48492 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:56] Prefill batch, #new-seq: 1, #new-token: 8, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:56] Decode batch, #running-req: 1, #token: 231, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.05, #queue-req: 0, 
[2025-12-15 18:32:56] Decode batch, #running-req: 1, #token: 271, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.38, #queue-req: 0, 
[2025-12-15 18:32:56] INFO:     127.0.0.1:48496 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:56] Prefill batch, #new-seq: 1, #new-token: 974, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:57] Decode batch, #running-req: 1, #token: 1065, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.64, #queue-req: 0, 
[2025-12-15 18:32:57] INFO:     127.0.0.1:48500 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:57] Prefill batch, #new-seq: 1, #new-token: 24, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:57] Decode batch, #running-req: 1, #token: 270, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.48, #queue-req: 0, 
[2025-12-15 18:32:58] INFO:     127.0.0.1:48504 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:58] Prefill batch, #new-seq: 1, #new-token: 1051, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:58] Decode batch, #running-req: 1, #token: 1134, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.73, #queue-req: 0, 
[2025-12-15 18:32:58] INFO:     127.0.0.1:48508 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:58] Prefill batch, #new-seq: 1, #new-token: 32, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:32:58] Decode batch, #running-req: 1, #token: 275, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.24, #queue-req: 0, 
[2025-12-15 18:32:59] Decode batch, #running-req: 1, #token: 315, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.23, #queue-req: 0, 
[2025-12-15 18:32:59] INFO:     127.0.0.1:48512 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:32:59] Prefill batch, #new-seq: 1, #new-token: 1177, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:00] Decode batch, #running-req: 1, #token: 1270, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.15, #queue-req: 0, 
[2025-12-15 18:33:00] INFO:     127.0.0.1:48516 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:00] Prefill batch, #new-seq: 1, #new-token: 10, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:00] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.50, #queue-req: 0, 
[2025-12-15 18:33:00] INFO:     127.0.0.1:48520 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:00] Prefill batch, #new-seq: 1, #new-token: 982, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:01] Decode batch, #running-req: 1, #token: 1067, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.72, #queue-req: 0, 
[2025-12-15 18:33:01] INFO:     127.0.0.1:48526 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:01] Prefill batch, #new-seq: 1, #new-token: 15, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:01] Decode batch, #running-req: 1, #token: 259, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.40, #queue-req: 0, 
[2025-12-15 18:33:02] INFO:     127.0.0.1:48530 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:02] Prefill batch, #new-seq: 1, #new-token: 1013, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:02] Decode batch, #running-req: 1, #token: 1079, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.76, #queue-req: 0, 
[2025-12-15 18:33:02] INFO:     127.0.0.1:48534 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:02] Prefill batch, #new-seq: 1, #new-token: 10, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:02] Decode batch, #running-req: 1, #token: 233, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.08, #queue-req: 0, 
[2025-12-15 18:33:03] Decode batch, #running-req: 1, #token: 273, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.38, #queue-req: 0, 
[2025-12-15 18:33:03] INFO:     127.0.0.1:48538 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:03] Prefill batch, #new-seq: 1, #new-token: 945, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:03] Decode batch, #running-req: 1, #token: 1025, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.85, #queue-req: 0, 
[2025-12-15 18:33:04] INFO:     127.0.0.1:48542 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:04] Prefill batch, #new-seq: 1, #new-token: 24, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:04] Decode batch, #running-req: 1, #token: 258, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.31, #queue-req: 0, 
[2025-12-15 18:33:04] INFO:     127.0.0.1:48546 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:04] Prefill batch, #new-seq: 1, #new-token: 1076, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:04] Decode batch, #running-req: 1, #token: 1150, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.67, #queue-req: 0, 
[2025-12-15 18:33:05] INFO:     127.0.0.1:48550 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:05] Prefill batch, #new-seq: 1, #new-token: 17, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:05] Decode batch, #running-req: 1, #token: 254, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.36, #queue-req: 0, 
[2025-12-15 18:33:05] Decode batch, #running-req: 1, #token: 294, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:33:06] INFO:     127.0.0.1:48554 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:06] Prefill batch, #new-seq: 1, #new-token: 1067, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:06] Decode batch, #running-req: 1, #token: 1155, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.66, #queue-req: 0, 
[2025-12-15 18:33:06] INFO:     127.0.0.1:49716 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:06] Prefill batch, #new-seq: 1, #new-token: 51, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:07] Decode batch, #running-req: 1, #token: 300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.34, #queue-req: 0, 
[2025-12-15 18:33:07] Decode batch, #running-req: 1, #token: 340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:33:08] INFO:     127.0.0.1:49720 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:08] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.81, #queue-req: 0, 
[2025-12-15 18:33:08] Prefill batch, #new-seq: 1, #new-token: 1304, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:08] Decode batch, #running-req: 1, #token: 1401, token usage: 0.00, cuda graph: True, gen throughput (token/s): 67.33, #queue-req: 0, 
[2025-12-15 18:33:08] INFO:     127.0.0.1:49724 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:08] Prefill batch, #new-seq: 1, #new-token: 26, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:09] Decode batch, #running-req: 1, #token: 281, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.33, #queue-req: 0, 
[2025-12-15 18:33:09] Decode batch, #running-req: 1, #token: 321, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:33:09] INFO:     127.0.0.1:49728 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:09] Prefill batch, #new-seq: 1, #new-token: 1054, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:10] Decode batch, #running-req: 1, #token: 1144, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.67, #queue-req: 0, 
[2025-12-15 18:33:10] INFO:     127.0.0.1:49732 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:10] Prefill batch, #new-seq: 1, #new-token: 9, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:10] Decode batch, #running-req: 1, #token: 262, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.39, #queue-req: 0, 
[2025-12-15 18:33:11] Decode batch, #running-req: 1, #token: 302, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.12, #queue-req: 0, 
[2025-12-15 18:33:11] Decode batch, #running-req: 1, #token: 342, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.09, #queue-req: 0, 
[2025-12-15 18:33:12] Decode batch, #running-req: 1, #token: 382, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.00, #queue-req: 0, 
[2025-12-15 18:33:12] Decode batch, #running-req: 1, #token: 422, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.93, #queue-req: 0, 
[2025-12-15 18:33:13] Decode batch, #running-req: 1, #token: 462, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.92, #queue-req: 0, 
[2025-12-15 18:33:13] Decode batch, #running-req: 1, #token: 502, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.90, #queue-req: 0, 
[2025-12-15 18:33:14] Decode batch, #running-req: 1, #token: 542, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.89, #queue-req: 0, 
[2025-12-15 18:33:14] Decode batch, #running-req: 1, #token: 582, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.88, #queue-req: 0, 
[2025-12-15 18:33:15] Decode batch, #running-req: 1, #token: 622, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.87, #queue-req: 0, 
[2025-12-15 18:33:15] Decode batch, #running-req: 1, #token: 662, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.87, #queue-req: 0, 
[2025-12-15 18:33:16] Decode batch, #running-req: 1, #token: 702, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.85, #queue-req: 0, 
[2025-12-15 18:33:16] Decode batch, #running-req: 1, #token: 742, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.84, #queue-req: 0, 
[2025-12-15 18:33:17] Decode batch, #running-req: 1, #token: 782, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.84, #queue-req: 0, 
[2025-12-15 18:33:17] Decode batch, #running-req: 1, #token: 822, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.85, #queue-req: 0, 
[2025-12-15 18:33:18] Decode batch, #running-req: 1, #token: 862, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.84, #queue-req: 0, 
[2025-12-15 18:33:18] Decode batch, #running-req: 1, #token: 902, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.83, #queue-req: 0, 
[2025-12-15 18:33:19] Decode batch, #running-req: 1, #token: 942, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.83, #queue-req: 0, 
[2025-12-15 18:33:19] Decode batch, #running-req: 1, #token: 982, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.83, #queue-req: 0, 
[2025-12-15 18:33:20] Decode batch, #running-req: 1, #token: 1022, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.81, #queue-req: 0, 
[2025-12-15 18:33:20] Decode batch, #running-req: 1, #token: 1062, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.76, #queue-req: 0, 
[2025-12-15 18:33:21] Decode batch, #running-req: 1, #token: 1102, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.75, #queue-req: 0, 
[2025-12-15 18:33:21] Decode batch, #running-req: 1, #token: 1142, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.73, #queue-req: 0, 
[2025-12-15 18:33:22] Decode batch, #running-req: 1, #token: 1182, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.71, #queue-req: 0, 
[2025-12-15 18:33:22] Decode batch, #running-req: 1, #token: 1222, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.70, #queue-req: 0, 
[2025-12-15 18:33:22] INFO:     127.0.0.1:49736 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:22] Prefill batch, #new-seq: 1, #new-token: 924, #cached-token: 69, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:23] Decode batch, #running-req: 1, #token: 1026, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.49, #queue-req: 0, 
[2025-12-15 18:33:23] INFO:     127.0.0.1:49742 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:23] Prefill batch, #new-seq: 1, #new-token: 24, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:23] Decode batch, #running-req: 1, #token: 279, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.52, #queue-req: 0, 
[2025-12-15 18:33:24] INFO:     127.0.0.1:49746 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:24] Prefill batch, #new-seq: 1, #new-token: 978, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:24] Decode batch, #running-req: 1, #token: 1056, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.75, #queue-req: 0, 
[2025-12-15 18:33:24] INFO:     127.0.0.1:49750 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:24] Prefill batch, #new-seq: 1, #new-token: 5, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:24] Decode batch, #running-req: 1, #token: 247, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.36, #queue-req: 0, 
[2025-12-15 18:33:25] INFO:     127.0.0.1:49756 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:25] Prefill batch, #new-seq: 1, #new-token: 908, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:25] Decode batch, #running-req: 1, #token: 985, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.94, #queue-req: 0, 
[2025-12-15 18:33:25] INFO:     127.0.0.1:49760 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:25] Prefill batch, #new-seq: 1, #new-token: 24, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:26] Decode batch, #running-req: 1, #token: 255, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.31, #queue-req: 0, 
[2025-12-15 18:33:26] Decode batch, #running-req: 1, #token: 295, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.21, #queue-req: 0, 
[2025-12-15 18:33:26] INFO:     127.0.0.1:49764 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:26] Prefill batch, #new-seq: 1, #new-token: 871, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:27] Decode batch, #running-req: 1, #token: 962, token usage: 0.00, cuda graph: True, gen throughput (token/s): 70.87, #queue-req: 0, 
[2025-12-15 18:33:27] INFO:     127.0.0.1:49768 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:27] Prefill batch, #new-seq: 1, #new-token: 55, #cached-token: 223, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:27] Decode batch, #running-req: 1, #token: 315, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.39, #queue-req: 0, 
[2025-12-15 18:33:28] INFO:     127.0.0.1:49772 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:28] Prefill batch, #new-seq: 1, #new-token: 1021, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:28] Decode batch, #running-req: 1, #token: 1082, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.70, #queue-req: 0, 
[2025-12-15 18:33:28] INFO:     127.0.0.1:49776 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:28] Prefill batch, #new-seq: 1, #new-token: 79, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:28] Decode batch, #running-req: 1, #token: 310, token usage: 0.00, cuda graph: True, gen throughput (token/s): 75.95, #queue-req: 0, 
[2025-12-15 18:33:29] Decode batch, #running-req: 1, #token: 350, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.14, #queue-req: 0, 
[2025-12-15 18:33:29] INFO:     127.0.0.1:49780 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:29] Prefill batch, #new-seq: 1, #new-token: 1194, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:29] Decode batch, #running-req: 1, #token: 1287, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.07, #queue-req: 0, 
[2025-12-15 18:33:29] INFO:     127.0.0.1:49784 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:29] Prefill batch, #new-seq: 1, #new-token: 38, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:30] Decode batch, #running-req: 1, #token: 293, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.35, #queue-req: 0, 
[2025-12-15 18:33:30] INFO:     127.0.0.1:49788 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:30] Prefill batch, #new-seq: 1, #new-token: 1219, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:30] Decode batch, #running-req: 1, #token: 1281, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.41, #queue-req: 0, 
[2025-12-15 18:33:31] INFO:     127.0.0.1:49794 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:31] Prefill batch, #new-seq: 1, #new-token: 50, #cached-token: 222, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:31] Decode batch, #running-req: 1, #token: 280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 75.99, #queue-req: 0, 
[2025-12-15 18:33:32] Decode batch, #running-req: 1, #token: 320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.17, #queue-req: 0, 
[2025-12-15 18:33:32] Decode batch, #running-req: 1, #token: 360, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.10, #queue-req: 0, 
[2025-12-15 18:33:32] INFO:     127.0.0.1:49798 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:32] Prefill batch, #new-seq: 1, #new-token: 1176, #cached-token: 61, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:33] Decode batch, #running-req: 1, #token: 1262, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.23, #queue-req: 0, 
[2025-12-15 18:33:33] INFO:     127.0.0.1:49802 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:33] Prefill batch, #new-seq: 1, #new-token: 56, #cached-token: 226, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:33] INFO:     127.0.0.1:49806 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:33] Prefill batch, #new-seq: 1, #new-token: 1084, #cached-token: 71, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:33] Decode batch, #running-req: 1, #token: 1170, token usage: 0.00, cuda graph: True, gen throughput (token/s): 65.87, #queue-req: 0, 
[2025-12-15 18:33:34] INFO:     127.0.0.1:49810 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:34] Prefill batch, #new-seq: 1, #new-token: 27, #cached-token: 223, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:34] Decode batch, #running-req: 1, #token: 258, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.15, #queue-req: 0, 
[2025-12-15 18:33:34] Decode batch, #running-req: 1, #token: 298, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:33:35] Decode batch, #running-req: 1, #token: 338, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.16, #queue-req: 0, 
[2025-12-15 18:33:35] INFO:     127.0.0.1:49816 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:35] Prefill batch, #new-seq: 1, #new-token: 1170, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:35] Decode batch, #running-req: 1, #token: 1255, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.23, #queue-req: 0, 
[2025-12-15 18:33:35] INFO:     127.0.0.1:49820 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:35] Prefill batch, #new-seq: 1, #new-token: 42, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:36] Decode batch, #running-req: 1, #token: 285, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.27, #queue-req: 0, 
[2025-12-15 18:33:36] Decode batch, #running-req: 1, #token: 325, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.21, #queue-req: 0, 
[2025-12-15 18:33:37] Decode batch, #running-req: 1, #token: 365, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.10, #queue-req: 0, 
[2025-12-15 18:33:37] INFO:     127.0.0.1:49824 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:37] Prefill batch, #new-seq: 1, #new-token: 1191, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:37] Decode batch, #running-req: 1, #token: 1284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.20, #queue-req: 0, 
[2025-12-15 18:33:38] INFO:     127.0.0.1:49828 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:38] Prefill batch, #new-seq: 1, #new-token: 44, #cached-token: 226, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:38] Decode batch, #running-req: 1, #token: 295, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.13, #queue-req: 0, 
[2025-12-15 18:33:38] Decode batch, #running-req: 1, #token: 335, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.16, #queue-req: 0, 
[2025-12-15 18:33:39] INFO:     127.0.0.1:49832 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:39] Prefill batch, #new-seq: 1, #new-token: 1161, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:39] Decode batch, #running-req: 1, #token: 1246, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.23, #queue-req: 0, 
[2025-12-15 18:33:39] INFO:     127.0.0.1:49836 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:39] Prefill batch, #new-seq: 1, #new-token: 61, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:40] Decode batch, #running-req: 1, #token: 298, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.18, #queue-req: 0, 
[2025-12-15 18:33:40] Decode batch, #running-req: 1, #token: 338, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.17, #queue-req: 0, 
[2025-12-15 18:33:40] INFO:     127.0.0.1:49840 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:40] Prefill batch, #new-seq: 1, #new-token: 1160, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:41] Decode batch, #running-req: 1, #token: 1225, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.46, #queue-req: 0, 
[2025-12-15 18:33:41] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.62, #queue-req: 0, 
[2025-12-15 18:33:41] INFO:     127.0.0.1:49844 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:41] Prefill batch, #new-seq: 1, #new-token: 33, #cached-token: 222, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:42] Decode batch, #running-req: 1, #token: 295, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.39, #queue-req: 0, 
[2025-12-15 18:33:42] Decode batch, #running-req: 1, #token: 335, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.21, #queue-req: 0, 
[2025-12-15 18:33:43] INFO:     127.0.0.1:49848 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:43] Prefill batch, #new-seq: 1, #new-token: 1224, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:43] Decode batch, #running-req: 1, #token: 1291, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.24, #queue-req: 0, 
[2025-12-15 18:33:43] INFO:     127.0.0.1:49852 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:43] Prefill batch, #new-seq: 1, #new-token: 40, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:43] Decode batch, #running-req: 1, #token: 272, token usage: 0.00, cuda graph: True, gen throughput (token/s): 75.96, #queue-req: 0, 
[2025-12-15 18:33:44] Decode batch, #running-req: 1, #token: 312, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:33:44] INFO:     127.0.0.1:49856 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:44] Prefill batch, #new-seq: 1, #new-token: 1142, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:44] Decode batch, #running-req: 1, #token: 1212, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.63, #queue-req: 0, 
[2025-12-15 18:33:45] INFO:     127.0.0.1:49860 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:45] Prefill batch, #new-seq: 1, #new-token: 17, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:45] Decode batch, #running-req: 1, #token: 249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.15, #queue-req: 0, 
[2025-12-15 18:33:45] Decode batch, #running-req: 1, #token: 289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.26, #queue-req: 0, 
[2025-12-15 18:33:46] Decode batch, #running-req: 1, #token: 329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:33:46] INFO:     127.0.0.1:49864 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:46] Prefill batch, #new-seq: 1, #new-token: 1037, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:46] Decode batch, #running-req: 1, #token: 1131, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.61, #queue-req: 0, 
[2025-12-15 18:33:46] INFO:     127.0.0.1:49868 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:46] Prefill batch, #new-seq: 1, #new-token: 62, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:47] Decode batch, #running-req: 1, #token: 321, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.41, #queue-req: 0, 
[2025-12-15 18:33:47] INFO:     127.0.0.1:49874 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:47] Prefill batch, #new-seq: 1, #new-token: 1114, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:48] Decode batch, #running-req: 1, #token: 1187, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.64, #queue-req: 0, 
[2025-12-15 18:33:48] INFO:     127.0.0.1:49878 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:48] Prefill batch, #new-seq: 1, #new-token: 37, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:48] Decode batch, #running-req: 1, #token: 263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.00, #queue-req: 0, 
[2025-12-15 18:33:49] Decode batch, #running-req: 1, #token: 303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:33:49] Decode batch, #running-req: 1, #token: 343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.15, #queue-req: 0, 
[2025-12-15 18:33:49] INFO:     127.0.0.1:49882 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:49] Prefill batch, #new-seq: 1, #new-token: 1260, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:50] Decode batch, #running-req: 1, #token: 1349, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.04, #queue-req: 0, 
[2025-12-15 18:33:50] INFO:     127.0.0.1:49886 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:50] Prefill batch, #new-seq: 1, #new-token: 48, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:50] Decode batch, #running-req: 1, #token: 300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.29, #queue-req: 0, 
[2025-12-15 18:33:51] Decode batch, #running-req: 1, #token: 340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:33:51] INFO:     127.0.0.1:49890 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:51] Prefill batch, #new-seq: 1, #new-token: 1065, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:51] Decode batch, #running-req: 1, #token: 1157, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.64, #queue-req: 0, 
[2025-12-15 18:33:51] INFO:     127.0.0.1:49894 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:51] Prefill batch, #new-seq: 1, #new-token: 66, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:52] Decode batch, #running-req: 1, #token: 318, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.26, #queue-req: 0, 
[2025-12-15 18:33:52] INFO:     127.0.0.1:49898 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:52] Prefill batch, #new-seq: 1, #new-token: 1200, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:52] Decode batch, #running-req: 1, #token: 1271, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.24, #queue-req: 0, 
[2025-12-15 18:33:53] INFO:     127.0.0.1:49902 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:53] Prefill batch, #new-seq: 1, #new-token: 14, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:53] Decode batch, #running-req: 1, #token: 233, token usage: 0.00, cuda graph: True, gen throughput (token/s): 75.97, #queue-req: 0, 
[2025-12-15 18:33:53] Decode batch, #running-req: 1, #token: 273, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.36, #queue-req: 0, 
[2025-12-15 18:33:54] INFO:     127.0.0.1:49906 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:54] Prefill batch, #new-seq: 1, #new-token: 980, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:54] Decode batch, #running-req: 1, #token: 1047, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.84, #queue-req: 0, 
[2025-12-15 18:33:54] INFO:     127.0.0.1:49910 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:54] Prefill batch, #new-seq: 1, #new-token: 25, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:55] Decode batch, #running-req: 1, #token: 254, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.29, #queue-req: 0, 
[2025-12-15 18:33:55] Decode batch, #running-req: 1, #token: 294, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.27, #queue-req: 0, 
[2025-12-15 18:33:55] INFO:     127.0.0.1:49914 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:55] Prefill batch, #new-seq: 1, #new-token: 1016, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:56] Decode batch, #running-req: 1, #token: 1091, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.69, #queue-req: 0, 
[2025-12-15 18:33:56] INFO:     127.0.0.1:49918 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:56] Prefill batch, #new-seq: 1, #new-token: 18, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:56] Decode batch, #running-req: 1, #token: 244, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.23, #queue-req: 0, 
[2025-12-15 18:33:57] Decode batch, #running-req: 1, #token: 284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.27, #queue-req: 0, 
[2025-12-15 18:33:57] INFO:     127.0.0.1:49922 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:57] Prefill batch, #new-seq: 1, #new-token: 915, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:57] Decode batch, #running-req: 1, #token: 1007, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.77, #queue-req: 0, 
[2025-12-15 18:33:57] INFO:     127.0.0.1:49926 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:57] Prefill batch, #new-seq: 1, #new-token: 16, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:58] Decode batch, #running-req: 1, #token: 261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.44, #queue-req: 0, 
[2025-12-15 18:33:58] INFO:     127.0.0.1:49930 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:58] Prefill batch, #new-seq: 1, #new-token: 885, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:58] Decode batch, #running-req: 1, #token: 946, token usage: 0.00, cuda graph: True, gen throughput (token/s): 71.10, #queue-req: 0, 
[2025-12-15 18:33:59] INFO:     127.0.0.1:49934 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:59] Prefill batch, #new-seq: 1, #new-token: 44, #cached-token: 223, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:33:59] Decode batch, #running-req: 1, #token: 272, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.02, #queue-req: 0, 
[2025-12-15 18:33:59] Decode batch, #running-req: 1, #token: 312, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:33:59] INFO:     127.0.0.1:49938 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:33:59] Prefill batch, #new-seq: 1, #new-token: 1193, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:00] Decode batch, #running-req: 1, #token: 1284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.11, #queue-req: 0, 
[2025-12-15 18:34:00] INFO:     127.0.0.1:49942 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:00] Prefill batch, #new-seq: 1, #new-token: 16, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:00] Decode batch, #running-req: 1, #token: 270, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.47, #queue-req: 0, 
[2025-12-15 18:34:01] Decode batch, #running-req: 1, #token: 310, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:34:01] INFO:     127.0.0.1:49946 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:01] Prefill batch, #new-seq: 1, #new-token: 942, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:01] Decode batch, #running-req: 1, #token: 1013, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.92, #queue-req: 0, 
[2025-12-15 18:34:02] INFO:     127.0.0.1:49952 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:02] Prefill batch, #new-seq: 1, #new-token: 47, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:02] Decode batch, #running-req: 1, #token: 281, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.16, #queue-req: 0, 
[2025-12-15 18:34:02] Decode batch, #running-req: 1, #token: 321, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:34:03] Decode batch, #running-req: 1, #token: 361, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.12, #queue-req: 0, 
[2025-12-15 18:34:03] INFO:     127.0.0.1:49956 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:03] Prefill batch, #new-seq: 1, #new-token: 1268, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:04] Decode batch, #running-req: 1, #token: 1357, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.03, #queue-req: 0, 
[2025-12-15 18:34:04] INFO:     127.0.0.1:49960 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:04] Prefill batch, #new-seq: 1, #new-token: 36, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:04] Decode batch, #running-req: 1, #token: 279, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.16, #queue-req: 0, 
[2025-12-15 18:34:04] INFO:     127.0.0.1:49964 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:04] Prefill batch, #new-seq: 1, #new-token: 1103, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:05] Decode batch, #running-req: 1, #token: 1180, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.69, #queue-req: 0, 
[2025-12-15 18:34:05] INFO:     127.0.0.1:49968 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:05] Prefill batch, #new-seq: 1, #new-token: 51, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:05] Decode batch, #running-req: 1, #token: 283, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.14, #queue-req: 0, 
[2025-12-15 18:34:06] Decode batch, #running-req: 1, #token: 323, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:34:06] Decode batch, #running-req: 1, #token: 363, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.10, #queue-req: 0, 
[2025-12-15 18:34:06] INFO:     127.0.0.1:49972 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:06] Prefill batch, #new-seq: 1, #new-token: 1180, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:07] Decode batch, #running-req: 1, #token: 1275, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.16, #queue-req: 0, 
[2025-12-15 18:34:07] INFO:     127.0.0.1:49976 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:07] Prefill batch, #new-seq: 1, #new-token: 11, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:07] Decode batch, #running-req: 1, #token: 269, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.41, #queue-req: 0, 
[2025-12-15 18:34:08] Decode batch, #running-req: 1, #token: 309, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:34:08] INFO:     127.0.0.1:49980 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:08] Prefill batch, #new-seq: 1, #new-token: 1141, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:08] Decode batch, #running-req: 1, #token: 1223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.65, #queue-req: 0, 
[2025-12-15 18:34:09] INFO:     127.0.0.1:49984 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:09] Prefill batch, #new-seq: 1, #new-token: 60, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:09] Decode batch, #running-req: 1, #token: 298, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.15, #queue-req: 0, 
[2025-12-15 18:34:09] Decode batch, #running-req: 1, #token: 338, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:34:10] INFO:     127.0.0.1:49988 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:10] Prefill batch, #new-seq: 1, #new-token: 1316, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:10] Decode batch, #running-req: 1, #token: 1381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 67.40, #queue-req: 0, 
[2025-12-15 18:34:11] INFO:     127.0.0.1:49992 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:11] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.24, #queue-req: 0, 
[2025-12-15 18:34:11] Prefill batch, #new-seq: 1, #new-token: 53, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:11] Decode batch, #running-req: 1, #token: 312, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.79, #queue-req: 0, 
[2025-12-15 18:34:12] Decode batch, #running-req: 1, #token: 352, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.15, #queue-req: 0, 
[2025-12-15 18:34:12] INFO:     127.0.0.1:49996 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:12] Prefill batch, #new-seq: 1, #new-token: 1211, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:12] Decode batch, #running-req: 1, #token: 1288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.20, #queue-req: 0, 
[2025-12-15 18:34:12] INFO:     127.0.0.1:50000 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:12] Prefill batch, #new-seq: 1, #new-token: 39, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:13] Decode batch, #running-req: 1, #token: 273, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.16, #queue-req: 0, 
[2025-12-15 18:34:13] Decode batch, #running-req: 1, #token: 313, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.21, #queue-req: 0, 
[2025-12-15 18:34:14] INFO:     127.0.0.1:50004 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:14] Prefill batch, #new-seq: 1, #new-token: 1221, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:14] Decode batch, #running-req: 1, #token: 1284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.43, #queue-req: 0, 
[2025-12-15 18:34:14] INFO:     127.0.0.1:50008 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:14] Prefill batch, #new-seq: 1, #new-token: 26, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:14] Decode batch, #running-req: 1, #token: 250, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.04, #queue-req: 0, 
[2025-12-15 18:34:15] INFO:     127.0.0.1:50012 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:15] Prefill batch, #new-seq: 1, #new-token: 1150, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:15] Decode batch, #running-req: 1, #token: 1209, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.83, #queue-req: 0, 
[2025-12-15 18:34:15] Decode batch, #running-req: 1, #token: 1249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.66, #queue-req: 0, 
[2025-12-15 18:34:15] INFO:     127.0.0.1:50016 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:15] Prefill batch, #new-seq: 1, #new-token: 20, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:16] Decode batch, #running-req: 1, #token: 271, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.46, #queue-req: 0, 
[2025-12-15 18:34:16] Decode batch, #running-req: 1, #token: 311, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.21, #queue-req: 0, 
[2025-12-15 18:34:17] INFO:     127.0.0.1:50020 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:17] Prefill batch, #new-seq: 1, #new-token: 1052, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:17] Decode batch, #running-req: 1, #token: 1138, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.73, #queue-req: 0, 
[2025-12-15 18:34:17] INFO:     127.0.0.1:50024 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:17] Prefill batch, #new-seq: 1, #new-token: 39, #cached-token: 230, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:17] Decode batch, #running-req: 1, #token: 301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.38, #queue-req: 0, 
[2025-12-15 18:34:18] INFO:     127.0.0.1:50028 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:18] Prefill batch, #new-seq: 1, #new-token: 1077, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:18] Decode batch, #running-req: 1, #token: 1160, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.72, #queue-req: 0, 
[2025-12-15 18:34:18] INFO:     127.0.0.1:50032 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:18] Prefill batch, #new-seq: 1, #new-token: 57, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:19] Decode batch, #running-req: 1, #token: 304, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.26, #queue-req: 0, 
[2025-12-15 18:34:19] Decode batch, #running-req: 1, #token: 344, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.17, #queue-req: 0, 
[2025-12-15 18:34:19] INFO:     127.0.0.1:50036 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:19] Prefill batch, #new-seq: 1, #new-token: 1104, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:20] Decode batch, #running-req: 1, #token: 1200, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.50, #queue-req: 0, 
[2025-12-15 18:34:20] INFO:     127.0.0.1:50040 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:20] Prefill batch, #new-seq: 1, #new-token: 43, #cached-token: 225, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:20] Decode batch, #running-req: 1, #token: 306, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.33, #queue-req: 0, 
[2025-12-15 18:34:20] INFO:     127.0.0.1:50044 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:20] Prefill batch, #new-seq: 1, #new-token: 1033, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:21] Decode batch, #running-req: 1, #token: 1123, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.71, #queue-req: 0, 
[2025-12-15 18:34:21] INFO:     127.0.0.1:50048 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:21] Prefill batch, #new-seq: 1, #new-token: 20, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:21] Decode batch, #running-req: 1, #token: 268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.50, #queue-req: 0, 
[2025-12-15 18:34:22] INFO:     127.0.0.1:50054 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:22] Prefill batch, #new-seq: 1, #new-token: 942, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:22] Decode batch, #running-req: 1, #token: 1019, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.91, #queue-req: 0, 
[2025-12-15 18:34:22] INFO:     127.0.0.1:50058 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:22] Prefill batch, #new-seq: 1, #new-token: 34, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:22] Decode batch, #running-req: 1, #token: 272, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.33, #queue-req: 0, 
[2025-12-15 18:34:23] Decode batch, #running-req: 1, #token: 312, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:34:23] INFO:     127.0.0.1:50062 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:23] Prefill batch, #new-seq: 1, #new-token: 1041, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:23] Decode batch, #running-req: 1, #token: 1112, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.85, #queue-req: 0, 
[2025-12-15 18:34:24] INFO:     127.0.0.1:50066 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:24] Prefill batch, #new-seq: 1, #new-token: 35, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:24] Decode batch, #running-req: 1, #token: 259, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.02, #queue-req: 0, 
[2025-12-15 18:34:25] Decode batch, #running-req: 1, #token: 299, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:34:25] Decode batch, #running-req: 1, #token: 339, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:34:25] INFO:     127.0.0.1:50070 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:25] Prefill batch, #new-seq: 1, #new-token: 1205, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:26] Decode batch, #running-req: 1, #token: 1288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.19, #queue-req: 0, 
[2025-12-15 18:34:26] INFO:     127.0.0.1:50074 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:26] Prefill batch, #new-seq: 1, #new-token: 20, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:26] Decode batch, #running-req: 1, #token: 249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.18, #queue-req: 0, 
[2025-12-15 18:34:26] INFO:     127.0.0.1:50078 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:26] Prefill batch, #new-seq: 1, #new-token: 923, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:27] Decode batch, #running-req: 1, #token: 994, token usage: 0.00, cuda graph: True, gen throughput (token/s): 70.02, #queue-req: 0, 
[2025-12-15 18:34:27] INFO:     127.0.0.1:50082 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:27] Prefill batch, #new-seq: 1, #new-token: 49, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:27] Decode batch, #running-req: 1, #token: 291, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.26, #queue-req: 0, 
[2025-12-15 18:34:28] Decode batch, #running-req: 1, #token: 331, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:34:28] INFO:     127.0.0.1:50086 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:28] Prefill batch, #new-seq: 1, #new-token: 1265, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:28] Decode batch, #running-req: 1, #token: 1325, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.21, #queue-req: 0, 
[2025-12-15 18:34:29] INFO:     127.0.0.1:50090 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:29] Prefill batch, #new-seq: 1, #new-token: 14, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:29] Decode batch, #running-req: 1, #token: 238, token usage: 0.00, cuda graph: True, gen throughput (token/s): 75.94, #queue-req: 0, 
[2025-12-15 18:34:29] Decode batch, #running-req: 1, #token: 278, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.35, #queue-req: 0, 
[2025-12-15 18:34:30] Decode batch, #running-req: 1, #token: 318, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.23, #queue-req: 0, 
[2025-12-15 18:34:30] INFO:     127.0.0.1:50094 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:30] Prefill batch, #new-seq: 1, #new-token: 982, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:30] Decode batch, #running-req: 1, #token: 1066, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.70, #queue-req: 0, 
[2025-12-15 18:34:31] INFO:     127.0.0.1:50098 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:31] Prefill batch, #new-seq: 1, #new-token: 33, #cached-token: 226, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:31] Decode batch, #running-req: 1, #token: 283, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.24, #queue-req: 0, 
[2025-12-15 18:34:31] INFO:     127.0.0.1:50102 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:31] Prefill batch, #new-seq: 1, #new-token: 1094, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:31] Decode batch, #running-req: 1, #token: 1151, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.88, #queue-req: 0, 
[2025-12-15 18:34:32] INFO:     127.0.0.1:50106 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:32] Prefill batch, #new-seq: 1, #new-token: 11, #cached-token: 223, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:32] Decode batch, #running-req: 1, #token: 238, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.05, #queue-req: 0, 
[2025-12-15 18:34:33] Decode batch, #running-req: 1, #token: 278, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.36, #queue-req: 0, 
[2025-12-15 18:34:33] INFO:     127.0.0.1:50110 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:33] Prefill batch, #new-seq: 1, #new-token: 1003, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:33] Decode batch, #running-req: 1, #token: 1073, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.75, #queue-req: 0, 
[2025-12-15 18:34:33] INFO:     127.0.0.1:50114 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:33] Prefill batch, #new-seq: 1, #new-token: 32, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:34] Decode batch, #running-req: 1, #token: 265, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.24, #queue-req: 0, 
[2025-12-15 18:34:34] Decode batch, #running-req: 1, #token: 305, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.24, #queue-req: 0, 
[2025-12-15 18:34:34] INFO:     127.0.0.1:50118 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:34] Prefill batch, #new-seq: 1, #new-token: 1136, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:35] Decode batch, #running-req: 1, #token: 1225, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.55, #queue-req: 0, 
[2025-12-15 18:34:35] INFO:     127.0.0.1:50122 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:35] Prefill batch, #new-seq: 1, #new-token: 14, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:35] Decode batch, #running-req: 1, #token: 263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.51, #queue-req: 0, 
[2025-12-15 18:34:36] INFO:     127.0.0.1:50126 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:36] Prefill batch, #new-seq: 1, #new-token: 976, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:36] Decode batch, #running-req: 1, #token: 1035, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.99, #queue-req: 0, 
[2025-12-15 18:34:36] INFO:     127.0.0.1:50132 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:36] Prefill batch, #new-seq: 1, #new-token: 21, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:36] Decode batch, #running-req: 1, #token: 243, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.09, #queue-req: 0, 
[2025-12-15 18:34:37] Decode batch, #running-req: 1, #token: 283, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.31, #queue-req: 0, 
[2025-12-15 18:34:37] Decode batch, #running-req: 1, #token: 323, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:34:37] INFO:     127.0.0.1:50136 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:37] Prefill batch, #new-seq: 1, #new-token: 1032, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:38] Decode batch, #running-req: 1, #token: 1121, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.76, #queue-req: 0, 
[2025-12-15 18:34:38] INFO:     127.0.0.1:50140 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:38] Prefill batch, #new-seq: 1, #new-token: 20, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:38] Decode batch, #running-req: 1, #token: 265, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.50, #queue-req: 0, 
[2025-12-15 18:34:39] INFO:     127.0.0.1:50144 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:39] Prefill batch, #new-seq: 1, #new-token: 972, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:39] Decode batch, #running-req: 1, #token: 1039, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.86, #queue-req: 0, 
[2025-12-15 18:34:39] INFO:     127.0.0.1:50148 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:39] Prefill batch, #new-seq: 1, #new-token: 66, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:40] Decode batch, #running-req: 1, #token: 299, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.13, #queue-req: 0, 
[2025-12-15 18:34:40] Decode batch, #running-req: 1, #token: 339, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:34:40] INFO:     127.0.0.1:50152 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:40] Prefill batch, #new-seq: 1, #new-token: 1087, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:41] Decode batch, #running-req: 1, #token: 1161, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.60, #queue-req: 0, 
[2025-12-15 18:34:41] INFO:     127.0.0.1:50156 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:41] Prefill batch, #new-seq: 1, #new-token: 32, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:41] Decode batch, #running-req: 1, #token: 259, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.13, #queue-req: 0, 
[2025-12-15 18:34:42] Decode batch, #running-req: 1, #token: 299, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:34:42] Decode batch, #running-req: 1, #token: 339, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.17, #queue-req: 0, 
[2025-12-15 18:34:42] INFO:     127.0.0.1:50160 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:42] Prefill batch, #new-seq: 1, #new-token: 1197, #cached-token: 61, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:43] Decode batch, #running-req: 1, #token: 1276, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.22, #queue-req: 0, 
[2025-12-15 18:34:43] INFO:     127.0.0.1:50164 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:43] Prefill batch, #new-seq: 1, #new-token: 41, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:43] Decode batch, #running-req: 1, #token: 277, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.12, #queue-req: 0, 
[2025-12-15 18:34:44] Decode batch, #running-req: 1, #token: 317, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:34:44] INFO:     127.0.0.1:50168 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:44] Prefill batch, #new-seq: 1, #new-token: 1195, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:44] Decode batch, #running-req: 1, #token: 1274, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.22, #queue-req: 0, 
[2025-12-15 18:34:45] INFO:     127.0.0.1:50172 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:45] Prefill batch, #new-seq: 1, #new-token: 28, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:45] Decode batch, #running-req: 1, #token: 267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.31, #queue-req: 0, 
[2025-12-15 18:34:45] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:34:45] INFO:     127.0.0.1:50176 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:45] Prefill batch, #new-seq: 1, #new-token: 1104, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:46] Decode batch, #running-req: 1, #token: 1199, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.53, #queue-req: 0, 
[2025-12-15 18:34:46] INFO:     127.0.0.1:50180 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:46] Prefill batch, #new-seq: 1, #new-token: 68, #cached-token: 223, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:46] Decode batch, #running-req: 1, #token: 330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.41, #queue-req: 0, 
[2025-12-15 18:34:47] Decode batch, #running-req: 1, #token: 370, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.09, #queue-req: 0, 
[2025-12-15 18:34:47] INFO:     127.0.0.1:50184 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:47] Prefill batch, #new-seq: 1, #new-token: 1146, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:48] Decode batch, #running-req: 1, #token: 1224, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.45, #queue-req: 0, 
[2025-12-15 18:34:48] INFO:     127.0.0.1:50188 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:48] Prefill batch, #new-seq: 1, #new-token: 22, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:48] Decode batch, #running-req: 1, #token: 253, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.15, #queue-req: 0, 
[2025-12-15 18:34:49] Decode batch, #running-req: 1, #token: 293, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.24, #queue-req: 0, 
[2025-12-15 18:34:49] Decode batch, #running-req: 1, #token: 333, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:34:49] INFO:     127.0.0.1:50192 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:49] Prefill batch, #new-seq: 1, #new-token: 1260, #cached-token: 61, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:50] Decode batch, #running-req: 1, #token: 1346, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.10, #queue-req: 0, 
[2025-12-15 18:34:50] INFO:     127.0.0.1:50196 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:50] Prefill batch, #new-seq: 1, #new-token: 22, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:50] Decode batch, #running-req: 1, #token: 264, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.40, #queue-req: 0, 
[2025-12-15 18:34:51] Decode batch, #running-req: 1, #token: 304, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:34:51] INFO:     127.0.0.1:50200 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:51] Prefill batch, #new-seq: 1, #new-token: 1164, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:51] Decode batch, #running-req: 1, #token: 1238, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.38, #queue-req: 0, 
[2025-12-15 18:34:52] INFO:     127.0.0.1:50204 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:52] Prefill batch, #new-seq: 1, #new-token: 17, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:52] Decode batch, #running-req: 1, #token: 248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.17, #queue-req: 0, 
[2025-12-15 18:34:52] Decode batch, #running-req: 1, #token: 288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.28, #queue-req: 0, 
[2025-12-15 18:34:52] INFO:     127.0.0.1:50208 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:52] Prefill batch, #new-seq: 1, #new-token: 1148, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:53] Decode batch, #running-req: 1, #token: 1237, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.45, #queue-req: 0, 
[2025-12-15 18:34:53] INFO:     127.0.0.1:50212 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:53] Prefill batch, #new-seq: 1, #new-token: 22, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:53] Decode batch, #running-req: 1, #token: 266, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.45, #queue-req: 0, 
[2025-12-15 18:34:54] INFO:     127.0.0.1:50216 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:54] Prefill batch, #new-seq: 1, #new-token: 1168, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:54] Decode batch, #running-req: 1, #token: 1227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.59, #queue-req: 0, 
[2025-12-15 18:34:54] INFO:     127.0.0.1:50220 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:54] Prefill batch, #new-seq: 1, #new-token: 15, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:54] Decode batch, #running-req: 1, #token: 236, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.04, #queue-req: 0, 
[2025-12-15 18:34:55] Decode batch, #running-req: 1, #token: 276, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.35, #queue-req: 0, 
[2025-12-15 18:34:55] INFO:     127.0.0.1:50224 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:55] Prefill batch, #new-seq: 1, #new-token: 1201, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:56] Decode batch, #running-req: 1, #token: 1280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.22, #queue-req: 0, 
[2025-12-15 18:34:56] INFO:     127.0.0.1:50228 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:56] Prefill batch, #new-seq: 1, #new-token: 12, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:56] Decode batch, #running-req: 1, #token: 246, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.25, #queue-req: 0, 
[2025-12-15 18:34:56] INFO:     127.0.0.1:50232 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:57] Prefill batch, #new-seq: 1, #new-token: 1100, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:57] Decode batch, #running-req: 1, #token: 1164, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.88, #queue-req: 0, 
[2025-12-15 18:34:57] INFO:     127.0.0.1:50236 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:57] Prefill batch, #new-seq: 1, #new-token: 19, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:57] Decode batch, #running-req: 1, #token: 246, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.19, #queue-req: 0, 
[2025-12-15 18:34:58] INFO:     127.0.0.1:50240 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:58] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.04, #queue-req: 0, 
[2025-12-15 18:34:58] Prefill batch, #new-seq: 1, #new-token: 969, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:58] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.77, #queue-req: 0, 
[2025-12-15 18:34:58] INFO:     127.0.0.1:50244 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:58] Prefill batch, #new-seq: 1, #new-token: 12, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:59] Decode batch, #running-req: 1, #token: 273, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.63, #queue-req: 0, 
[2025-12-15 18:34:59] INFO:     127.0.0.1:50250 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:34:59] Prefill batch, #new-seq: 1, #new-token: 1102, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:34:59] Decode batch, #running-req: 1, #token: 1167, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.78, #queue-req: 0, 
[2025-12-15 18:35:00] INFO:     127.0.0.1:50254 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:00] Prefill batch, #new-seq: 1, #new-token: 15, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:00] Decode batch, #running-req: 1, #token: 240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.03, #queue-req: 0, 
[2025-12-15 18:35:00] Decode batch, #running-req: 1, #token: 280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.33, #queue-req: 0, 
[2025-12-15 18:35:01] Decode batch, #running-req: 1, #token: 320, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:35:01] INFO:     127.0.0.1:50258 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:01] Prefill batch, #new-seq: 1, #new-token: 954, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:01] Decode batch, #running-req: 1, #token: 1048, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.67, #queue-req: 0, 
[2025-12-15 18:35:02] INFO:     127.0.0.1:50264 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:02] Prefill batch, #new-seq: 1, #new-token: 27, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:02] Decode batch, #running-req: 1, #token: 283, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.39, #queue-req: 0, 
[2025-12-15 18:35:03] Decode batch, #running-req: 1, #token: 323, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.21, #queue-req: 0, 
[2025-12-15 18:35:03] INFO:     127.0.0.1:50268 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:03] Prefill batch, #new-seq: 1, #new-token: 1174, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:03] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.08, #queue-req: 0, 
[2025-12-15 18:35:03] INFO:     127.0.0.1:50272 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:03] Prefill batch, #new-seq: 1, #new-token: 14, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:04] Decode batch, #running-req: 1, #token: 272, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.60, #queue-req: 0, 
[2025-12-15 18:35:04] INFO:     127.0.0.1:50276 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:04] Prefill batch, #new-seq: 1, #new-token: 955, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:04] Decode batch, #running-req: 1, #token: 1016, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.94, #queue-req: 0, 
[2025-12-15 18:35:05] INFO:     127.0.0.1:50280 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:05] Prefill batch, #new-seq: 1, #new-token: 25, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:05] Decode batch, #running-req: 1, #token: 251, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.08, #queue-req: 0, 
[2025-12-15 18:35:05] Decode batch, #running-req: 1, #token: 291, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.29, #queue-req: 0, 
[2025-12-15 18:35:05] INFO:     127.0.0.1:50284 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:05] Prefill batch, #new-seq: 1, #new-token: 1042, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:06] Decode batch, #running-req: 1, #token: 1123, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.68, #queue-req: 0, 
[2025-12-15 18:35:06] INFO:     127.0.0.1:50288 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:06] Prefill batch, #new-seq: 1, #new-token: 17, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:06] Decode batch, #running-req: 1, #token: 261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.47, #queue-req: 0, 
[2025-12-15 18:35:07] Decode batch, #running-req: 1, #token: 301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.24, #queue-req: 0, 
[2025-12-15 18:35:07] INFO:     127.0.0.1:50292 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:07] Prefill batch, #new-seq: 1, #new-token: 1081, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:07] Decode batch, #running-req: 1, #token: 1161, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.69, #queue-req: 0, 
[2025-12-15 18:35:08] INFO:     127.0.0.1:50296 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:08] Prefill batch, #new-seq: 1, #new-token: 28, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:08] Decode batch, #running-req: 1, #token: 268, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.36, #queue-req: 0, 
[2025-12-15 18:35:08] Decode batch, #running-req: 1, #token: 308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.25, #queue-req: 0, 
[2025-12-15 18:35:09] INFO:     127.0.0.1:50300 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:09] Prefill batch, #new-seq: 1, #new-token: 1122, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:09] Decode batch, #running-req: 1, #token: 1204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.71, #queue-req: 0, 
[2025-12-15 18:35:09] INFO:     127.0.0.1:50304 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:09] Prefill batch, #new-seq: 1, #new-token: 14, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:10] Decode batch, #running-req: 1, #token: 256, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.38, #queue-req: 0, 
[2025-12-15 18:35:10] Decode batch, #running-req: 1, #token: 296, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:35:10] INFO:     127.0.0.1:50308 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:10] Prefill batch, #new-seq: 1, #new-token: 1079, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:11] Decode batch, #running-req: 1, #token: 1159, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.72, #queue-req: 0, 
[2025-12-15 18:35:11] INFO:     127.0.0.1:50312 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:11] Prefill batch, #new-seq: 1, #new-token: 44, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:11] Decode batch, #running-req: 1, #token: 283, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.28, #queue-req: 0, 
[2025-12-15 18:35:11] INFO:     127.0.0.1:50316 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:11] Prefill batch, #new-seq: 1, #new-token: 1107, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:12] Decode batch, #running-req: 1, #token: 1180, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.69, #queue-req: 0, 
[2025-12-15 18:35:12] INFO:     127.0.0.1:50320 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:12] Prefill batch, #new-seq: 1, #new-token: 30, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:12] Decode batch, #running-req: 1, #token: 258, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.17, #queue-req: 0, 
[2025-12-15 18:35:13] Decode batch, #running-req: 1, #token: 298, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.21, #queue-req: 0, 
[2025-12-15 18:35:13] Decode batch, #running-req: 1, #token: 338, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:35:13] INFO:     127.0.0.1:50324 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:13] Prefill batch, #new-seq: 1, #new-token: 1176, #cached-token: 63, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:14] Decode batch, #running-req: 1, #token: 1266, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.21, #queue-req: 0, 
[2025-12-15 18:35:14] INFO:     127.0.0.1:50330 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:14] Prefill batch, #new-seq: 1, #new-token: 16, #cached-token: 221, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:14] Decode batch, #running-req: 1, #token: 266, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.39, #queue-req: 0, 
[2025-12-15 18:35:15] Decode batch, #running-req: 1, #token: 306, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:35:15] INFO:     127.0.0.1:50334 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:15] Prefill batch, #new-seq: 1, #new-token: 928, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:15] Decode batch, #running-req: 1, #token: 1016, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.75, #queue-req: 0, 
[2025-12-15 18:35:16] INFO:     127.0.0.1:50338 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:16] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:16] Decode batch, #running-req: 1, #token: 249, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.36, #queue-req: 0, 
[2025-12-15 18:35:16] Decode batch, #running-req: 1, #token: 289, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.27, #queue-req: 0, 
[2025-12-15 18:35:17] Decode batch, #running-req: 1, #token: 329, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:35:17] INFO:     127.0.0.1:50342 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:17] Prefill batch, #new-seq: 1, #new-token: 1090, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:18] Decode batch, #running-req: 1, #token: 1176, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.68, #queue-req: 0, 
[2025-12-15 18:35:18] INFO:     127.0.0.1:50346 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:18] Prefill batch, #new-seq: 1, #new-token: 34, #cached-token: 216, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:18] Decode batch, #running-req: 1, #token: 270, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.26, #queue-req: 0, 
[2025-12-15 18:35:19] Decode batch, #running-req: 1, #token: 310, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.21, #queue-req: 0, 
[2025-12-15 18:35:19] INFO:     127.0.0.1:50350 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:19] Prefill batch, #new-seq: 1, #new-token: 1060, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:19] Decode batch, #running-req: 1, #token: 1130, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.80, #queue-req: 0, 
[2025-12-15 18:35:19] INFO:     127.0.0.1:50354 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:19] Prefill batch, #new-seq: 1, #new-token: 50, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:20] Decode batch, #running-req: 1, #token: 286, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.12, #queue-req: 0, 
[2025-12-15 18:35:20] Decode batch, #running-req: 1, #token: 326, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:35:21] INFO:     127.0.0.1:50358 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:21] Prefill batch, #new-seq: 1, #new-token: 1226, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:21] Decode batch, #running-req: 1, #token: 1291, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.27, #queue-req: 0, 
[2025-12-15 18:35:21] INFO:     127.0.0.1:50362 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:21] Prefill batch, #new-seq: 1, #new-token: 64, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:21] Decode batch, #running-req: 1, #token: 290, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.03, #queue-req: 0, 
[2025-12-15 18:35:22] Decode batch, #running-req: 1, #token: 330, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.18, #queue-req: 0, 
[2025-12-15 18:35:22] INFO:     127.0.0.1:50366 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:22] Prefill batch, #new-seq: 1, #new-token: 1135, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:22] Decode batch, #running-req: 1, #token: 1204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.62, #queue-req: 0, 
[2025-12-15 18:35:23] INFO:     127.0.0.1:50370 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:23] Prefill batch, #new-seq: 1, #new-token: 30, #cached-token: 224, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:23] Decode batch, #running-req: 1, #token: 258, token usage: 0.00, cuda graph: True, gen throughput (token/s): 75.91, #queue-req: 0, 
[2025-12-15 18:35:23] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.15, #queue-req: 0, 
[2025-12-15 18:35:23] INFO:     127.0.0.1:50374 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:23] Prefill batch, #new-seq: 1, #new-token: 1185, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:24] Decode batch, #running-req: 1, #token: 1280, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.21, #queue-req: 0, 
[2025-12-15 18:35:24] INFO:     127.0.0.1:50378 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:24] Prefill batch, #new-seq: 1, #new-token: 31, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:24] Decode batch, #running-req: 1, #token: 284, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.38, #queue-req: 0, 
[2025-12-15 18:35:25] Decode batch, #running-req: 1, #token: 324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.23, #queue-req: 0, 
[2025-12-15 18:35:25] INFO:     127.0.0.1:50382 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:25] Prefill batch, #new-seq: 1, #new-token: 1226, #cached-token: 61, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:26] Decode batch, #running-req: 1, #token: 1304, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.12, #queue-req: 0, 
[2025-12-15 18:35:26] INFO:     127.0.0.1:50386 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:26] Prefill batch, #new-seq: 1, #new-token: 23, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:26] Decode batch, #running-req: 1, #token: 251, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.12, #queue-req: 0, 
[2025-12-15 18:35:27] Decode batch, #running-req: 1, #token: 291, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.24, #queue-req: 0, 
[2025-12-15 18:35:27] INFO:     127.0.0.1:50390 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:27] Prefill batch, #new-seq: 1, #new-token: 1135, #cached-token: 59, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:27] Decode batch, #running-req: 1, #token: 1204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.77, #queue-req: 0, 
[2025-12-15 18:35:28] INFO:     127.0.0.1:50394 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:28] Prefill batch, #new-seq: 1, #new-token: 40, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:28] Decode batch, #running-req: 1, #token: 262, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.05, #queue-req: 0, 
[2025-12-15 18:35:28] Decode batch, #running-req: 1, #token: 302, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:35:28] INFO:     127.0.0.1:50398 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:28] Prefill batch, #new-seq: 1, #new-token: 1161, #cached-token: 56, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:29] Decode batch, #running-req: 1, #token: 1234, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.37, #queue-req: 0, 
[2025-12-15 18:35:29] INFO:     127.0.0.1:50402 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:29] Prefill batch, #new-seq: 1, #new-token: 30, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:29] Decode batch, #running-req: 1, #token: 264, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.22, #queue-req: 0, 
[2025-12-15 18:35:30] Decode batch, #running-req: 1, #token: 304, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.19, #queue-req: 0, 
[2025-12-15 18:35:30] INFO:     127.0.0.1:50406 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:30] Prefill batch, #new-seq: 1, #new-token: 1100, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:30] Decode batch, #running-req: 1, #token: 1187, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.60, #queue-req: 0, 
[2025-12-15 18:35:31] INFO:     127.0.0.1:50410 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:31] Prefill batch, #new-seq: 1, #new-token: 73, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:31] Decode batch, #running-req: 1, #token: 318, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.07, #queue-req: 0, 
[2025-12-15 18:35:31] INFO:     127.0.0.1:50414 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:31] Prefill batch, #new-seq: 1, #new-token: 1233, #cached-token: 57, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:31] Decode batch, #running-req: 1, #token: 1298, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.24, #queue-req: 0, 
[2025-12-15 18:35:32] INFO:     127.0.0.1:50418 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:32] Prefill batch, #new-seq: 1, #new-token: 24, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:32] Decode batch, #running-req: 1, #token: 254, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.06, #queue-req: 0, 
[2025-12-15 18:35:33] Decode batch, #running-req: 1, #token: 294, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:35:33] Decode batch, #running-req: 1, #token: 334, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.22, #queue-req: 0, 
[2025-12-15 18:35:33] INFO:     127.0.0.1:50422 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:33] Prefill batch, #new-seq: 1, #new-token: 1107, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:34] Decode batch, #running-req: 1, #token: 1188, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.71, #queue-req: 0, 
[2025-12-15 18:35:34] INFO:     127.0.0.1:50426 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:34] Prefill batch, #new-seq: 1, #new-token: 33, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:34] Decode batch, #running-req: 1, #token: 267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.17, #queue-req: 0, 
[2025-12-15 18:35:35] Decode batch, #running-req: 1, #token: 307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:35:35] INFO:     127.0.0.1:50430 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:35] Prefill batch, #new-seq: 1, #new-token: 1010, #cached-token: 55, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:35] Decode batch, #running-req: 1, #token: 1081, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.68, #queue-req: 0, 
[2025-12-15 18:35:36] Decode batch, #running-req: 1, #token: 1121, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.84, #queue-req: 0, 
[2025-12-15 18:35:36] Decode batch, #running-req: 1, #token: 1161, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.84, #queue-req: 0, 
[2025-12-15 18:35:37] Decode batch, #running-req: 1, #token: 1201, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.81, #queue-req: 0, 
[2025-12-15 18:35:37] Decode batch, #running-req: 1, #token: 1241, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.80, #queue-req: 0, 
[2025-12-15 18:35:38] Decode batch, #running-req: 1, #token: 1281, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.76, #queue-req: 0, 
[2025-12-15 18:35:38] Decode batch, #running-req: 1, #token: 1321, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.80, #queue-req: 0, 
[2025-12-15 18:35:38] INFO:     127.0.0.1:50434 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:38] Prefill batch, #new-seq: 1, #new-token: 13, #cached-token: 219, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:39] Decode batch, #running-req: 1, #token: 251, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.34, #queue-req: 0, 
[2025-12-15 18:35:39] Decode batch, #running-req: 1, #token: 291, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.29, #queue-req: 0, 
[2025-12-15 18:35:40] INFO:     127.0.0.1:50438 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:40] Prefill batch, #new-seq: 1, #new-token: 1018, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:40] Decode batch, #running-req: 1, #token: 1080, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.78, #queue-req: 0, 
[2025-12-15 18:35:40] INFO:     127.0.0.1:50442 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:40] Prefill batch, #new-seq: 1, #new-token: 22, #cached-token: 218, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:40] Decode batch, #running-req: 1, #token: 243, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.07, #queue-req: 0, 
[2025-12-15 18:35:41] Decode batch, #running-req: 1, #token: 283, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.31, #queue-req: 0, 
[2025-12-15 18:35:41] INFO:     127.0.0.1:50446 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:41] Prefill batch, #new-seq: 1, #new-token: 1042, #cached-token: 60, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:41] Decode batch, #running-req: 1, #token: 1133, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.71, #queue-req: 0, 
[2025-12-15 18:35:42] INFO:     127.0.0.1:50450 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:42] Prefill batch, #new-seq: 1, #new-token: 10, #cached-token: 217, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:42] Decode batch, #running-req: 1, #token: 257, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.52, #queue-req: 0, 
[2025-12-15 18:35:42] Decode batch, #running-req: 1, #token: 297, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.20, #queue-req: 0, 
[2025-12-15 18:35:43] INFO:     127.0.0.1:50454 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:43] Prefill batch, #new-seq: 1, #new-token: 1030, #cached-token: 58, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:43] Decode batch, #running-req: 1, #token: 1122, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.66, #queue-req: 0, 
[2025-12-15 18:35:43] INFO:     127.0.0.1:50458 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:43] Prefill batch, #new-seq: 1, #new-token: 36, #cached-token: 225, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:43] INFO:     127.0.0.1:50462 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:43] Prefill batch, #new-seq: 1, #new-token: 1070, #cached-token: 73, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:44] Decode batch, #running-req: 1, #token: 1156, token usage: 0.00, cuda graph: True, gen throughput (token/s): 65.90, #queue-req: 0, 
[2025-12-15 18:35:44] INFO:     127.0.0.1:50466 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:44] Prefill batch, #new-seq: 1, #new-token: 70, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:44] Decode batch, #running-req: 1, #token: 76, token usage: 0.00, cuda graph: True, gen throughput (token/s): 69.45, #queue-req: 0, 
[2025-12-15 18:35:44] INFO:     127.0.0.1:50470 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:35:44] Prefill batch, #new-seq: 1, #new-token: 1575, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:35:45] Decode batch, #running-req: 1, #token: 1605, token usage: 0.00, cuda graph: True, gen throughput (token/s): 63.19, #queue-req: 0, 
[2025-12-15 18:35:45] Decode batch, #running-req: 1, #token: 1645, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.61, #queue-req: 0, 
[2025-12-15 18:35:46] Decode batch, #running-req: 1, #token: 1685, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.60, #queue-req: 0, 
[2025-12-15 18:35:46] Decode batch, #running-req: 1, #token: 1725, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.58, #queue-req: 0, 
[2025-12-15 18:35:47] Decode batch, #running-req: 1, #token: 1765, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:35:47] Decode batch, #running-req: 1, #token: 1805, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.55, #queue-req: 0, 
[2025-12-15 18:35:48] Decode batch, #running-req: 1, #token: 1845, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:35:48] Decode batch, #running-req: 1, #token: 1885, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.54, #queue-req: 0, 
[2025-12-15 18:35:49] Decode batch, #running-req: 1, #token: 1925, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.54, #queue-req: 0, 
[2025-12-15 18:35:49] Decode batch, #running-req: 1, #token: 1965, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.54, #queue-req: 0, 
[2025-12-15 18:35:50] Decode batch, #running-req: 1, #token: 2005, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.52, #queue-req: 0, 
[2025-12-15 18:35:50] Decode batch, #running-req: 1, #token: 2045, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.51, #queue-req: 0, 
[2025-12-15 18:35:51] Decode batch, #running-req: 1, #token: 2085, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.45, #queue-req: 0, 
[2025-12-15 18:35:51] Decode batch, #running-req: 1, #token: 2125, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.44, #queue-req: 0, 
[2025-12-15 18:35:52] Decode batch, #running-req: 1, #token: 2165, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.43, #queue-req: 0, 
[2025-12-15 18:35:52] Decode batch, #running-req: 1, #token: 2205, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.40, #queue-req: 0, 
[2025-12-15 18:35:53] Decode batch, #running-req: 1, #token: 2245, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.40, #queue-req: 0, 
[2025-12-15 18:35:53] Decode batch, #running-req: 1, #token: 2285, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.40, #queue-req: 0, 
[2025-12-15 18:35:54] Decode batch, #running-req: 1, #token: 2325, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.37, #queue-req: 0, 
[2025-12-15 18:35:54] Decode batch, #running-req: 1, #token: 2365, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.39, #queue-req: 0, 
[2025-12-15 18:35:55] Decode batch, #running-req: 1, #token: 2405, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.38, #queue-req: 0, 
[2025-12-15 18:35:55] Decode batch, #running-req: 1, #token: 2445, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.36, #queue-req: 0, 
[2025-12-15 18:35:56] Decode batch, #running-req: 1, #token: 2485, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.35, #queue-req: 0, 
[2025-12-15 18:35:56] Decode batch, #running-req: 1, #token: 2525, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.35, #queue-req: 0, 
[2025-12-15 18:35:57] Decode batch, #running-req: 1, #token: 2565, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.34, #queue-req: 0, 
[2025-12-15 18:35:57] INFO:     127.0.0.1:50474 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:36:30] Prefill batch, #new-seq: 1, #new-token: 44, #cached-token: 23, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:36:30] INFO:     127.0.0.1:50486 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:36:30] Prefill batch, #new-seq: 1, #new-token: 1891, #cached-token: 11, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:36:31] Decode batch, #running-req: 1, #token: 1919, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1.18, #queue-req: 0, 
[2025-12-15 18:36:31] INFO:     127.0.0.1:50490 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:36:32] Prefill batch, #new-seq: 1, #new-token: 52, #cached-token: 21, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:36:33] INFO:     127.0.0.1:50494 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:36:33] Prefill batch, #new-seq: 1, #new-token: 1881, #cached-token: 14, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:36:33] INFO:     127.0.0.1:50498 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:36:34] Prefill batch, #new-seq: 1, #new-token: 43, #cached-token: 22, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:36:34] Decode batch, #running-req: 1, #token: 75, token usage: 0.00, cuda graph: True, gen throughput (token/s): 10.99, #queue-req: 0, 
[2025-12-15 18:36:34] INFO:     127.0.0.1:50502 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:36:34] Prefill batch, #new-seq: 1, #new-token: 1397, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:36:35] INFO:     127.0.0.1:50506 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:36:36] Prefill batch, #new-seq: 1, #new-token: 44, #cached-token: 22, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:36:36] INFO:     127.0.0.1:50510 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:36:36] Prefill batch, #new-seq: 1, #new-token: 1458, #cached-token: 9, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:36:37] Decode batch, #running-req: 1, #token: 1477, token usage: 0.00, cuda graph: True, gen throughput (token/s): 18.76, #queue-req: 0, 
[2025-12-15 18:36:37] INFO:     127.0.0.1:50514 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:36:38] Prefill batch, #new-seq: 1, #new-token: 44, #cached-token: 24, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:36:38] INFO:     127.0.0.1:50518 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:36:38] Prefill batch, #new-seq: 1, #new-token: 1268, #cached-token: 13, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:36:39] Decode batch, #running-req: 1, #token: 1305, token usage: 0.00, cuda graph: True, gen throughput (token/s): 19.75, #queue-req: 0, 
[2025-12-15 18:36:39] INFO:     127.0.0.1:50522 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:36:40] Prefill batch, #new-seq: 1, #new-token: 46, #cached-token: 22, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:36:40] INFO:     127.0.0.1:50526 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:36:40] Prefill batch, #new-seq: 1, #new-token: 1370, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:36:41] Decode batch, #running-req: 1, #token: 1427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 19.91, #queue-req: 0, 
[2025-12-15 18:36:41] Decode batch, #running-req: 1, #token: 1467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.70, #queue-req: 0, 
[2025-12-15 18:36:42] Decode batch, #running-req: 1, #token: 1507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.67, #queue-req: 0, 
[2025-12-15 18:36:42] Decode batch, #running-req: 1, #token: 1547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.67, #queue-req: 0, 
[2025-12-15 18:36:43] Decode batch, #running-req: 1, #token: 1587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.66, #queue-req: 0, 
[2025-12-15 18:36:43] Decode batch, #running-req: 1, #token: 1627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.66, #queue-req: 0, 
[2025-12-15 18:36:44] Decode batch, #running-req: 1, #token: 1667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.62, #queue-req: 0, 
[2025-12-15 18:36:44] Decode batch, #running-req: 1, #token: 1707, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.61, #queue-req: 0, 
[2025-12-15 18:36:45] Decode batch, #running-req: 1, #token: 1747, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.61, #queue-req: 0, 
[2025-12-15 18:36:45] Decode batch, #running-req: 1, #token: 1787, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:36:46] Decode batch, #running-req: 1, #token: 1827, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.58, #queue-req: 0, 
[2025-12-15 18:36:46] Decode batch, #running-req: 1, #token: 1867, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.59, #queue-req: 0, 
[2025-12-15 18:36:47] Decode batch, #running-req: 1, #token: 1907, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:36:47] Decode batch, #running-req: 1, #token: 1947, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:36:48] Decode batch, #running-req: 1, #token: 1987, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.55, #queue-req: 0, 
[2025-12-15 18:36:48] Decode batch, #running-req: 1, #token: 2027, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.55, #queue-req: 0, 
[2025-12-15 18:36:49] Decode batch, #running-req: 1, #token: 2067, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.53, #queue-req: 0, 
[2025-12-15 18:36:49] Decode batch, #running-req: 1, #token: 2107, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.48, #queue-req: 0, 
[2025-12-15 18:36:50] Decode batch, #running-req: 1, #token: 2147, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.48, #queue-req: 0, 
[2025-12-15 18:36:50] Decode batch, #running-req: 1, #token: 2187, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.45, #queue-req: 0, 
[2025-12-15 18:36:51] Decode batch, #running-req: 1, #token: 2227, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.44, #queue-req: 0, 
[2025-12-15 18:36:51] Decode batch, #running-req: 1, #token: 2267, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.42, #queue-req: 0, 
[2025-12-15 18:36:52] Decode batch, #running-req: 1, #token: 2307, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.41, #queue-req: 0, 
[2025-12-15 18:36:52] Decode batch, #running-req: 1, #token: 2347, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.41, #queue-req: 0, 
[2025-12-15 18:36:53] Decode batch, #running-req: 1, #token: 2387, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.41, #queue-req: 0, 
[2025-12-15 18:36:53] INFO:     127.0.0.1:50532 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:36:54] Prefill batch, #new-seq: 1, #new-token: 42, #cached-token: 25, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:36:54] INFO:     127.0.0.1:50540 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:36:54] Prefill batch, #new-seq: 1, #new-token: 1361, #cached-token: 10, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:36:55] INFO:     127.0.0.1:50544 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:36:55] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 19.38, #queue-req: 0, 
[2025-12-15 18:36:56] Prefill batch, #new-seq: 1, #new-token: 46, #cached-token: 24, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:36:56] INFO:     127.0.0.1:50548 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:36:56] Prefill batch, #new-seq: 1, #new-token: 1127, #cached-token: 11, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:36:57] Decode batch, #running-req: 1, #token: 1166, token usage: 0.00, cuda graph: True, gen throughput (token/s): 19.76, #queue-req: 0, 
[2025-12-15 18:36:57] Decode batch, #running-req: 1, #token: 1206, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.71, #queue-req: 0, 
[2025-12-15 18:36:58] Decode batch, #running-req: 1, #token: 1246, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.69, #queue-req: 0, 
[2025-12-15 18:36:58] Decode batch, #running-req: 1, #token: 1286, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.68, #queue-req: 0, 
[2025-12-15 18:36:59] Decode batch, #running-req: 1, #token: 1326, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.70, #queue-req: 0, 
[2025-12-15 18:36:59] Decode batch, #running-req: 1, #token: 1366, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.67, #queue-req: 0, 
[2025-12-15 18:37:00] Decode batch, #running-req: 1, #token: 1406, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.67, #queue-req: 0, 
[2025-12-15 18:37:00] Decode batch, #running-req: 1, #token: 1446, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.67, #queue-req: 0, 
[2025-12-15 18:37:01] Decode batch, #running-req: 1, #token: 1486, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.67, #queue-req: 0, 
[2025-12-15 18:37:01] Decode batch, #running-req: 1, #token: 1526, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.64, #queue-req: 0, 
[2025-12-15 18:37:02] Decode batch, #running-req: 1, #token: 1566, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.63, #queue-req: 0, 
[2025-12-15 18:37:02] Decode batch, #running-req: 1, #token: 1606, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.62, #queue-req: 0, 
[2025-12-15 18:37:03] Decode batch, #running-req: 1, #token: 1646, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.61, #queue-req: 0, 
[2025-12-15 18:37:03] Decode batch, #running-req: 1, #token: 1686, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.59, #queue-req: 0, 
[2025-12-15 18:37:04] Decode batch, #running-req: 1, #token: 1726, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:37:04] Decode batch, #running-req: 1, #token: 1766, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:37:05] Decode batch, #running-req: 1, #token: 1806, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.55, #queue-req: 0, 
[2025-12-15 18:37:05] Decode batch, #running-req: 1, #token: 1846, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:37:06] Decode batch, #running-req: 1, #token: 1886, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:37:06] Decode batch, #running-req: 1, #token: 1926, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.53, #queue-req: 0, 
[2025-12-15 18:37:07] Decode batch, #running-req: 1, #token: 1966, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.52, #queue-req: 0, 
[2025-12-15 18:37:07] Decode batch, #running-req: 1, #token: 2006, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.52, #queue-req: 0, 
[2025-12-15 18:37:08] Decode batch, #running-req: 1, #token: 2046, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.50, #queue-req: 0, 
[2025-12-15 18:37:08] Decode batch, #running-req: 1, #token: 2086, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.46, #queue-req: 0, 
[2025-12-15 18:37:09] Decode batch, #running-req: 1, #token: 2126, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.45, #queue-req: 0, 
[2025-12-15 18:37:09] INFO:     127.0.0.1:50552 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:37:10] Prefill batch, #new-seq: 1, #new-token: 50, #cached-token: 24, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:37:11] INFO:     127.0.0.1:50556 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:37:11] Prefill batch, #new-seq: 1, #new-token: 1519, #cached-token: 11, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:37:11] Decode batch, #running-req: 1, #token: 1547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 19.73, #queue-req: 0, 
[2025-12-15 18:37:11] Decode batch, #running-req: 1, #token: 1587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.64, #queue-req: 0, 
[2025-12-15 18:37:12] Decode batch, #running-req: 1, #token: 1627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.64, #queue-req: 0, 
[2025-12-15 18:37:12] Decode batch, #running-req: 1, #token: 1667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.61, #queue-req: 0, 
[2025-12-15 18:37:13] Decode batch, #running-req: 1, #token: 1707, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.58, #queue-req: 0, 
[2025-12-15 18:37:13] Decode batch, #running-req: 1, #token: 1747, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.59, #queue-req: 0, 
[2025-12-15 18:37:14] Decode batch, #running-req: 1, #token: 1787, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:37:14] Decode batch, #running-req: 1, #token: 1827, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.59, #queue-req: 0, 
[2025-12-15 18:37:15] Decode batch, #running-req: 1, #token: 1867, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:37:15] Decode batch, #running-req: 1, #token: 1907, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.53, #queue-req: 0, 
[2025-12-15 18:37:16] Decode batch, #running-req: 1, #token: 1947, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.54, #queue-req: 0, 
[2025-12-15 18:37:16] Decode batch, #running-req: 1, #token: 1987, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.54, #queue-req: 0, 
[2025-12-15 18:37:17] Decode batch, #running-req: 1, #token: 2027, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.53, #queue-req: 0, 
[2025-12-15 18:37:17] Decode batch, #running-req: 1, #token: 2067, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.49, #queue-req: 0, 
[2025-12-15 18:37:18] Decode batch, #running-req: 1, #token: 2107, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.47, #queue-req: 0, 
[2025-12-15 18:37:18] Decode batch, #running-req: 1, #token: 2147, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.46, #queue-req: 0, 
[2025-12-15 18:37:19] Decode batch, #running-req: 1, #token: 2187, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.42, #queue-req: 0, 
[2025-12-15 18:37:19] Decode batch, #running-req: 1, #token: 2227, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.42, #queue-req: 0, 
[2025-12-15 18:37:20] Decode batch, #running-req: 1, #token: 2267, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.41, #queue-req: 0, 
[2025-12-15 18:37:20] Decode batch, #running-req: 1, #token: 2307, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.38, #queue-req: 0, 
[2025-12-15 18:37:21] Decode batch, #running-req: 1, #token: 2347, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.40, #queue-req: 0, 
[2025-12-15 18:37:21] Decode batch, #running-req: 1, #token: 2387, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.40, #queue-req: 0, 
[2025-12-15 18:37:22] Decode batch, #running-req: 1, #token: 2427, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.37, #queue-req: 0, 
[2025-12-15 18:37:22] Decode batch, #running-req: 1, #token: 2467, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.37, #queue-req: 0, 
[2025-12-15 18:37:23] Decode batch, #running-req: 1, #token: 2507, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.35, #queue-req: 0, 
[2025-12-15 18:37:23] INFO:     127.0.0.1:50560 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:37:25] Prefill batch, #new-seq: 1, #new-token: 51, #cached-token: 21, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:37:25] INFO:     127.0.0.1:50572 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:37:25] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 20.26, #queue-req: 0, 
[2025-12-15 18:37:25] Prefill batch, #new-seq: 1, #new-token: 1248, #cached-token: 12, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:37:25] Decode batch, #running-req: 1, #token: 1301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.30, #queue-req: 0, 
[2025-12-15 18:37:26] Decode batch, #running-req: 1, #token: 1341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.67, #queue-req: 0, 
[2025-12-15 18:37:26] Decode batch, #running-req: 1, #token: 1381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.68, #queue-req: 0, 
[2025-12-15 18:37:27] Decode batch, #running-req: 1, #token: 1421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.67, #queue-req: 0, 
[2025-12-15 18:37:28] Decode batch, #running-req: 1, #token: 1461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.67, #queue-req: 0, 
[2025-12-15 18:37:28] Decode batch, #running-req: 1, #token: 1501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.66, #queue-req: 0, 
[2025-12-15 18:37:29] Decode batch, #running-req: 1, #token: 1541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.62, #queue-req: 0, 
[2025-12-15 18:37:29] Decode batch, #running-req: 1, #token: 1581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.64, #queue-req: 0, 
[2025-12-15 18:37:30] Decode batch, #running-req: 1, #token: 1621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.62, #queue-req: 0, 
[2025-12-15 18:37:30] Decode batch, #running-req: 1, #token: 1661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.60, #queue-req: 0, 
[2025-12-15 18:37:31] Decode batch, #running-req: 1, #token: 1701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.58, #queue-req: 0, 
[2025-12-15 18:37:31] Decode batch, #running-req: 1, #token: 1741, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:37:32] Decode batch, #running-req: 1, #token: 1781, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.55, #queue-req: 0, 
[2025-12-15 18:37:32] Decode batch, #running-req: 1, #token: 1821, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:37:33] Decode batch, #running-req: 1, #token: 1861, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:37:33] Decode batch, #running-req: 1, #token: 1901, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.53, #queue-req: 0, 
[2025-12-15 18:37:34] Decode batch, #running-req: 1, #token: 1941, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.54, #queue-req: 0, 
[2025-12-15 18:37:34] Decode batch, #running-req: 1, #token: 1981, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.54, #queue-req: 0, 
[2025-12-15 18:37:35] Decode batch, #running-req: 1, #token: 2021, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.52, #queue-req: 0, 
[2025-12-15 18:37:35] Decode batch, #running-req: 1, #token: 2061, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.48, #queue-req: 0, 
[2025-12-15 18:37:36] Decode batch, #running-req: 1, #token: 2101, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.44, #queue-req: 0, 
[2025-12-15 18:37:36] Decode batch, #running-req: 1, #token: 2141, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.42, #queue-req: 0, 
[2025-12-15 18:37:37] Decode batch, #running-req: 1, #token: 2181, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.41, #queue-req: 0, 
[2025-12-15 18:37:37] Decode batch, #running-req: 1, #token: 2221, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.39, #queue-req: 0, 
[2025-12-15 18:37:38] INFO:     127.0.0.1:50576 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:37:38] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.39, #queue-req: 0, 
[2025-12-15 18:37:39] Prefill batch, #new-seq: 1, #new-token: 49, #cached-token: 21, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:37:39] INFO:     127.0.0.1:50580 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:37:39] Prefill batch, #new-seq: 1, #new-token: 1311, #cached-token: 10, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:37:40] INFO:     127.0.0.1:50584 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:37:41] Prefill batch, #new-seq: 1, #new-token: 49, #cached-token: 23, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:37:41] Decode batch, #running-req: 1, #token: 75, token usage: 0.00, cuda graph: True, gen throughput (token/s): 10.72, #queue-req: 0, 
[2025-12-15 18:37:41] INFO:     127.0.0.1:50588 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:37:41] Prefill batch, #new-seq: 1, #new-token: 1414, #cached-token: 36, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:37:42] INFO:     127.0.0.1:50592 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:37:44] Prefill batch, #new-seq: 1, #new-token: 48, #cached-token: 22, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:37:44] Decode batch, #running-req: 1, #token: 82, token usage: 0.00, cuda graph: True, gen throughput (token/s): 15.85, #queue-req: 0, 
[2025-12-15 18:37:44] INFO:     127.0.0.1:50596 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:37:44] Prefill batch, #new-seq: 1, #new-token: 1448, #cached-token: 11, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:37:44] INFO:     127.0.0.1:50600 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:37:46] Prefill batch, #new-seq: 1, #new-token: 58, #cached-token: 21, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:37:46] INFO:     127.0.0.1:50604 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:37:46] Prefill batch, #new-seq: 1, #new-token: 1461, #cached-token: 11, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:37:46] Decode batch, #running-req: 1, #token: 1475, token usage: 0.00, cuda graph: True, gen throughput (token/s): 18.68, #queue-req: 0, 
[2025-12-15 18:37:46] INFO:     127.0.0.1:50608 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:37:48] Prefill batch, #new-seq: 1, #new-token: 46, #cached-token: 22, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:37:48] INFO:     127.0.0.1:50612 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:37:48] Prefill batch, #new-seq: 1, #new-token: 1044, #cached-token: 34, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:37:48] Decode batch, #running-req: 1, #token: 1088, token usage: 0.00, cuda graph: True, gen throughput (token/s): 19.69, #queue-req: 0, 
[2025-12-15 18:37:48] INFO:     127.0.0.1:50616 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:37:50] Prefill batch, #new-seq: 1, #new-token: 46, #cached-token: 24, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:37:50] INFO:     127.0.0.1:50622 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:37:50] Prefill batch, #new-seq: 1, #new-token: 1412, #cached-token: 12, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:37:50] Decode batch, #running-req: 1, #token: 1435, token usage: 0.00, cuda graph: True, gen throughput (token/s): 18.84, #queue-req: 0, 
[2025-12-15 18:37:51] Decode batch, #running-req: 1, #token: 1475, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.68, #queue-req: 0, 
[2025-12-15 18:37:51] Decode batch, #running-req: 1, #token: 1515, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.70, #queue-req: 0, 
[2025-12-15 18:37:52] Decode batch, #running-req: 1, #token: 1555, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.66, #queue-req: 0, 
[2025-12-15 18:37:52] Decode batch, #running-req: 1, #token: 1595, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.66, #queue-req: 0, 
[2025-12-15 18:37:53] Decode batch, #running-req: 1, #token: 1635, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.64, #queue-req: 0, 
[2025-12-15 18:37:53] Decode batch, #running-req: 1, #token: 1675, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.63, #queue-req: 0, 
[2025-12-15 18:37:54] Decode batch, #running-req: 1, #token: 1715, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.61, #queue-req: 0, 
[2025-12-15 18:37:54] Decode batch, #running-req: 1, #token: 1755, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.61, #queue-req: 0, 
[2025-12-15 18:37:55] Decode batch, #running-req: 1, #token: 1795, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.59, #queue-req: 0, 
[2025-12-15 18:37:55] Decode batch, #running-req: 1, #token: 1835, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.61, #queue-req: 0, 
[2025-12-15 18:37:56] Decode batch, #running-req: 1, #token: 1875, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.59, #queue-req: 0, 
[2025-12-15 18:37:56] Decode batch, #running-req: 1, #token: 1915, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:37:57] Decode batch, #running-req: 1, #token: 1955, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:37:57] Decode batch, #running-req: 1, #token: 1995, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:37:58] Decode batch, #running-req: 1, #token: 2035, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.53, #queue-req: 0, 
[2025-12-15 18:37:58] Decode batch, #running-req: 1, #token: 2075, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.49, #queue-req: 0, 
[2025-12-15 18:37:59] Decode batch, #running-req: 1, #token: 2115, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.48, #queue-req: 0, 
[2025-12-15 18:37:59] Decode batch, #running-req: 1, #token: 2155, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.47, #queue-req: 0, 
[2025-12-15 18:38:00] Decode batch, #running-req: 1, #token: 2195, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.45, #queue-req: 0, 
[2025-12-15 18:38:00] Decode batch, #running-req: 1, #token: 2235, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.43, #queue-req: 0, 
[2025-12-15 18:38:01] Decode batch, #running-req: 1, #token: 2275, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.43, #queue-req: 0, 
[2025-12-15 18:38:01] Decode batch, #running-req: 1, #token: 2315, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.42, #queue-req: 0, 
[2025-12-15 18:38:02] Decode batch, #running-req: 1, #token: 2355, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.42, #queue-req: 0, 
[2025-12-15 18:38:02] Decode batch, #running-req: 1, #token: 2395, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.43, #queue-req: 0, 
[2025-12-15 18:38:03] INFO:     127.0.0.1:50626 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:38:04] Prefill batch, #new-seq: 1, #new-token: 46, #cached-token: 23, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:38:04] Decode batch, #running-req: 1, #token: 80, token usage: 0.00, cuda graph: True, gen throughput (token/s): 20.64, #queue-req: 0, 
[2025-12-15 18:38:04] INFO:     127.0.0.1:50630 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:38:04] Prefill batch, #new-seq: 1, #new-token: 1532, #cached-token: 13, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:38:05] INFO:     127.0.0.1:50634 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:38:06] Prefill batch, #new-seq: 1, #new-token: 44, #cached-token: 22, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:38:07] Decode batch, #running-req: 1, #token: 73, token usage: 0.00, cuda graph: True, gen throughput (token/s): 16.17, #queue-req: 0, 
[2025-12-15 18:38:07] INFO:     127.0.0.1:50638 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:38:07] Prefill batch, #new-seq: 1, #new-token: 1356, #cached-token: 12, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:38:07] INFO:     127.0.0.1:50642 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:38:08] Prefill batch, #new-seq: 1, #new-token: 45, #cached-token: 22, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:38:09] INFO:     127.0.0.1:50648 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:38:09] Prefill batch, #new-seq: 1, #new-token: 1236, #cached-token: 9, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:38:09] Decode batch, #running-req: 1, #token: 1252, token usage: 0.00, cuda graph: True, gen throughput (token/s): 18.81, #queue-req: 0, 
[2025-12-15 18:38:09] INFO:     127.0.0.1:50652 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:38:10] Prefill batch, #new-seq: 1, #new-token: 44, #cached-token: 24, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:38:11] INFO:     127.0.0.1:50656 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:38:11] Prefill batch, #new-seq: 1, #new-token: 1330, #cached-token: 14, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:38:11] Decode batch, #running-req: 1, #token: 1356, token usage: 0.00, cuda graph: True, gen throughput (token/s): 19.67, #queue-req: 0, 
[2025-12-15 18:38:11] Decode batch, #running-req: 1, #token: 1396, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.72, #queue-req: 0, 
[2025-12-15 18:38:12] Decode batch, #running-req: 1, #token: 1436, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.70, #queue-req: 0, 
[2025-12-15 18:38:12] Decode batch, #running-req: 1, #token: 1476, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.70, #queue-req: 0, 
[2025-12-15 18:38:13] Decode batch, #running-req: 1, #token: 1516, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.70, #queue-req: 0, 
[2025-12-15 18:38:13] Decode batch, #running-req: 1, #token: 1556, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.68, #queue-req: 0, 
[2025-12-15 18:38:14] Decode batch, #running-req: 1, #token: 1596, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.66, #queue-req: 0, 
[2025-12-15 18:38:14] Decode batch, #running-req: 1, #token: 1636, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.64, #queue-req: 0, 
[2025-12-15 18:38:15] Decode batch, #running-req: 1, #token: 1676, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.63, #queue-req: 0, 
[2025-12-15 18:38:15] Decode batch, #running-req: 1, #token: 1716, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.61, #queue-req: 0, 
[2025-12-15 18:38:16] Decode batch, #running-req: 1, #token: 1756, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.60, #queue-req: 0, 
[2025-12-15 18:38:16] Decode batch, #running-req: 1, #token: 1796, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:38:17] Decode batch, #running-req: 1, #token: 1836, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.60, #queue-req: 0, 
[2025-12-15 18:38:17] Decode batch, #running-req: 1, #token: 1876, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.59, #queue-req: 0, 
[2025-12-15 18:38:18] Decode batch, #running-req: 1, #token: 1916, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.55, #queue-req: 0, 
[2025-12-15 18:38:18] Decode batch, #running-req: 1, #token: 1956, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:38:19] Decode batch, #running-req: 1, #token: 1996, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:38:19] Decode batch, #running-req: 1, #token: 2036, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.53, #queue-req: 0, 
[2025-12-15 18:38:20] Decode batch, #running-req: 1, #token: 2076, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.51, #queue-req: 0, 
[2025-12-15 18:38:20] Decode batch, #running-req: 1, #token: 2116, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.47, #queue-req: 0, 
[2025-12-15 18:38:21] Decode batch, #running-req: 1, #token: 2156, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.47, #queue-req: 0, 
[2025-12-15 18:38:21] Decode batch, #running-req: 1, #token: 2196, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.44, #queue-req: 0, 
[2025-12-15 18:38:22] Decode batch, #running-req: 1, #token: 2236, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.42, #queue-req: 0, 
[2025-12-15 18:38:22] Decode batch, #running-req: 1, #token: 2276, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.40, #queue-req: 0, 
[2025-12-15 18:38:23] Decode batch, #running-req: 1, #token: 2316, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.39, #queue-req: 0, 
[2025-12-15 18:38:23] INFO:     127.0.0.1:50660 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:38:25] Prefill batch, #new-seq: 1, #new-token: 43, #cached-token: 24, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:38:25] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 20.42, #queue-req: 0, 
[2025-12-15 18:38:25] INFO:     127.0.0.1:50666 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:38:25] Prefill batch, #new-seq: 1, #new-token: 1341, #cached-token: 11, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:38:25] Decode batch, #running-req: 1, #token: 1392, token usage: 0.00, cuda graph: True, gen throughput (token/s): 67.23, #queue-req: 0, 
[2025-12-15 18:38:26] Decode batch, #running-req: 1, #token: 1432, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.65, #queue-req: 0, 
[2025-12-15 18:38:26] Decode batch, #running-req: 1, #token: 1472, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.68, #queue-req: 0, 
[2025-12-15 18:38:27] Decode batch, #running-req: 1, #token: 1512, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.65, #queue-req: 0, 
[2025-12-15 18:38:27] Decode batch, #running-req: 1, #token: 1552, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.62, #queue-req: 0, 
[2025-12-15 18:38:28] Decode batch, #running-req: 1, #token: 1592, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.64, #queue-req: 0, 
[2025-12-15 18:38:28] Decode batch, #running-req: 1, #token: 1632, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.63, #queue-req: 0, 
[2025-12-15 18:38:29] Decode batch, #running-req: 1, #token: 1672, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.60, #queue-req: 0, 
[2025-12-15 18:38:29] Decode batch, #running-req: 1, #token: 1712, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.58, #queue-req: 0, 
[2025-12-15 18:38:30] Decode batch, #running-req: 1, #token: 1752, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:38:30] Decode batch, #running-req: 1, #token: 1792, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.54, #queue-req: 0, 
[2025-12-15 18:38:31] Decode batch, #running-req: 1, #token: 1832, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:38:31] Decode batch, #running-req: 1, #token: 1872, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.55, #queue-req: 0, 
[2025-12-15 18:38:32] Decode batch, #running-req: 1, #token: 1912, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.53, #queue-req: 0, 
[2025-12-15 18:38:32] Decode batch, #running-req: 1, #token: 1952, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.54, #queue-req: 0, 
[2025-12-15 18:38:33] Decode batch, #running-req: 1, #token: 1992, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.55, #queue-req: 0, 
[2025-12-15 18:38:33] Decode batch, #running-req: 1, #token: 2032, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.51, #queue-req: 0, 
[2025-12-15 18:38:34] Decode batch, #running-req: 1, #token: 2072, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.47, #queue-req: 0, 
[2025-12-15 18:38:34] Decode batch, #running-req: 1, #token: 2112, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.45, #queue-req: 0, 
[2025-12-15 18:38:35] Decode batch, #running-req: 1, #token: 2152, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.43, #queue-req: 0, 
[2025-12-15 18:38:35] Decode batch, #running-req: 1, #token: 2192, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.43, #queue-req: 0, 
[2025-12-15 18:38:36] Decode batch, #running-req: 1, #token: 2232, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.39, #queue-req: 0, 
[2025-12-15 18:38:36] Decode batch, #running-req: 1, #token: 2272, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.39, #queue-req: 0, 
[2025-12-15 18:38:37] Decode batch, #running-req: 1, #token: 2312, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.37, #queue-req: 0, 
[2025-12-15 18:38:37] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.34, #queue-req: 0, 
[2025-12-15 18:38:37] INFO:     127.0.0.1:50670 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:38:39] Prefill batch, #new-seq: 1, #new-token: 51, #cached-token: 23, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:38:39] INFO:     127.0.0.1:50674 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:38:39] Prefill batch, #new-seq: 1, #new-token: 1363, #cached-token: 13, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:38:39] INFO:     127.0.0.1:50678 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:38:41] Prefill batch, #new-seq: 1, #new-token: 44, #cached-token: 22, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:38:41] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 11.66, #queue-req: 0, 
[2025-12-15 18:38:41] INFO:     127.0.0.1:50682 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:38:41] Prefill batch, #new-seq: 1, #new-token: 1202, #cached-token: 13, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:38:41] INFO:     127.0.0.1:50686 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:38:43] Prefill batch, #new-seq: 1, #new-token: 46, #cached-token: 22, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:38:43] INFO:     127.0.0.1:50692 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:38:43] Prefill batch, #new-seq: 1, #new-token: 1022, #cached-token: 9, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:38:43] Decode batch, #running-req: 1, #token: 1039, token usage: 0.00, cuda graph: True, gen throughput (token/s): 18.30, #queue-req: 0, 
[2025-12-15 18:38:43] INFO:     127.0.0.1:50698 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:38:45] Prefill batch, #new-seq: 1, #new-token: 41, #cached-token: 28, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:38:45] INFO:     127.0.0.1:50704 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:38:45] Prefill batch, #new-seq: 1, #new-token: 1352, #cached-token: 186, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:38:45] INFO:     127.0.0.1:50708 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:38:47] Prefill batch, #new-seq: 1, #new-token: 47, #cached-token: 24, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:38:47] Decode batch, #running-req: 1, #token: 77, token usage: 0.00, cuda graph: True, gen throughput (token/s): 11.28, #queue-req: 0, 
[2025-12-15 18:38:47] INFO:     127.0.0.1:50712 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:38:47] Prefill batch, #new-seq: 1, #new-token: 1280, #cached-token: 35, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:38:47] INFO:     127.0.0.1:50716 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:38:48] Prefill batch, #new-seq: 1, #new-token: 46, #cached-token: 23, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:38:49] INFO:     127.0.0.1:50720 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:38:49] Prefill batch, #new-seq: 1, #new-token: 1274, #cached-token: 89, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:38:49] Decode batch, #running-req: 1, #token: 1373, token usage: 0.00, cuda graph: True, gen throughput (token/s): 18.01, #queue-req: 0, 
[2025-12-15 18:38:49] INFO:     127.0.0.1:50724 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:38:51] Prefill batch, #new-seq: 1, #new-token: 42, #cached-token: 27, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:38:51] INFO:     127.0.0.1:50728 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:38:51] Prefill batch, #new-seq: 1, #new-token: 1459, #cached-token: 35, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:38:51] Decode batch, #running-req: 1, #token: 1504, token usage: 0.00, cuda graph: True, gen throughput (token/s): 17.69, #queue-req: 0, 
[2025-12-15 18:38:51] INFO:     127.0.0.1:50732 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:38:53] Prefill batch, #new-seq: 1, #new-token: 42, #cached-token: 27, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:38:53] INFO:     127.0.0.1:50736 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:38:53] Prefill batch, #new-seq: 1, #new-token: 1383, #cached-token: 40, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:38:53] Decode batch, #running-req: 1, #token: 1442, token usage: 0.00, cuda graph: True, gen throughput (token/s): 19.31, #queue-req: 0, 
[2025-12-15 18:38:54] Decode batch, #running-req: 1, #token: 1482, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.66, #queue-req: 0, 
[2025-12-15 18:38:54] Decode batch, #running-req: 1, #token: 1522, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.64, #queue-req: 0, 
[2025-12-15 18:38:55] Decode batch, #running-req: 1, #token: 1562, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.62, #queue-req: 0, 
[2025-12-15 18:38:55] Decode batch, #running-req: 1, #token: 1602, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.63, #queue-req: 0, 
[2025-12-15 18:38:56] Decode batch, #running-req: 1, #token: 1642, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.60, #queue-req: 0, 
[2025-12-15 18:38:56] Decode batch, #running-req: 1, #token: 1682, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.59, #queue-req: 0, 
[2025-12-15 18:38:57] Decode batch, #running-req: 1, #token: 1722, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:38:57] Decode batch, #running-req: 1, #token: 1762, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:38:58] Decode batch, #running-req: 1, #token: 1802, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.53, #queue-req: 0, 
[2025-12-15 18:38:58] Decode batch, #running-req: 1, #token: 1842, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:38:59] Decode batch, #running-req: 1, #token: 1882, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.54, #queue-req: 0, 
[2025-12-15 18:38:59] Decode batch, #running-req: 1, #token: 1922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.52, #queue-req: 0, 
[2025-12-15 18:39:00] Decode batch, #running-req: 1, #token: 1962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.52, #queue-req: 0, 
[2025-12-15 18:39:00] Decode batch, #running-req: 1, #token: 2002, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.51, #queue-req: 0, 
[2025-12-15 18:39:01] Decode batch, #running-req: 1, #token: 2042, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.50, #queue-req: 0, 
[2025-12-15 18:39:01] Decode batch, #running-req: 1, #token: 2082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.45, #queue-req: 0, 
[2025-12-15 18:39:02] Decode batch, #running-req: 1, #token: 2122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.43, #queue-req: 0, 
[2025-12-15 18:39:02] Decode batch, #running-req: 1, #token: 2162, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.43, #queue-req: 0, 
[2025-12-15 18:39:03] Decode batch, #running-req: 1, #token: 2202, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.40, #queue-req: 0, 
[2025-12-15 18:39:03] Decode batch, #running-req: 1, #token: 2242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.38, #queue-req: 0, 
[2025-12-15 18:39:04] Decode batch, #running-req: 1, #token: 2282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.37, #queue-req: 0, 
[2025-12-15 18:39:04] Decode batch, #running-req: 1, #token: 2322, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.37, #queue-req: 0, 
[2025-12-15 18:39:05] Decode batch, #running-req: 1, #token: 2362, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.38, #queue-req: 0, 
[2025-12-15 18:39:05] Decode batch, #running-req: 1, #token: 2402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.36, #queue-req: 0, 
[2025-12-15 18:39:05] INFO:     127.0.0.1:50740 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:39:07] Prefill batch, #new-seq: 1, #new-token: 49, #cached-token: 22, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:39:07] INFO:     127.0.0.1:50744 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:39:07] Prefill batch, #new-seq: 1, #new-token: 1486, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:39:07] Decode batch, #running-req: 1, #token: 1521, token usage: 0.00, cuda graph: True, gen throughput (token/s): 17.63, #queue-req: 0, 
[2025-12-15 18:39:08] INFO:     127.0.0.1:50748 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:39:09] Prefill batch, #new-seq: 1, #new-token: 43, #cached-token: 25, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:39:09] INFO:     127.0.0.1:50752 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:39:09] Prefill batch, #new-seq: 1, #new-token: 1378, #cached-token: 14, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:39:10] Decode batch, #running-req: 1, #token: 1396, token usage: 0.00, cuda graph: True, gen throughput (token/s): 19.48, #queue-req: 0, 
[2025-12-15 18:39:10] Decode batch, #running-req: 1, #token: 1436, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.66, #queue-req: 0, 
[2025-12-15 18:39:11] Decode batch, #running-req: 1, #token: 1476, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.70, #queue-req: 0, 
[2025-12-15 18:39:11] Decode batch, #running-req: 1, #token: 1516, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.66, #queue-req: 0, 
[2025-12-15 18:39:12] Decode batch, #running-req: 1, #token: 1556, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.64, #queue-req: 0, 
[2025-12-15 18:39:12] Decode batch, #running-req: 1, #token: 1596, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.66, #queue-req: 0, 
[2025-12-15 18:39:13] Decode batch, #running-req: 1, #token: 1636, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.63, #queue-req: 0, 
[2025-12-15 18:39:13] Decode batch, #running-req: 1, #token: 1676, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.59, #queue-req: 0, 
[2025-12-15 18:39:14] Decode batch, #running-req: 1, #token: 1716, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:39:14] Decode batch, #running-req: 1, #token: 1756, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:39:15] Decode batch, #running-req: 1, #token: 1796, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.55, #queue-req: 0, 
[2025-12-15 18:39:15] Decode batch, #running-req: 1, #token: 1836, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:39:16] Decode batch, #running-req: 1, #token: 1876, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.55, #queue-req: 0, 
[2025-12-15 18:39:16] Decode batch, #running-req: 1, #token: 1916, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.52, #queue-req: 0, 
[2025-12-15 18:39:17] Decode batch, #running-req: 1, #token: 1956, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.53, #queue-req: 0, 
[2025-12-15 18:39:17] Decode batch, #running-req: 1, #token: 1996, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.53, #queue-req: 0, 
[2025-12-15 18:39:18] Decode batch, #running-req: 1, #token: 2036, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.51, #queue-req: 0, 
[2025-12-15 18:39:18] Decode batch, #running-req: 1, #token: 2076, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.48, #queue-req: 0, 
[2025-12-15 18:39:19] Decode batch, #running-req: 1, #token: 2116, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.46, #queue-req: 0, 
[2025-12-15 18:39:19] Decode batch, #running-req: 1, #token: 2156, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.44, #queue-req: 0, 
[2025-12-15 18:39:20] Decode batch, #running-req: 1, #token: 2196, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.41, #queue-req: 0, 
[2025-12-15 18:39:20] Decode batch, #running-req: 1, #token: 2236, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.40, #queue-req: 0, 
[2025-12-15 18:39:21] Decode batch, #running-req: 1, #token: 2276, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.38, #queue-req: 0, 
[2025-12-15 18:39:21] Decode batch, #running-req: 1, #token: 2316, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.37, #queue-req: 0, 
[2025-12-15 18:39:22] Decode batch, #running-req: 1, #token: 2356, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.39, #queue-req: 0, 
[2025-12-15 18:39:22] INFO:     127.0.0.1:50756 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:39:24] Prefill batch, #new-seq: 1, #new-token: 47, #cached-token: 22, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:39:24] Decode batch, #running-req: 1, #token: 73, token usage: 0.00, cuda graph: True, gen throughput (token/s): 20.15, #queue-req: 0, 
[2025-12-15 18:39:24] INFO:     127.0.0.1:50764 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:39:24] Prefill batch, #new-seq: 1, #new-token: 1485, #cached-token: 9, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:39:24] Decode batch, #running-req: 1, #token: 1525, token usage: 0.00, cuda graph: True, gen throughput (token/s): 66.27, #queue-req: 0, 
[2025-12-15 18:39:24] INFO:     127.0.0.1:50768 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:39:26] Prefill batch, #new-seq: 1, #new-token: 46, #cached-token: 26, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:39:26] INFO:     127.0.0.1:50772 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:39:26] Prefill batch, #new-seq: 1, #new-token: 1470, #cached-token: 35, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:39:26] Decode batch, #running-req: 1, #token: 1529, token usage: 0.00, cuda graph: True, gen throughput (token/s): 19.57, #queue-req: 0, 
[2025-12-15 18:39:27] Decode batch, #running-req: 1, #token: 1569, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.67, #queue-req: 0, 
[2025-12-15 18:39:27] Decode batch, #running-req: 1, #token: 1609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.67, #queue-req: 0, 
[2025-12-15 18:39:28] Decode batch, #running-req: 1, #token: 1649, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.65, #queue-req: 0, 
[2025-12-15 18:39:28] Decode batch, #running-req: 1, #token: 1689, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.62, #queue-req: 0, 
[2025-12-15 18:39:29] Decode batch, #running-req: 1, #token: 1729, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.61, #queue-req: 0, 
[2025-12-15 18:39:29] Decode batch, #running-req: 1, #token: 1769, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.58, #queue-req: 0, 
[2025-12-15 18:39:30] Decode batch, #running-req: 1, #token: 1809, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.58, #queue-req: 0, 
[2025-12-15 18:39:30] Decode batch, #running-req: 1, #token: 1849, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.59, #queue-req: 0, 
[2025-12-15 18:39:31] Decode batch, #running-req: 1, #token: 1889, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:39:31] Decode batch, #running-req: 1, #token: 1929, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.55, #queue-req: 0, 
[2025-12-15 18:39:32] Decode batch, #running-req: 1, #token: 1969, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.54, #queue-req: 0, 
[2025-12-15 18:39:32] Decode batch, #running-req: 1, #token: 2009, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.55, #queue-req: 0, 
[2025-12-15 18:39:33] Decode batch, #running-req: 1, #token: 2049, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.52, #queue-req: 0, 
[2025-12-15 18:39:33] Decode batch, #running-req: 1, #token: 2089, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.47, #queue-req: 0, 
[2025-12-15 18:39:34] Decode batch, #running-req: 1, #token: 2129, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.46, #queue-req: 0, 
[2025-12-15 18:39:34] Decode batch, #running-req: 1, #token: 2169, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.43, #queue-req: 0, 
[2025-12-15 18:39:35] Decode batch, #running-req: 1, #token: 2209, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.43, #queue-req: 0, 
[2025-12-15 18:39:35] Decode batch, #running-req: 1, #token: 2249, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.40, #queue-req: 0, 
[2025-12-15 18:39:36] Decode batch, #running-req: 1, #token: 2289, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.40, #queue-req: 0, 
[2025-12-15 18:39:36] Decode batch, #running-req: 1, #token: 2329, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.40, #queue-req: 0, 
[2025-12-15 18:39:37] Decode batch, #running-req: 1, #token: 2369, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.41, #queue-req: 0, 
[2025-12-15 18:39:37] Decode batch, #running-req: 1, #token: 2409, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.41, #queue-req: 0, 
[2025-12-15 18:39:38] Decode batch, #running-req: 1, #token: 2449, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.38, #queue-req: 0, 
[2025-12-15 18:39:38] Decode batch, #running-req: 1, #token: 2489, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.40, #queue-req: 0, 
[2025-12-15 18:39:39] INFO:     127.0.0.1:50776 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:39:40] Prefill batch, #new-seq: 1, #new-token: 48, #cached-token: 22, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:39:40] INFO:     127.0.0.1:50782 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:39:40] Prefill batch, #new-seq: 1, #new-token: 1431, #cached-token: 9, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:39:40] INFO:     127.0.0.1:50786 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:39:40] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 19.48, #queue-req: 0, 
[2025-12-15 18:39:42] Prefill batch, #new-seq: 1, #new-token: 41, #cached-token: 27, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:39:42] INFO:     127.0.0.1:50790 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:39:42] Prefill batch, #new-seq: 1, #new-token: 1311, #cached-token: 101, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:39:42] Decode batch, #running-req: 1, #token: 1442, token usage: 0.00, cuda graph: True, gen throughput (token/s): 19.69, #queue-req: 0, 
[2025-12-15 18:39:43] Decode batch, #running-req: 1, #token: 1482, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.67, #queue-req: 0, 
[2025-12-15 18:39:43] Decode batch, #running-req: 1, #token: 1522, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.64, #queue-req: 0, 
[2025-12-15 18:39:44] Decode batch, #running-req: 1, #token: 1562, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.61, #queue-req: 0, 
[2025-12-15 18:39:44] Decode batch, #running-req: 1, #token: 1602, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.64, #queue-req: 0, 
[2025-12-15 18:39:45] Decode batch, #running-req: 1, #token: 1642, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.59, #queue-req: 0, 
[2025-12-15 18:39:45] Decode batch, #running-req: 1, #token: 1682, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.59, #queue-req: 0, 
[2025-12-15 18:39:46] Decode batch, #running-req: 1, #token: 1722, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:39:46] Decode batch, #running-req: 1, #token: 1762, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.54, #queue-req: 0, 
[2025-12-15 18:39:47] Decode batch, #running-req: 1, #token: 1802, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.55, #queue-req: 0, 
[2025-12-15 18:39:47] Decode batch, #running-req: 1, #token: 1842, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:39:48] Decode batch, #running-req: 1, #token: 1882, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.53, #queue-req: 0, 
[2025-12-15 18:39:48] Decode batch, #running-req: 1, #token: 1922, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.52, #queue-req: 0, 
[2025-12-15 18:39:49] Decode batch, #running-req: 1, #token: 1962, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.51, #queue-req: 0, 
[2025-12-15 18:39:49] Decode batch, #running-req: 1, #token: 2002, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.52, #queue-req: 0, 
[2025-12-15 18:39:50] Decode batch, #running-req: 1, #token: 2042, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.49, #queue-req: 0, 
[2025-12-15 18:39:50] Decode batch, #running-req: 1, #token: 2082, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.45, #queue-req: 0, 
[2025-12-15 18:39:51] Decode batch, #running-req: 1, #token: 2122, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.44, #queue-req: 0, 
[2025-12-15 18:39:51] Decode batch, #running-req: 1, #token: 2162, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.40, #queue-req: 0, 
[2025-12-15 18:39:52] Decode batch, #running-req: 1, #token: 2202, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.42, #queue-req: 0, 
[2025-12-15 18:39:52] Decode batch, #running-req: 1, #token: 2242, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.37, #queue-req: 0, 
[2025-12-15 18:39:53] Decode batch, #running-req: 1, #token: 2282, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.36, #queue-req: 0, 
[2025-12-15 18:39:53] Decode batch, #running-req: 1, #token: 2322, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.35, #queue-req: 0, 
[2025-12-15 18:39:54] Decode batch, #running-req: 1, #token: 2362, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.36, #queue-req: 0, 
[2025-12-15 18:39:54] Decode batch, #running-req: 1, #token: 2402, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.37, #queue-req: 0, 
[2025-12-15 18:39:55] INFO:     127.0.0.1:50794 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:39:56] Prefill batch, #new-seq: 1, #new-token: 45, #cached-token: 24, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:39:56] INFO:     127.0.0.1:50798 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:39:56] Prefill batch, #new-seq: 1, #new-token: 1512, #cached-token: 13, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:39:57] Decode batch, #running-req: 1, #token: 1539, token usage: 0.00, cuda graph: True, gen throughput (token/s): 19.69, #queue-req: 0, 
[2025-12-15 18:39:57] INFO:     127.0.0.1:50802 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:39:59] Prefill batch, #new-seq: 1, #new-token: 43, #cached-token: 23, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:39:59] INFO:     127.0.0.1:50806 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:39:59] Prefill batch, #new-seq: 1, #new-token: 1941, #cached-token: 11, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:39:59] Decode batch, #running-req: 1, #token: 1965, token usage: 0.01, cuda graph: True, gen throughput (token/s): 15.76, #queue-req: 0, 
[2025-12-15 18:40:00] Decode batch, #running-req: 1, #token: 2005, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.55, #queue-req: 0, 
[2025-12-15 18:40:00] Decode batch, #running-req: 1, #token: 2045, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.53, #queue-req: 0, 
[2025-12-15 18:40:01] Decode batch, #running-req: 1, #token: 2085, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.49, #queue-req: 0, 
[2025-12-15 18:40:01] Decode batch, #running-req: 1, #token: 2125, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.48, #queue-req: 0, 
[2025-12-15 18:40:02] Decode batch, #running-req: 1, #token: 2165, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.47, #queue-req: 0, 
[2025-12-15 18:40:02] Decode batch, #running-req: 1, #token: 2205, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.44, #queue-req: 0, 
[2025-12-15 18:40:03] Decode batch, #running-req: 1, #token: 2245, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.43, #queue-req: 0, 
[2025-12-15 18:40:03] Decode batch, #running-req: 1, #token: 2285, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.43, #queue-req: 0, 
[2025-12-15 18:40:04] Decode batch, #running-req: 1, #token: 2325, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.41, #queue-req: 0, 
[2025-12-15 18:40:04] Decode batch, #running-req: 1, #token: 2365, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.42, #queue-req: 0, 
[2025-12-15 18:40:05] Decode batch, #running-req: 1, #token: 2405, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.42, #queue-req: 0, 
[2025-12-15 18:40:05] Decode batch, #running-req: 1, #token: 2445, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.40, #queue-req: 0, 
[2025-12-15 18:40:06] Decode batch, #running-req: 1, #token: 2485, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.39, #queue-req: 0, 
[2025-12-15 18:40:06] Decode batch, #running-req: 1, #token: 2525, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.38, #queue-req: 0, 
[2025-12-15 18:40:07] Decode batch, #running-req: 1, #token: 2565, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.37, #queue-req: 0, 
[2025-12-15 18:40:07] Decode batch, #running-req: 1, #token: 2605, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.35, #queue-req: 0, 
[2025-12-15 18:40:08] Decode batch, #running-req: 1, #token: 2645, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.34, #queue-req: 0, 
[2025-12-15 18:40:08] Decode batch, #running-req: 1, #token: 2685, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.31, #queue-req: 0, 
[2025-12-15 18:40:09] Decode batch, #running-req: 1, #token: 2725, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.30, #queue-req: 0, 
[2025-12-15 18:40:09] Decode batch, #running-req: 1, #token: 2765, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.29, #queue-req: 0, 
[2025-12-15 18:40:10] Decode batch, #running-req: 1, #token: 2805, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.27, #queue-req: 0, 
[2025-12-15 18:40:10] Decode batch, #running-req: 1, #token: 2845, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.26, #queue-req: 0, 
[2025-12-15 18:40:11] Decode batch, #running-req: 1, #token: 2885, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.27, #queue-req: 0, 
[2025-12-15 18:40:11] Decode batch, #running-req: 1, #token: 2925, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.24, #queue-req: 0, 
[2025-12-15 18:40:11] INFO:     127.0.0.1:50810 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:40:13] Prefill batch, #new-seq: 1, #new-token: 46, #cached-token: 23, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:40:13] INFO:     127.0.0.1:50814 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:40:13] Prefill batch, #new-seq: 1, #new-token: 1121, #cached-token: 9, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:40:13] Decode batch, #running-req: 1, #token: 1132, token usage: 0.00, cuda graph: True, gen throughput (token/s): 18.96, #queue-req: 0, 
[2025-12-15 18:40:14] INFO:     127.0.0.1:50818 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:40:15] Prefill batch, #new-seq: 1, #new-token: 50, #cached-token: 21, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:40:15] INFO:     127.0.0.1:50826 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:40:15] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 20.83, #queue-req: 0, 
[2025-12-15 18:40:15] Prefill batch, #new-seq: 1, #new-token: 1510, #cached-token: 29, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:40:16] INFO:     127.0.0.1:50830 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:40:17] Prefill batch, #new-seq: 1, #new-token: 54, #cached-token: 22, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:40:17] Decode batch, #running-req: 1, #token: 79, token usage: 0.00, cuda graph: True, gen throughput (token/s): 19.50, #queue-req: 0, 
[2025-12-15 18:40:17] INFO:     127.0.0.1:50836 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:40:17] Prefill batch, #new-seq: 1, #new-token: 1255, #cached-token: 34, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:40:18] INFO:     127.0.0.1:50840 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:40:19] Prefill batch, #new-seq: 1, #new-token: 43, #cached-token: 26, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:40:19] INFO:     127.0.0.1:50844 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:40:19] Prefill batch, #new-seq: 1, #new-token: 1343, #cached-token: 167, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:40:19] Decode batch, #running-req: 1, #token: 1512, token usage: 0.00, cuda graph: True, gen throughput (token/s): 18.94, #queue-req: 0, 
[2025-12-15 18:40:20] Decode batch, #running-req: 1, #token: 1552, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.58, #queue-req: 0, 
[2025-12-15 18:40:20] Decode batch, #running-req: 1, #token: 1592, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.63, #queue-req: 0, 
[2025-12-15 18:40:21] Decode batch, #running-req: 1, #token: 1632, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.62, #queue-req: 0, 
[2025-12-15 18:40:21] Decode batch, #running-req: 1, #token: 1672, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.60, #queue-req: 0, 
[2025-12-15 18:40:22] Decode batch, #running-req: 1, #token: 1712, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.59, #queue-req: 0, 
[2025-12-15 18:40:22] Decode batch, #running-req: 1, #token: 1752, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:40:23] Decode batch, #running-req: 1, #token: 1792, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.53, #queue-req: 0, 
[2025-12-15 18:40:23] Decode batch, #running-req: 1, #token: 1832, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.54, #queue-req: 0, 
[2025-12-15 18:40:24] Decode batch, #running-req: 1, #token: 1872, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.55, #queue-req: 0, 
[2025-12-15 18:40:24] Decode batch, #running-req: 1, #token: 1912, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.52, #queue-req: 0, 
[2025-12-15 18:40:25] Decode batch, #running-req: 1, #token: 1952, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.50, #queue-req: 0, 
[2025-12-15 18:40:25] Decode batch, #running-req: 1, #token: 1992, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.51, #queue-req: 0, 
[2025-12-15 18:40:26] Decode batch, #running-req: 1, #token: 2032, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.51, #queue-req: 0, 
[2025-12-15 18:40:26] Decode batch, #running-req: 1, #token: 2072, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.47, #queue-req: 0, 
[2025-12-15 18:40:27] Decode batch, #running-req: 1, #token: 2112, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.43, #queue-req: 0, 
[2025-12-15 18:40:27] Decode batch, #running-req: 1, #token: 2152, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.43, #queue-req: 0, 
[2025-12-15 18:40:28] Decode batch, #running-req: 1, #token: 2192, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.40, #queue-req: 0, 
[2025-12-15 18:40:28] Decode batch, #running-req: 1, #token: 2232, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.39, #queue-req: 0, 
[2025-12-15 18:40:29] Decode batch, #running-req: 1, #token: 2272, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.37, #queue-req: 0, 
[2025-12-15 18:40:29] Decode batch, #running-req: 1, #token: 2312, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.37, #queue-req: 0, 
[2025-12-15 18:40:30] Decode batch, #running-req: 1, #token: 2352, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.38, #queue-req: 0, 
[2025-12-15 18:40:30] Decode batch, #running-req: 1, #token: 2392, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.38, #queue-req: 0, 
[2025-12-15 18:40:31] Decode batch, #running-req: 1, #token: 2432, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.37, #queue-req: 0, 
[2025-12-15 18:40:31] Decode batch, #running-req: 1, #token: 2472, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.35, #queue-req: 0, 
[2025-12-15 18:40:32] INFO:     127.0.0.1:50848 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:40:33] Prefill batch, #new-seq: 1, #new-token: 52, #cached-token: 24, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:40:33] Decode batch, #running-req: 1, #token: 78, token usage: 0.00, cuda graph: True, gen throughput (token/s): 20.50, #queue-req: 0, 
[2025-12-15 18:40:34] INFO:     127.0.0.1:50852 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:40:34] Prefill batch, #new-seq: 1, #new-token: 1808, #cached-token: 101, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:40:34] INFO:     127.0.0.1:50856 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:40:35] Prefill batch, #new-seq: 1, #new-token: 46, #cached-token: 22, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:40:35] INFO:     127.0.0.1:50860 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:40:35] Prefill batch, #new-seq: 1, #new-token: 1284, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:40:36] Decode batch, #running-req: 1, #token: 1319, token usage: 0.00, cuda graph: True, gen throughput (token/s): 18.70, #queue-req: 0, 
[2025-12-15 18:40:36] Decode batch, #running-req: 1, #token: 1359, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.70, #queue-req: 0, 
[2025-12-15 18:40:37] Decode batch, #running-req: 1, #token: 1399, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.69, #queue-req: 0, 
[2025-12-15 18:40:37] Decode batch, #running-req: 1, #token: 1439, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.70, #queue-req: 0, 
[2025-12-15 18:40:38] Decode batch, #running-req: 1, #token: 1479, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.70, #queue-req: 0, 
[2025-12-15 18:40:38] Decode batch, #running-req: 1, #token: 1519, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.69, #queue-req: 0, 
[2025-12-15 18:40:39] Decode batch, #running-req: 1, #token: 1559, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.67, #queue-req: 0, 
[2025-12-15 18:40:39] Decode batch, #running-req: 1, #token: 1599, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.67, #queue-req: 0, 
[2025-12-15 18:40:40] Decode batch, #running-req: 1, #token: 1639, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.63, #queue-req: 0, 
[2025-12-15 18:40:40] Decode batch, #running-req: 1, #token: 1679, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.63, #queue-req: 0, 
[2025-12-15 18:40:41] Decode batch, #running-req: 1, #token: 1719, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.61, #queue-req: 0, 
[2025-12-15 18:40:41] Decode batch, #running-req: 1, #token: 1759, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.60, #queue-req: 0, 
[2025-12-15 18:40:42] Decode batch, #running-req: 1, #token: 1799, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.58, #queue-req: 0, 
[2025-12-15 18:40:42] Decode batch, #running-req: 1, #token: 1839, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.62, #queue-req: 0, 
[2025-12-15 18:40:43] Decode batch, #running-req: 1, #token: 1879, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.59, #queue-req: 0, 
[2025-12-15 18:40:43] Decode batch, #running-req: 1, #token: 1919, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:40:44] Decode batch, #running-req: 1, #token: 1959, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:40:44] Decode batch, #running-req: 1, #token: 1999, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:40:45] Decode batch, #running-req: 1, #token: 2039, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.54, #queue-req: 0, 
[2025-12-15 18:40:45] Decode batch, #running-req: 1, #token: 2079, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.50, #queue-req: 0, 
[2025-12-15 18:40:46] Decode batch, #running-req: 1, #token: 2119, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.47, #queue-req: 0, 
[2025-12-15 18:40:46] Decode batch, #running-req: 1, #token: 2159, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.48, #queue-req: 0, 
[2025-12-15 18:40:47] Decode batch, #running-req: 1, #token: 2199, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.45, #queue-req: 0, 
[2025-12-15 18:40:47] Decode batch, #running-req: 1, #token: 2239, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.44, #queue-req: 0, 
[2025-12-15 18:40:48] Decode batch, #running-req: 1, #token: 2279, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.43, #queue-req: 0, 
[2025-12-15 18:40:48] INFO:     127.0.0.1:50864 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:40:49] Prefill batch, #new-seq: 1, #new-token: 44, #cached-token: 24, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:40:50] Decode batch, #running-req: 1, #token: 73, token usage: 0.00, cuda graph: True, gen throughput (token/s): 20.43, #queue-req: 0, 
[2025-12-15 18:40:50] INFO:     127.0.0.1:50872 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:40:50] Prefill batch, #new-seq: 1, #new-token: 1109, #cached-token: 165, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:40:50] Decode batch, #running-req: 1, #token: 1308, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.57, #queue-req: 0, 
[2025-12-15 18:40:51] Decode batch, #running-req: 1, #token: 1348, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.68, #queue-req: 0, 
[2025-12-15 18:40:51] Decode batch, #running-req: 1, #token: 1388, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.65, #queue-req: 0, 
[2025-12-15 18:40:52] Decode batch, #running-req: 1, #token: 1428, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.65, #queue-req: 0, 
[2025-12-15 18:40:52] Decode batch, #running-req: 1, #token: 1468, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.66, #queue-req: 0, 
[2025-12-15 18:40:53] Decode batch, #running-req: 1, #token: 1508, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.63, #queue-req: 0, 
[2025-12-15 18:40:53] Decode batch, #running-req: 1, #token: 1548, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.62, #queue-req: 0, 
[2025-12-15 18:40:54] Decode batch, #running-req: 1, #token: 1588, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.60, #queue-req: 0, 
[2025-12-15 18:40:54] Decode batch, #running-req: 1, #token: 1628, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.59, #queue-req: 0, 
[2025-12-15 18:40:55] Decode batch, #running-req: 1, #token: 1668, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.58, #queue-req: 0, 
[2025-12-15 18:40:55] Decode batch, #running-req: 1, #token: 1708, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:40:56] Decode batch, #running-req: 1, #token: 1748, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.54, #queue-req: 0, 
[2025-12-15 18:40:56] Decode batch, #running-req: 1, #token: 1788, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.54, #queue-req: 0, 
[2025-12-15 18:40:57] Decode batch, #running-req: 1, #token: 1828, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.54, #queue-req: 0, 
[2025-12-15 18:40:57] Decode batch, #running-req: 1, #token: 1868, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.53, #queue-req: 0, 
[2025-12-15 18:40:58] Decode batch, #running-req: 1, #token: 1908, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.51, #queue-req: 0, 
[2025-12-15 18:40:58] Decode batch, #running-req: 1, #token: 1948, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.50, #queue-req: 0, 
[2025-12-15 18:40:59] Decode batch, #running-req: 1, #token: 1988, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.50, #queue-req: 0, 
[2025-12-15 18:40:59] Decode batch, #running-req: 1, #token: 2028, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.51, #queue-req: 0, 
[2025-12-15 18:41:00] Decode batch, #running-req: 1, #token: 2068, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.45, #queue-req: 0, 
[2025-12-15 18:41:00] Decode batch, #running-req: 1, #token: 2108, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.42, #queue-req: 0, 
[2025-12-15 18:41:01] Decode batch, #running-req: 1, #token: 2148, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.40, #queue-req: 0, 
[2025-12-15 18:41:01] Decode batch, #running-req: 1, #token: 2188, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.39, #queue-req: 0, 
[2025-12-15 18:41:02] Decode batch, #running-req: 1, #token: 2228, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.37, #queue-req: 0, 
[2025-12-15 18:41:02] Decode batch, #running-req: 1, #token: 2268, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.37, #queue-req: 0, 
[2025-12-15 18:41:02] INFO:     127.0.0.1:50876 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:41:04] Prefill batch, #new-seq: 1, #new-token: 46, #cached-token: 24, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:41:04] INFO:     127.0.0.1:50882 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:41:04] Prefill batch, #new-seq: 1, #new-token: 1095, #cached-token: 465, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:41:04] Decode batch, #running-req: 1, #token: 1581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 18.81, #queue-req: 0, 
[2025-12-15 18:41:05] Decode batch, #running-req: 1, #token: 1621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.63, #queue-req: 0, 
[2025-12-15 18:41:05] Decode batch, #running-req: 1, #token: 1661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.60, #queue-req: 0, 
[2025-12-15 18:41:06] Decode batch, #running-req: 1, #token: 1701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.58, #queue-req: 0, 
[2025-12-15 18:41:06] Decode batch, #running-req: 1, #token: 1741, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:41:07] Decode batch, #running-req: 1, #token: 1781, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.54, #queue-req: 0, 
[2025-12-15 18:41:07] Decode batch, #running-req: 1, #token: 1821, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:41:08] Decode batch, #running-req: 1, #token: 1861, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:41:08] Decode batch, #running-req: 1, #token: 1901, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.54, #queue-req: 0, 
[2025-12-15 18:41:09] Decode batch, #running-req: 1, #token: 1941, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.52, #queue-req: 0, 
[2025-12-15 18:41:09] Decode batch, #running-req: 1, #token: 1981, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.53, #queue-req: 0, 
[2025-12-15 18:41:10] Decode batch, #running-req: 1, #token: 2021, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.51, #queue-req: 0, 
[2025-12-15 18:41:10] Decode batch, #running-req: 1, #token: 2061, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.49, #queue-req: 0, 
[2025-12-15 18:41:11] Decode batch, #running-req: 1, #token: 2101, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.45, #queue-req: 0, 
[2025-12-15 18:41:11] Decode batch, #running-req: 1, #token: 2141, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.44, #queue-req: 0, 
[2025-12-15 18:41:12] Decode batch, #running-req: 1, #token: 2181, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.43, #queue-req: 0, 
[2025-12-15 18:41:12] Decode batch, #running-req: 1, #token: 2221, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.40, #queue-req: 0, 
[2025-12-15 18:41:13] Decode batch, #running-req: 1, #token: 2261, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.40, #queue-req: 0, 
[2025-12-15 18:41:13] Decode batch, #running-req: 1, #token: 2301, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.37, #queue-req: 0, 
[2025-12-15 18:41:14] Decode batch, #running-req: 1, #token: 2341, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.39, #queue-req: 0, 
[2025-12-15 18:41:14] Decode batch, #running-req: 1, #token: 2381, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.39, #queue-req: 0, 
[2025-12-15 18:41:15] Decode batch, #running-req: 1, #token: 2421, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.37, #queue-req: 0, 
[2025-12-15 18:41:15] Decode batch, #running-req: 1, #token: 2461, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.35, #queue-req: 0, 
[2025-12-15 18:41:16] Decode batch, #running-req: 1, #token: 2501, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.36, #queue-req: 0, 
[2025-12-15 18:41:16] Decode batch, #running-req: 1, #token: 2541, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.35, #queue-req: 0, 
[2025-12-15 18:41:17] INFO:     127.0.0.1:50886 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:41:18] Prefill batch, #new-seq: 1, #new-token: 46, #cached-token: 25, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:41:18] INFO:     127.0.0.1:50892 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:41:18] Prefill batch, #new-seq: 1, #new-token: 1317, #cached-token: 162, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:41:18] Decode batch, #running-req: 1, #token: 1486, token usage: 0.00, cuda graph: True, gen throughput (token/s): 19.71, #queue-req: 0, 
[2025-12-15 18:41:18] INFO:     127.0.0.1:50896 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:41:20] Prefill batch, #new-seq: 1, #new-token: 49, #cached-token: 21, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:41:20] INFO:     127.0.0.1:50900 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:41:20] Prefill batch, #new-seq: 1, #new-token: 1339, #cached-token: 32, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:41:20] Decode batch, #running-req: 1, #token: 1395, token usage: 0.00, cuda graph: True, gen throughput (token/s): 19.77, #queue-req: 0, 
[2025-12-15 18:41:21] Decode batch, #running-req: 1, #token: 1435, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.67, #queue-req: 0, 
[2025-12-15 18:41:21] Decode batch, #running-req: 1, #token: 1475, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.68, #queue-req: 0, 
[2025-12-15 18:41:22] Decode batch, #running-req: 1, #token: 1515, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.65, #queue-req: 0, 
[2025-12-15 18:41:22] Decode batch, #running-req: 1, #token: 1555, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.63, #queue-req: 0, 
[2025-12-15 18:41:23] Decode batch, #running-req: 1, #token: 1595, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.63, #queue-req: 0, 
[2025-12-15 18:41:23] Decode batch, #running-req: 1, #token: 1635, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.61, #queue-req: 0, 
[2025-12-15 18:41:24] Decode batch, #running-req: 1, #token: 1675, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.59, #queue-req: 0, 
[2025-12-15 18:41:24] Decode batch, #running-req: 1, #token: 1715, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.59, #queue-req: 0, 
[2025-12-15 18:41:25] Decode batch, #running-req: 1, #token: 1755, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:41:25] Decode batch, #running-req: 1, #token: 1795, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:41:26] Decode batch, #running-req: 1, #token: 1835, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:41:26] Decode batch, #running-req: 1, #token: 1875, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:41:27] Decode batch, #running-req: 1, #token: 1915, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.55, #queue-req: 0, 
[2025-12-15 18:41:27] Decode batch, #running-req: 1, #token: 1955, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.54, #queue-req: 0, 
[2025-12-15 18:41:28] Decode batch, #running-req: 1, #token: 1995, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.54, #queue-req: 0, 
[2025-12-15 18:41:28] Decode batch, #running-req: 1, #token: 2035, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.51, #queue-req: 0, 
[2025-12-15 18:41:29] Decode batch, #running-req: 1, #token: 2075, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.45, #queue-req: 0, 
[2025-12-15 18:41:29] Decode batch, #running-req: 1, #token: 2115, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.44, #queue-req: 0, 
[2025-12-15 18:41:30] Decode batch, #running-req: 1, #token: 2155, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.43, #queue-req: 0, 
[2025-12-15 18:41:31] Decode batch, #running-req: 1, #token: 2195, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.41, #queue-req: 0, 
[2025-12-15 18:41:31] Decode batch, #running-req: 1, #token: 2235, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.39, #queue-req: 0, 
[2025-12-15 18:41:32] Decode batch, #running-req: 1, #token: 2275, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.39, #queue-req: 0, 
[2025-12-15 18:41:32] Decode batch, #running-req: 1, #token: 2315, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.36, #queue-req: 0, 
[2025-12-15 18:41:33] Decode batch, #running-req: 1, #token: 2355, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.38, #queue-req: 0, 
[2025-12-15 18:41:33] INFO:     127.0.0.1:50904 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:41:34] Prefill batch, #new-seq: 1, #new-token: 49, #cached-token: 22, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:41:35] INFO:     127.0.0.1:50910 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:41:35] Prefill batch, #new-seq: 1, #new-token: 2021, #cached-token: 12, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:41:35] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 16.70, #queue-req: 0, 
[2025-12-15 18:41:35] INFO:     127.0.0.1:50914 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:41:36] Prefill batch, #new-seq: 1, #new-token: 51, #cached-token: 24, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:41:37] INFO:     127.0.0.1:50918 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:41:37] Prefill batch, #new-seq: 1, #new-token: 1389, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:41:37] INFO:     127.0.0.1:50924 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:41:38] Prefill batch, #new-seq: 1, #new-token: 47, #cached-token: 22, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:41:38] INFO:     127.0.0.1:50928 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:41:38] Prefill batch, #new-seq: 1, #new-token: 1242, #cached-token: 169, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:41:38] Decode batch, #running-req: 1, #token: 1413, token usage: 0.00, cuda graph: True, gen throughput (token/s): 11.20, #queue-req: 0, 
[2025-12-15 18:41:39] INFO:     127.0.0.1:50932 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:41:40] Prefill batch, #new-seq: 1, #new-token: 43, #cached-token: 22, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:41:40] INFO:     127.0.0.1:50936 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:41:40] Prefill batch, #new-seq: 1, #new-token: 1938, #cached-token: 34, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:41:41] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 19.34, #queue-req: 0, 
[2025-12-15 18:41:41] INFO:     127.0.0.1:50940 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:41:42] Prefill batch, #new-seq: 1, #new-token: 46, #cached-token: 22, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:41:42] INFO:     127.0.0.1:50944 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:41:42] Prefill batch, #new-seq: 1, #new-token: 1080, #cached-token: 29, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:41:42] INFO:     127.0.0.1:50948 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:41:44] Prefill batch, #new-seq: 1, #new-token: 44, #cached-token: 24, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:41:44] INFO:     127.0.0.1:50952 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:41:44] Prefill batch, #new-seq: 1, #new-token: 1564, #cached-token: 11, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:41:44] Decode batch, #running-req: 1, #token: 1577, token usage: 0.00, cuda graph: True, gen throughput (token/s): 11.24, #queue-req: 0, 
[2025-12-15 18:41:45] Decode batch, #running-req: 1, #token: 1617, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:41:45] Decode batch, #running-req: 1, #token: 1657, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.59, #queue-req: 0, 
[2025-12-15 18:41:46] Decode batch, #running-req: 1, #token: 1697, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:41:46] Decode batch, #running-req: 1, #token: 1737, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.58, #queue-req: 0, 
[2025-12-15 18:41:47] Decode batch, #running-req: 1, #token: 1777, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.55, #queue-req: 0, 
[2025-12-15 18:41:47] Decode batch, #running-req: 1, #token: 1817, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.55, #queue-req: 0, 
[2025-12-15 18:41:48] Decode batch, #running-req: 1, #token: 1857, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.55, #queue-req: 0, 
[2025-12-15 18:41:48] Decode batch, #running-req: 1, #token: 1897, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.55, #queue-req: 0, 
[2025-12-15 18:41:49] Decode batch, #running-req: 1, #token: 1937, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.52, #queue-req: 0, 
[2025-12-15 18:41:49] Decode batch, #running-req: 1, #token: 1977, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.52, #queue-req: 0, 
[2025-12-15 18:41:50] Decode batch, #running-req: 1, #token: 2017, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.50, #queue-req: 0, 
[2025-12-15 18:41:50] Decode batch, #running-req: 1, #token: 2057, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.48, #queue-req: 0, 
[2025-12-15 18:41:51] Decode batch, #running-req: 1, #token: 2097, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.42, #queue-req: 0, 
[2025-12-15 18:41:51] Decode batch, #running-req: 1, #token: 2137, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.42, #queue-req: 0, 
[2025-12-15 18:41:52] Decode batch, #running-req: 1, #token: 2177, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.40, #queue-req: 0, 
[2025-12-15 18:41:52] Decode batch, #running-req: 1, #token: 2217, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.39, #queue-req: 0, 
[2025-12-15 18:41:53] Decode batch, #running-req: 1, #token: 2257, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.39, #queue-req: 0, 
[2025-12-15 18:41:53] Decode batch, #running-req: 1, #token: 2297, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.36, #queue-req: 0, 
[2025-12-15 18:41:54] Decode batch, #running-req: 1, #token: 2337, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.37, #queue-req: 0, 
[2025-12-15 18:41:54] Decode batch, #running-req: 1, #token: 2377, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.37, #queue-req: 0, 
[2025-12-15 18:41:55] Decode batch, #running-req: 1, #token: 2417, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.36, #queue-req: 0, 
[2025-12-15 18:41:55] Decode batch, #running-req: 1, #token: 2457, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.37, #queue-req: 0, 
[2025-12-15 18:41:56] Decode batch, #running-req: 1, #token: 2497, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.35, #queue-req: 0, 
[2025-12-15 18:41:56] Decode batch, #running-req: 1, #token: 2537, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.33, #queue-req: 0, 
[2025-12-15 18:41:57] INFO:     127.0.0.1:50956 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:41:58] Prefill batch, #new-seq: 1, #new-token: 45, #cached-token: 24, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:41:58] Decode batch, #running-req: 1, #token: 71, token usage: 0.00, cuda graph: True, gen throughput (token/s): 20.71, #queue-req: 0, 
[2025-12-15 18:41:58] INFO:     127.0.0.1:50962 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:41:58] Prefill batch, #new-seq: 1, #new-token: 1328, #cached-token: 42, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:41:59] Decode batch, #running-req: 1, #token: 1399, token usage: 0.00, cuda graph: True, gen throughput (token/s): 67.50, #queue-req: 0, 
[2025-12-15 18:41:59] INFO:     127.0.0.1:50966 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:42:01] Prefill batch, #new-seq: 1, #new-token: 47, #cached-token: 22, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:42:01] Decode batch, #running-req: 1, #token: 84, token usage: 0.00, cuda graph: True, gen throughput (token/s): 14.85, #queue-req: 0, 
[2025-12-15 18:42:01] INFO:     127.0.0.1:50970 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:42:01] Prefill batch, #new-seq: 1, #new-token: 1944, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:42:02] INFO:     127.0.0.1:50974 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:42:04] Prefill batch, #new-seq: 1, #new-token: 45, #cached-token: 22, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:42:04] INFO:     127.0.0.1:50978 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:42:04] Prefill batch, #new-seq: 1, #new-token: 1529, #cached-token: 34, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:42:04] Decode batch, #running-req: 1, #token: 1579, token usage: 0.00, cuda graph: True, gen throughput (token/s): 14.13, #queue-req: 0, 
[2025-12-15 18:42:05] INFO:     127.0.0.1:50982 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:42:07] Prefill batch, #new-seq: 1, #new-token: 50, #cached-token: 24, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:42:07] INFO:     127.0.0.1:50986 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:42:07] Prefill batch, #new-seq: 1, #new-token: 1414, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:42:07] Decode batch, #running-req: 1, #token: 1450, token usage: 0.00, cuda graph: True, gen throughput (token/s): 14.73, #queue-req: 0, 
[2025-12-15 18:42:07] Decode batch, #running-req: 1, #token: 1490, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.63, #queue-req: 0, 
[2025-12-15 18:42:08] Decode batch, #running-req: 1, #token: 1530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.64, #queue-req: 0, 
[2025-12-15 18:42:08] Decode batch, #running-req: 1, #token: 1570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.62, #queue-req: 0, 
[2025-12-15 18:42:09] Decode batch, #running-req: 1, #token: 1610, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.61, #queue-req: 0, 
[2025-12-15 18:42:09] Decode batch, #running-req: 1, #token: 1650, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.59, #queue-req: 0, 
[2025-12-15 18:42:10] Decode batch, #running-req: 1, #token: 1690, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.58, #queue-req: 0, 
[2025-12-15 18:42:10] Decode batch, #running-req: 1, #token: 1730, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:42:11] Decode batch, #running-req: 1, #token: 1770, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.54, #queue-req: 0, 
[2025-12-15 18:42:11] Decode batch, #running-req: 1, #token: 1810, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:42:12] Decode batch, #running-req: 1, #token: 1850, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:42:12] Decode batch, #running-req: 1, #token: 1890, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.55, #queue-req: 0, 
[2025-12-15 18:42:13] Decode batch, #running-req: 1, #token: 1930, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.52, #queue-req: 0, 
[2025-12-15 18:42:13] Decode batch, #running-req: 1, #token: 1970, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.53, #queue-req: 0, 
[2025-12-15 18:42:14] Decode batch, #running-req: 1, #token: 2010, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.52, #queue-req: 0, 
[2025-12-15 18:42:15] Decode batch, #running-req: 1, #token: 2050, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.49, #queue-req: 0, 
[2025-12-15 18:42:15] Decode batch, #running-req: 1, #token: 2090, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.45, #queue-req: 0, 
[2025-12-15 18:42:16] Decode batch, #running-req: 1, #token: 2130, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.44, #queue-req: 0, 
[2025-12-15 18:42:16] Decode batch, #running-req: 1, #token: 2170, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.42, #queue-req: 0, 
[2025-12-15 18:42:17] Decode batch, #running-req: 1, #token: 2210, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.39, #queue-req: 0, 
[2025-12-15 18:42:17] Decode batch, #running-req: 1, #token: 2250, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.39, #queue-req: 0, 
[2025-12-15 18:42:18] Decode batch, #running-req: 1, #token: 2290, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.38, #queue-req: 0, 
[2025-12-15 18:42:18] Decode batch, #running-req: 1, #token: 2330, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.37, #queue-req: 0, 
[2025-12-15 18:42:19] Decode batch, #running-req: 1, #token: 2370, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.40, #queue-req: 0, 
[2025-12-15 18:42:19] Decode batch, #running-req: 1, #token: 2410, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.37, #queue-req: 0, 
[2025-12-15 18:42:19] INFO:     127.0.0.1:50990 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:42:22] Prefill batch, #new-seq: 1, #new-token: 46, #cached-token: 24, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:42:22] Decode batch, #running-req: 1, #token: 75, token usage: 0.00, cuda graph: True, gen throughput (token/s): 15.43, #queue-req: 0, 
[2025-12-15 18:42:22] INFO:     127.0.0.1:50994 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:42:22] Prefill batch, #new-seq: 1, #new-token: 1394, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:42:22] Decode batch, #running-req: 1, #token: 1457, token usage: 0.00, cuda graph: True, gen throughput (token/s): 67.16, #queue-req: 0, 
[2025-12-15 18:42:23] Decode batch, #running-req: 1, #token: 1497, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.67, #queue-req: 0, 
[2025-12-15 18:42:23] Decode batch, #running-req: 1, #token: 1537, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.66, #queue-req: 0, 
[2025-12-15 18:42:24] Decode batch, #running-req: 1, #token: 1577, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.64, #queue-req: 0, 
[2025-12-15 18:42:24] Decode batch, #running-req: 1, #token: 1617, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.62, #queue-req: 0, 
[2025-12-15 18:42:25] Decode batch, #running-req: 1, #token: 1657, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.62, #queue-req: 0, 
[2025-12-15 18:42:25] Decode batch, #running-req: 1, #token: 1697, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.59, #queue-req: 0, 
[2025-12-15 18:42:26] Decode batch, #running-req: 1, #token: 1737, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:42:26] Decode batch, #running-req: 1, #token: 1777, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:42:27] Decode batch, #running-req: 1, #token: 1817, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.58, #queue-req: 0, 
[2025-12-15 18:42:27] Decode batch, #running-req: 1, #token: 1857, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.57, #queue-req: 0, 
[2025-12-15 18:42:28] Decode batch, #running-req: 1, #token: 1897, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.55, #queue-req: 0, 
[2025-12-15 18:42:28] Decode batch, #running-req: 1, #token: 1937, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.54, #queue-req: 0, 
[2025-12-15 18:42:29] Decode batch, #running-req: 1, #token: 1977, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.53, #queue-req: 0, 
[2025-12-15 18:42:29] Decode batch, #running-req: 1, #token: 2017, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.51, #queue-req: 0, 
[2025-12-15 18:42:30] Decode batch, #running-req: 1, #token: 2057, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.50, #queue-req: 0, 
[2025-12-15 18:42:30] Decode batch, #running-req: 1, #token: 2097, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.44, #queue-req: 0, 
[2025-12-15 18:42:31] Decode batch, #running-req: 1, #token: 2137, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.44, #queue-req: 0, 
[2025-12-15 18:42:31] Decode batch, #running-req: 1, #token: 2177, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.42, #queue-req: 0, 
[2025-12-15 18:42:32] Decode batch, #running-req: 1, #token: 2217, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.40, #queue-req: 0, 
[2025-12-15 18:42:32] Decode batch, #running-req: 1, #token: 2257, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.39, #queue-req: 0, 
[2025-12-15 18:42:33] Decode batch, #running-req: 1, #token: 2297, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.38, #queue-req: 0, 
[2025-12-15 18:42:33] Decode batch, #running-req: 1, #token: 2337, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.38, #queue-req: 0, 
[2025-12-15 18:42:34] Decode batch, #running-req: 1, #token: 2377, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.39, #queue-req: 0, 
[2025-12-15 18:42:34] Decode batch, #running-req: 1, #token: 2417, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.37, #queue-req: 0, 
[2025-12-15 18:42:34] INFO:     127.0.0.1:50998 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:42:36] Prefill batch, #new-seq: 1, #new-token: 43, #cached-token: 23, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:42:37] INFO:     127.0.0.1:51006 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:42:37] Prefill batch, #new-seq: 1, #new-token: 1442, #cached-token: 174, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:42:37] INFO:     127.0.0.1:51010 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:42:39] Prefill batch, #new-seq: 1, #new-token: 44, #cached-token: 22, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:42:39] Decode batch, #running-req: 1, #token: 71, token usage: 0.00, cuda graph: True, gen throughput (token/s): 7.73, #queue-req: 0, 
[2025-12-15 18:42:40] INFO:     127.0.0.1:51014 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:42:40] Prefill batch, #new-seq: 1, #new-token: 1054, #cached-token: 29, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:42:40] INFO:     127.0.0.1:51018 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:42:42] Prefill batch, #new-seq: 1, #new-token: 47, #cached-token: 22, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:42:42] INFO:     127.0.0.1:51022 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:42:42] Prefill batch, #new-seq: 1, #new-token: 995, #cached-token: 34, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:42:42] Decode batch, #running-req: 1, #token: 1035, token usage: 0.00, cuda graph: True, gen throughput (token/s): 14.76, #queue-req: 0, 
[2025-12-15 18:42:42] INFO:     127.0.0.1:51026 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:42:45] Prefill batch, #new-seq: 1, #new-token: 41, #cached-token: 27, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:42:45] INFO:     127.0.0.1:51032 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:42:45] Prefill batch, #new-seq: 1, #new-token: 804, #cached-token: 364, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:42:46] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 11.70, #queue-req: 0, 
[2025-12-15 18:42:46] INFO:     127.0.0.1:51036 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:42:48] Prefill batch, #new-seq: 1, #new-token: 48, #cached-token: 24, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:42:48] INFO:     127.0.0.1:51040 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:42:48] Prefill batch, #new-seq: 1, #new-token: 1236, #cached-token: 13, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:42:48] INFO:     127.0.0.1:51044 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:42:50] Prefill batch, #new-seq: 1, #new-token: 53, #cached-token: 23, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:42:50] INFO:     127.0.0.1:51048 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:42:50] Prefill batch, #new-seq: 1, #new-token: 1021, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:42:50] Decode batch, #running-req: 1, #token: 1055, token usage: 0.00, cuda graph: True, gen throughput (token/s): 8.36, #queue-req: 0, 
[2025-12-15 18:42:50] INFO:     127.0.0.1:51052 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:42:52] Prefill batch, #new-seq: 1, #new-token: 49, #cached-token: 23, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:42:53] INFO:     127.0.0.1:51056 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:42:53] Prefill batch, #new-seq: 1, #new-token: 1472, #cached-token: 29, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:42:53] INFO:     127.0.0.1:51060 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:42:55] Prefill batch, #new-seq: 1, #new-token: 45, #cached-token: 25, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:42:55] INFO:     127.0.0.1:51064 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:42:55] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 8.46, #queue-req: 0, 
[2025-12-15 18:42:55] Prefill batch, #new-seq: 1, #new-token: 1291, #cached-token: 99, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:42:55] INFO:     127.0.0.1:51068 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:42:58] Prefill batch, #new-seq: 1, #new-token: 43, #cached-token: 25, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:42:58] INFO:     127.0.0.1:51074 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:42:58] Prefill batch, #new-seq: 1, #new-token: 1432, #cached-token: 32, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:42:58] Decode batch, #running-req: 1, #token: 1469, token usage: 0.00, cuda graph: True, gen throughput (token/s): 14.39, #queue-req: 0, 
[2025-12-15 18:42:58] Decode batch, #running-req: 1, #token: 1509, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.62, #queue-req: 0, 
[2025-12-15 18:42:59] Decode batch, #running-req: 1, #token: 1549, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.62, #queue-req: 0, 
[2025-12-15 18:42:59] Decode batch, #running-req: 1, #token: 1589, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.61, #queue-req: 0, 
[2025-12-15 18:43:00] Decode batch, #running-req: 1, #token: 1629, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.61, #queue-req: 0, 
[2025-12-15 18:43:00] Decode batch, #running-req: 1, #token: 1669, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.59, #queue-req: 0, 
[2025-12-15 18:43:01] Decode batch, #running-req: 1, #token: 1709, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:43:01] Decode batch, #running-req: 1, #token: 1749, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:43:02] Decode batch, #running-req: 1, #token: 1789, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.54, #queue-req: 0, 
[2025-12-15 18:43:02] Decode batch, #running-req: 1, #token: 1829, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:43:03] Decode batch, #running-req: 1, #token: 1869, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:43:03] Decode batch, #running-req: 1, #token: 1909, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.52, #queue-req: 0, 
[2025-12-15 18:43:04] Decode batch, #running-req: 1, #token: 1949, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.51, #queue-req: 0, 
[2025-12-15 18:43:04] Decode batch, #running-req: 1, #token: 1989, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.52, #queue-req: 0, 
[2025-12-15 18:43:05] Decode batch, #running-req: 1, #token: 2029, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.51, #queue-req: 0, 
[2025-12-15 18:43:05] Decode batch, #running-req: 1, #token: 2069, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.45, #queue-req: 0, 
[2025-12-15 18:43:06] Decode batch, #running-req: 1, #token: 2109, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.43, #queue-req: 0, 
[2025-12-15 18:43:06] Decode batch, #running-req: 1, #token: 2149, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.43, #queue-req: 0, 
[2025-12-15 18:43:07] Decode batch, #running-req: 1, #token: 2189, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.40, #queue-req: 0, 
[2025-12-15 18:43:07] Decode batch, #running-req: 1, #token: 2229, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.38, #queue-req: 0, 
[2025-12-15 18:43:08] Decode batch, #running-req: 1, #token: 2269, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.37, #queue-req: 0, 
[2025-12-15 18:43:08] Decode batch, #running-req: 1, #token: 2309, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.36, #queue-req: 0, 
[2025-12-15 18:43:09] Decode batch, #running-req: 1, #token: 2349, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.37, #queue-req: 0, 
[2025-12-15 18:43:09] Decode batch, #running-req: 1, #token: 2389, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.36, #queue-req: 0, 
[2025-12-15 18:43:10] Decode batch, #running-req: 1, #token: 2429, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.36, #queue-req: 0, 
[2025-12-15 18:43:10] INFO:     127.0.0.1:51078 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:43:12] Prefill batch, #new-seq: 1, #new-token: 44, #cached-token: 24, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:43:13] Decode batch, #running-req: 1, #token: 73, token usage: 0.00, cuda graph: True, gen throughput (token/s): 15.46, #queue-req: 0, 
[2025-12-15 18:43:13] INFO:     127.0.0.1:51082 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:43:13] Prefill batch, #new-seq: 1, #new-token: 1428, #cached-token: 37, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:43:13] Decode batch, #running-req: 1, #token: 1499, token usage: 0.00, cuda graph: True, gen throughput (token/s): 64.69, #queue-req: 0, 
[2025-12-15 18:43:14] Decode batch, #running-req: 1, #token: 1539, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.62, #queue-req: 0, 
[2025-12-15 18:43:14] Decode batch, #running-req: 1, #token: 1579, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.61, #queue-req: 0, 
[2025-12-15 18:43:15] Decode batch, #running-req: 1, #token: 1619, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.60, #queue-req: 0, 
[2025-12-15 18:43:15] Decode batch, #running-req: 1, #token: 1659, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.60, #queue-req: 0, 
[2025-12-15 18:43:16] Decode batch, #running-req: 1, #token: 1699, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.58, #queue-req: 0, 
[2025-12-15 18:43:16] Decode batch, #running-req: 1, #token: 1739, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:43:17] Decode batch, #running-req: 1, #token: 1779, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.54, #queue-req: 0, 
[2025-12-15 18:43:17] Decode batch, #running-req: 1, #token: 1819, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.55, #queue-req: 0, 
[2025-12-15 18:43:18] Decode batch, #running-req: 1, #token: 1859, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.54, #queue-req: 0, 
[2025-12-15 18:43:18] Decode batch, #running-req: 1, #token: 1899, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.53, #queue-req: 0, 
[2025-12-15 18:43:19] Decode batch, #running-req: 1, #token: 1939, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.52, #queue-req: 0, 
[2025-12-15 18:43:19] Decode batch, #running-req: 1, #token: 1979, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.50, #queue-req: 0, 
[2025-12-15 18:43:20] Decode batch, #running-req: 1, #token: 2019, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.49, #queue-req: 0, 
[2025-12-15 18:43:20] Decode batch, #running-req: 1, #token: 2059, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.46, #queue-req: 0, 
[2025-12-15 18:43:21] Decode batch, #running-req: 1, #token: 2099, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.43, #queue-req: 0, 
[2025-12-15 18:43:21] Decode batch, #running-req: 1, #token: 2139, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.42, #queue-req: 0, 
[2025-12-15 18:43:22] Decode batch, #running-req: 1, #token: 2179, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.40, #queue-req: 0, 
[2025-12-15 18:43:22] Decode batch, #running-req: 1, #token: 2219, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.38, #queue-req: 0, 
[2025-12-15 18:43:23] Decode batch, #running-req: 1, #token: 2259, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.37, #queue-req: 0, 
[2025-12-15 18:43:23] Decode batch, #running-req: 1, #token: 2299, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.37, #queue-req: 0, 
[2025-12-15 18:43:24] Decode batch, #running-req: 1, #token: 2339, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.37, #queue-req: 0, 
[2025-12-15 18:43:24] Decode batch, #running-req: 1, #token: 2379, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.36, #queue-req: 0, 
[2025-12-15 18:43:25] Decode batch, #running-req: 1, #token: 2419, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.35, #queue-req: 0, 
[2025-12-15 18:43:25] Decode batch, #running-req: 1, #token: 2459, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.34, #queue-req: 0, 
[2025-12-15 18:43:25] INFO:     127.0.0.1:51086 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:43:28] Prefill batch, #new-seq: 1, #new-token: 47, #cached-token: 22, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:43:28] INFO:     127.0.0.1:51094 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:43:28] Prefill batch, #new-seq: 1, #new-token: 1361, #cached-token: 36, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:43:28] INFO:     127.0.0.1:51098 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:43:30] Prefill batch, #new-seq: 1, #new-token: 46, #cached-token: 22, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:43:30] Decode batch, #running-req: 1, #token: 77, token usage: 0.00, cuda graph: True, gen throughput (token/s): 7.83, #queue-req: 0, 
[2025-12-15 18:43:30] INFO:     127.0.0.1:51102 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:43:30] Prefill batch, #new-seq: 1, #new-token: 1510, #cached-token: 28, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:43:31] INFO:     127.0.0.1:51106 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:43:33] Prefill batch, #new-seq: 1, #new-token: 48, #cached-token: 22, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:43:33] INFO:     127.0.0.1:51110 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:43:33] Prefill batch, #new-seq: 1, #new-token: 1213, #cached-token: 33, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:43:33] Decode batch, #running-req: 1, #token: 1255, token usage: 0.00, cuda graph: True, gen throughput (token/s): 14.35, #queue-req: 0, 
[2025-12-15 18:43:33] INFO:     127.0.0.1:51114 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:43:36] Prefill batch, #new-seq: 1, #new-token: 45, #cached-token: 24, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:43:36] INFO:     127.0.0.1:51118 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:43:36] Prefill batch, #new-seq: 1, #new-token: 1304, #cached-token: 36, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:43:36] Decode batch, #running-req: 1, #token: 1365, token usage: 0.00, cuda graph: True, gen throughput (token/s): 12.69, #queue-req: 0, 
[2025-12-15 18:43:36] INFO:     127.0.0.1:51122 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:43:38] Prefill batch, #new-seq: 1, #new-token: 42, #cached-token: 24, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:43:39] INFO:     127.0.0.1:51126 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:43:39] Prefill batch, #new-seq: 1, #new-token: 1082, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:43:39] Decode batch, #running-req: 1, #token: 1137, token usage: 0.00, cuda graph: True, gen throughput (token/s): 14.83, #queue-req: 0, 
[2025-12-15 18:43:40] Decode batch, #running-req: 1, #token: 1177, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.71, #queue-req: 0, 
[2025-12-15 18:43:40] Decode batch, #running-req: 1, #token: 1217, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.71, #queue-req: 0, 
[2025-12-15 18:43:41] Decode batch, #running-req: 1, #token: 1257, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.70, #queue-req: 0, 
[2025-12-15 18:43:41] Decode batch, #running-req: 1, #token: 1297, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.68, #queue-req: 0, 
[2025-12-15 18:43:42] Decode batch, #running-req: 1, #token: 1337, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.70, #queue-req: 0, 
[2025-12-15 18:43:42] Decode batch, #running-req: 1, #token: 1377, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.68, #queue-req: 0, 
[2025-12-15 18:43:43] Decode batch, #running-req: 1, #token: 1417, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.68, #queue-req: 0, 
[2025-12-15 18:43:43] Decode batch, #running-req: 1, #token: 1457, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.67, #queue-req: 0, 
[2025-12-15 18:43:44] Decode batch, #running-req: 1, #token: 1497, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.66, #queue-req: 0, 
[2025-12-15 18:43:44] Decode batch, #running-req: 1, #token: 1537, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.65, #queue-req: 0, 
[2025-12-15 18:43:45] Decode batch, #running-req: 1, #token: 1577, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.62, #queue-req: 0, 
[2025-12-15 18:43:45] Decode batch, #running-req: 1, #token: 1617, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.62, #queue-req: 0, 
[2025-12-15 18:43:46] Decode batch, #running-req: 1, #token: 1657, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.61, #queue-req: 0, 
[2025-12-15 18:43:46] Decode batch, #running-req: 1, #token: 1697, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.59, #queue-req: 0, 
[2025-12-15 18:43:47] Decode batch, #running-req: 1, #token: 1737, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.56, #queue-req: 0, 
[2025-12-15 18:43:47] Decode batch, #running-req: 1, #token: 1777, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.54, #queue-req: 0, 
[2025-12-15 18:43:48] Decode batch, #running-req: 1, #token: 1817, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.53, #queue-req: 0, 
[2025-12-15 18:43:48] Decode batch, #running-req: 1, #token: 1857, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.54, #queue-req: 0, 
[2025-12-15 18:43:49] Decode batch, #running-req: 1, #token: 1897, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.52, #queue-req: 0, 
[2025-12-15 18:43:49] Decode batch, #running-req: 1, #token: 1937, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.51, #queue-req: 0, 
[2025-12-15 18:43:50] Decode batch, #running-req: 1, #token: 1977, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.50, #queue-req: 0, 
[2025-12-15 18:43:50] Decode batch, #running-req: 1, #token: 2017, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.48, #queue-req: 0, 
[2025-12-15 18:43:51] Decode batch, #running-req: 1, #token: 2057, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.47, #queue-req: 0, 
[2025-12-15 18:43:51] Decode batch, #running-req: 1, #token: 2097, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.43, #queue-req: 0, 
[2025-12-15 18:43:51] INFO:     127.0.0.1:51130 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:43:53] Prefill batch, #new-seq: 1, #new-token: 46, #cached-token: 25, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:43:54] INFO:     127.0.0.1:51138 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:43:54] Prefill batch, #new-seq: 1, #new-token: 1178, #cached-token: 11, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:43:54] Decode batch, #running-req: 1, #token: 1201, token usage: 0.00, cuda graph: True, gen throughput (token/s): 14.91, #queue-req: 0, 
[2025-12-15 18:43:54] Decode batch, #running-req: 1, #token: 1241, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.68, #queue-req: 0, 
[2025-12-15 18:43:55] Decode batch, #running-req: 1, #token: 1281, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.66, #queue-req: 0, 
[2025-12-15 18:43:55] Decode batch, #running-req: 1, #token: 1321, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.68, #queue-req: 0, 
[2025-12-15 18:43:56] Decode batch, #running-req: 1, #token: 1361, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.67, #queue-req: 0, 
[2025-12-15 18:43:56] Decode batch, #running-req: 1, #token: 1401, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.66, #queue-req: 0, 
[2025-12-15 18:43:57] Decode batch, #running-req: 1, #token: 1441, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.65, #queue-req: 0, 
[2025-12-15 18:43:57] Decode batch, #running-req: 1, #token: 1481, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.68, #queue-req: 0, 
[2025-12-15 18:43:58] Decode batch, #running-req: 1, #token: 1521, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.64, #queue-req: 0, 
[2025-12-15 18:43:58] Decode batch, #running-req: 1, #token: 1561, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.62, #queue-req: 0, 
[2025-12-15 18:43:59] Decode batch, #running-req: 1, #token: 1601, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.62, #queue-req: 0, 
[2025-12-15 18:43:59] Decode batch, #running-req: 1, #token: 1641, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.61, #queue-req: 0, 
[2025-12-15 18:44:00] Decode batch, #running-req: 1, #token: 1681, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.58, #queue-req: 0, 
[2025-12-15 18:44:00] Decode batch, #running-req: 1, #token: 1721, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.58, #queue-req: 0, 
[2025-12-15 18:44:01] Decode batch, #running-req: 1, #token: 1761, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.54, #queue-req: 0, 
[2025-12-15 18:44:01] Decode batch, #running-req: 1, #token: 1801, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.55, #queue-req: 0, 
[2025-12-15 18:44:02] Decode batch, #running-req: 1, #token: 1841, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.54, #queue-req: 0, 
[2025-12-15 18:44:02] Decode batch, #running-req: 1, #token: 1881, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.54, #queue-req: 0, 
[2025-12-15 18:44:03] Decode batch, #running-req: 1, #token: 1921, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.53, #queue-req: 0, 
[2025-12-15 18:44:03] Decode batch, #running-req: 1, #token: 1961, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.53, #queue-req: 0, 
[2025-12-15 18:44:04] Decode batch, #running-req: 1, #token: 2001, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.52, #queue-req: 0, 
[2025-12-15 18:44:04] Decode batch, #running-req: 1, #token: 2041, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.50, #queue-req: 0, 
[2025-12-15 18:44:05] Decode batch, #running-req: 1, #token: 2081, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.43, #queue-req: 0, 
[2025-12-15 18:44:05] Decode batch, #running-req: 1, #token: 2121, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.44, #queue-req: 0, 
[2025-12-15 18:44:06] Decode batch, #running-req: 1, #token: 2161, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.43, #queue-req: 0, 
[2025-12-15 18:44:06] INFO:     127.0.0.1:51142 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:44:08] Prefill batch, #new-seq: 1, #new-token: 44, #cached-token: 23, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:44:08] INFO:     127.0.0.1:51148 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:44:08] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 15.34, #queue-req: 0, 
[2025-12-15 18:44:08] Prefill batch, #new-seq: 1, #new-token: 1042, #cached-token: 124, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:44:09] INFO:     127.0.0.1:51152 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:44:11] Prefill batch, #new-seq: 1, #new-token: 47, #cached-token: 25, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:44:11] INFO:     127.0.0.1:51156 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:44:11] Prefill batch, #new-seq: 1, #new-token: 1230, #cached-token: 29, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:44:12] Decode batch, #running-req: 1, #token: 1270, token usage: 0.00, cuda graph: True, gen throughput (token/s): 12.52, #queue-req: 0, 
[2025-12-15 18:44:12] INFO:     127.0.0.1:51160 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:44:14] Prefill batch, #new-seq: 1, #new-token: 46, #cached-token: 23, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:44:14] INFO:     127.0.0.1:51164 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:44:14] Prefill batch, #new-seq: 1, #new-token: 953, #cached-token: 198, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:44:14] Decode batch, #running-req: 1, #token: 1153, token usage: 0.00, cuda graph: True, gen throughput (token/s): 15.02, #queue-req: 0, 
[2025-12-15 18:44:15] INFO:     127.0.0.1:51168 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:44:17] Prefill batch, #new-seq: 1, #new-token: 44, #cached-token: 22, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:44:17] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 13.39, #queue-req: 0, 
[2025-12-15 18:44:17] INFO:     127.0.0.1:51172 - "POST /generate HTTP/1.1" 200 OK
[2025-12-15 18:44:17] Prefill batch, #new-seq: 1, #new-token: 2050, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-15 18:44:18] Decode batch, #running-req: 1, #token: 2120, token usage: 0.01, cuda graph: True, gen throughput (token/s): 62.14, #queue-req: 0, 
[2025-12-15 18:44:18] Decode batch, #running-req: 1, #token: 2160, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.43, #queue-req: 0, 
[2025-12-15 18:44:19] Decode batch, #running-req: 1, #token: 2200, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.39, #queue-req: 0, 
[2025-12-15 18:44:19] Decode batch, #running-req: 1, #token: 2240, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.39, #queue-req: 0, 
[2025-12-15 18:44:20] Decode batch, #running-req: 1, #token: 2280, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.36, #queue-req: 0, 
[2025-12-15 18:44:20] Decode batch, #running-req: 1, #token: 2320, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.36, #queue-req: 0, 
[2025-12-15 18:44:21] Decode batch, #running-req: 1, #token: 2360, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.37, #queue-req: 0, 
[2025-12-15 18:44:21] Decode batch, #running-req: 1, #token: 2400, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.39, #queue-req: 0, 
[2025-12-15 18:44:22] Decode batch, #running-req: 1, #token: 2440, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.34, #queue-req: 0, 
[2025-12-15 18:44:22] Decode batch, #running-req: 1, #token: 2480, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.33, #queue-req: 0, 
[2025-12-15 18:44:23] Decode batch, #running-req: 1, #token: 2520, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.35, #queue-req: 0, 
[2025-12-15 18:44:23] Decode batch, #running-req: 1, #token: 2560, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.33, #queue-req: 0, 
[2025-12-15 18:44:24] Decode batch, #running-req: 1, #token: 2600, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.30, #queue-req: 0, 
[2025-12-15 18:44:24] Decode batch, #running-req: 1, #token: 2640, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.31, #queue-req: 0, 
[2025-12-15 18:44:25] Decode batch, #running-req: 1, #token: 2680, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.27, #queue-req: 0, 
[2025-12-15 18:44:25] Decode batch, #running-req: 1, #token: 2720, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.25, #queue-req: 0, 
[2025-12-15 18:44:26] Decode batch, #running-req: 1, #token: 2760, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.25, #queue-req: 0, 
[2025-12-15 18:44:26] Decode batch, #running-req: 1, #token: 2800, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.24, #queue-req: 0, 
[2025-12-15 18:44:27] Decode batch, #running-req: 1, #token: 2840, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.22, #queue-req: 0, 
[2025-12-15 18:44:27] Decode batch, #running-req: 1, #token: 2880, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.21, #queue-req: 0, 
[2025-12-15 18:44:28] Decode batch, #running-req: 1, #token: 2920, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.22, #queue-req: 0, 
[2025-12-15 18:44:28] Decode batch, #running-req: 1, #token: 2960, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.19, #queue-req: 0, 
[2025-12-15 18:44:29] Decode batch, #running-req: 1, #token: 3000, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.19, #queue-req: 0, 
[2025-12-15 18:44:30] Decode batch, #running-req: 1, #token: 3040, token usage: 0.01, cuda graph: True, gen throughput (token/s): 79.18, #queue-req: 0, 
[2025-12-15 18:44:30] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 79.10, #queue-req: 0, 
[2025-12-15 18:44:30] INFO:     127.0.0.1:51176 - "POST /generate HTTP/1.1" 200 OK
