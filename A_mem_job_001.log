{
  "model": "ollama/qwen3:8b",
  "dataset": "/home/users/ntu/yzheng05/workspace/A-mem/data/locomo10.json",
  "total_questions": 199,
  "category_distribution": {
    "2": 37,
    "3": 13,
    "1": 32,
    "4": 70,
    "5": 47
  },
  "aggregate_metrics": {
    "overall": {
      "exact_match": {
        "mean": 0.11055276381909548,
        "std": 0.31436836782152106,
        "median": 0,
        "min": 0,
        "max": 1,
        "count": 199
      },
      "f1": {
        "mean": 0.28811547177083646,
        "std": 0.3460210962778185,
        "median": 0.14285714285714285,
        "min": 0.0,
        "max": 1.0,
        "count": 199
      },
      "rouge1_f": {
        "mean": 0.30684490226545647,
        "std": 0.3452611717987824,
        "median": 0.16666666666666669,
        "min": 0.0,
        "max": 1.0,
        "count": 199
      },
      "rouge2_f": {
        "mean": 0.20067277318243076,
        "std": 0.3502047867155181,
        "median": 0.0,
        "min": 0.0,
        "max": 1.0,
        "count": 199
      },
      "rougeL_f": {
        "mean": 0.29650263331327165,
        "std": 0.34348099697254114,
        "median": 0.15384615384615385,
        "min": 0.0,
        "max": 1.0,
        "count": 199
      },
      "bleu1": {
        "mean": 0.2496742682981024,
        "std": 0.32434157229382027,
        "median": 0.0909090909090909,
        "min": 0,
        "max": 1.0,
        "count": 199
      },
      "bleu2": {
        "mean": 0.20188263417124439,
        "std": 0.32291235754249603,
        "median": 0.030261889307125377,
        "min": 0,
        "max": 1.0,
        "count": 199
      },
      "bleu3": {
        "mean": 0.16761601955934902,
        "std": 0.3043641146577157,
        "median": 0.022461584318318094,
        "min": 0,
        "max": 1.0,
        "count": 199
      },
      "bleu4": {
        "mean": 0.13795246872025502,
        "std": 0.2737318185432326,
        "median": 0.017182633466700372,
        "min": 0,
        "max": 1.0,
        "count": 199
      },
      "bert_precision": {
        "mean": 0.8909870727577401,
        "std": 0.0564534619480201,
        "median": 0.8777000904083252,
        "min": 0.7723919749259949,
        "max": 1.000000238418579,
        "count": 199
      },
      "bert_recall": {
        "mean": 0.8841066306559884,
        "std": 0.061013756374527765,
        "median": 0.8720123767852783,
        "min": 0.7347403168678284,
        "max": 1.000000238418579,
        "count": 199
      },
      "bert_f1": {
        "mean": 0.887028952040265,
        "std": 0.055177769086925475,
        "median": 0.8724974989891052,
        "min": 0.7802848815917969,
        "max": 1.000000238418579,
        "count": 199
      },
      "meteor": {
        "mean": 0.23680878993203933,
        "std": 0.3302519120932196,
        "median": 0.08547008547008547,
        "min": 0.0,
        "max": 0.9997724169321802,
        "count": 199
      },
      "sbert_similarity": {
        "mean": 0.4765543181486615,
        "std": 0.3280576483364908,
        "median": 0.4238954186439514,
        "min": -0.0485512837767601,
        "max": 1.000000238418579,
        "count": 199
      }
    },
    "category_1": {
      "exact_match": {
        "mean": 0,
        "std": 0.0,
        "median": 0.0,
        "min": 0,
        "max": 0,
        "count": 32
      },
      "f1": {
        "mean": 0.1658926384932577,
        "std": 0.2101025799987061,
        "median": 0.09523809523809523,
        "min": 0.0,
        "max": 0.6666666666666666,
        "count": 32
      },
      "rouge1_f": {
        "mean": 0.17795350461372134,
        "std": 0.20507887725871135,
        "median": 0.11145510835913312,
        "min": 0.0,
        "max": 0.6666666666666666,
        "count": 32
      },
      "rouge2_f": {
        "mean": 0.04481593773873186,
        "std": 0.10069998185216963,
        "median": 0.0,
        "min": 0.0,
        "max": 0.3636363636363636,
        "count": 32
      },
      "rougeL_f": {
        "mean": 0.14982648457493658,
        "std": 0.16865021486213658,
        "median": 0.09429824561403509,
        "min": 0.0,
        "max": 0.6153846153846153,
        "count": 32
      },
      "bleu1": {
        "mean": 0.13990886083609205,
        "std": 0.1696499035597889,
        "median": 0.07919254658385093,
        "min": 0,
        "max": 0.5643211499270759,
        "count": 32
      },
      "bleu2": {
        "mean": 0.07790938370853853,
        "std": 0.11578127125039432,
        "median": 0.0206822577666858,
        "min": 0,
        "max": 0.4371212831160368,
        "count": 32
      },
      "bleu3": {
        "mean": 0.036937429057645674,
        "std": 0.052466978236095946,
        "median": 0.012947356638032,
        "min": 0,
        "max": 0.1670021540516531,
        "count": 32
      },
      "bleu4": {
        "mean": 0.02290146017552303,
        "std": 0.031491848979705375,
        "median": 0.009705878021665369,
        "min": 0,
        "max": 0.11362193664674995,
        "count": 32
      },
      "bert_precision": {
        "mean": 0.8524591401219368,
        "std": 0.030929924843846553,
        "median": 0.8506037592887878,
        "min": 0.8037406206130981,
        "max": 0.9092127084732056,
        "count": 32
      },
      "bert_recall": {
        "mean": 0.8637528568506241,
        "std": 0.030892511422014483,
        "median": 0.8721693754196167,
        "min": 0.7719936370849609,
        "max": 0.9141997694969177,
        "count": 32
      },
      "bert_f1": {
        "mean": 0.8576658740639687,
        "std": 0.02473497755155467,
        "median": 0.8571981191635132,
        "min": 0.817753255367279,
        "max": 0.8983089327812195,
        "count": 32
      },
      "meteor": {
        "mean": 0.09754344235650192,
        "std": 0.1563353796621447,
        "median": 0.0,
        "min": 0.0,
        "max": 0.5434782608695652,
        "count": 32
      },
      "sbert_similarity": {
        "mean": 0.3825328853272367,
        "std": 0.23358232108171528,
        "median": 0.3817981034517288,
        "min": 0.00461356807500124,
        "max": 0.8309195041656494,
        "count": 32
      }
    },
    "category_2": {
      "exact_match": {
        "mean": 0,
        "std": 0.0,
        "median": 0,
        "min": 0,
        "max": 0,
        "count": 37
      },
      "f1": {
        "mean": 0.3028314028314028,
        "std": 0.27128430294875144,
        "median": 0.25,
        "min": 0.0,
        "max": 0.8,
        "count": 37
      },
      "rouge1_f": {
        "mean": 0.3082368082368082,
        "std": 0.26704205800337455,
        "median": 0.25,
        "min": 0.0,
        "max": 0.8,
        "count": 37
      },
      "rouge2_f": {
        "mean": 0.15881595881595884,
        "std": 0.241966455221643,
        "median": 0.0,
        "min": 0.0,
        "max": 0.6666666666666666,
        "count": 37
      },
      "rougeL_f": {
        "mean": 0.3082368082368082,
        "std": 0.26704205800337455,
        "median": 0.25,
        "min": 0.0,
        "max": 0.8,
        "count": 37
      },
      "bleu1": {
        "mean": 0.18985569042668482,
        "std": 0.2056701180964486,
        "median": 0.12262648039048077,
        "min": 0,
        "max": 0.6666666666666666,
        "count": 37
      },
      "bleu2": {
        "mean": 0.14355627410516517,
        "std": 0.18543285919212862,
        "median": 0.04749303163563625,
        "min": 0,
        "max": 0.6065306597126334,
        "count": 37
      },
      "bleu3": {
        "mean": 0.10679601084264077,
        "std": 0.13638047383877497,
        "median": 0.044556965439969995,
        "min": 0,
        "max": 0.36787944117144233,
        "count": 37
      },
      "bleu4": {
        "mean": 0.0733635338742219,
        "std": 0.08434394466770011,
        "median": 0.0417991745584234,
        "min": 0,
        "max": 0.24028114141347542,
        "count": 37
      },
      "bert_precision": {
        "mean": 0.9096009328558639,
        "std": 0.05752464901980337,
        "median": 0.9114874005317688,
        "min": 0.7723919749259949,
        "max": 0.9997365474700928,
        "count": 37
      },
      "bert_recall": {
        "mean": 0.8755363934748882,
        "std": 0.06011720454671346,
        "median": 0.8762023448944092,
        "min": 0.7883407473564148,
        "max": 0.9997365474700928,
        "count": 37
      },
      "bert_f1": {
        "mean": 0.8913616409172883,
        "std": 0.05210013065541834,
        "median": 0.8947923183441162,
        "min": 0.7802848815917969,
        "max": 0.9997365474700928,
        "count": 37
      },
      "meteor": {
        "mean": 0.1915747872329747,
        "std": 0.2215131570300893,
        "median": 0.08928571428571427,
        "min": 0.0,
        "max": 0.6465517241379309,
        "count": 37
      },
      "sbert_similarity": {
        "mean": 0.6569743454456329,
        "std": 0.20900534092352027,
        "median": 0.7047868967056274,
        "min": 0.16463962197303772,
        "max": 0.9786192178726196,
        "count": 37
      }
    },
    "category_3": {
      "exact_match": {
        "mean": 0,
        "std": 0.0,
        "median": 0,
        "min": 0,
        "max": 0,
        "count": 13
      },
      "f1": {
        "mean": 0.11221523350342842,
        "std": 0.10604059167497545,
        "median": 0.09999999999999999,
        "min": 0.0,
        "max": 0.26086956521739135,
        "count": 13
      },
      "rouge1_f": {
        "mean": 0.16179209880919398,
        "std": 0.14294788726796456,
        "median": 0.13333333333333333,
        "min": 0.0,
        "max": 0.5,
        "count": 13
      },
      "rouge2_f": {
        "mean": 0.03263403263403264,
        "std": 0.0937775882978622,
        "median": 0.0,
        "min": 0.0,
        "max": 0.33333333333333337,
        "count": 13
      },
      "rougeL_f": {
        "mean": 0.14029166941402774,
        "std": 0.13632002311140723,
        "median": 0.10526315789473682,
        "min": 0.0,
        "max": 0.5,
        "count": 13
      },
      "bleu1": {
        "mean": 0.1267318261739302,
        "std": 0.09675743464851735,
        "median": 0.1384182521267043,
        "min": 0,
        "max": 0.3125,
        "count": 13
      },
      "bleu2": {
        "mean": 0.058645931770501515,
        "std": 0.05500877853968929,
        "median": 0.03580574370197164,
        "min": 0,
        "max": 0.1507556722888818,
        "count": 13
      },
      "bleu3": {
        "mean": 0.02764016383611718,
        "std": 0.021435934251774456,
        "median": 0.023547737997202885,
        "min": 0,
        "max": 0.06275677488087485,
        "count": 13
      },
      "bleu4": {
        "mean": 0.018851071874377647,
        "std": 0.013297423248205082,
        "median": 0.018476860420522198,
        "min": 0,
        "max": 0.03986357128268015,
        "count": 13
      },
      "bert_precision": {
        "mean": 0.8677959717237033,
        "std": 0.03698804793909435,
        "median": 0.8667200803756714,
        "min": 0.7975392937660217,
        "max": 0.9354639053344727,
        "count": 13
      },
      "bert_recall": {
        "mean": 0.864599177470574,
        "std": 0.02900952230682097,
        "median": 0.8672050833702087,
        "min": 0.8121376037597656,
        "max": 0.9077224731445312,
        "count": 13
      },
      "bert_f1": {
        "mean": 0.8659122815498939,
        "std": 0.02896594981060433,
        "median": 0.8677550554275513,
        "min": 0.8118115663528442,
        "max": 0.9012690782546997,
        "count": 13
      },
      "meteor": {
        "mean": 0.08089081216004324,
        "std": 0.08827521878567095,
        "median": 0.08547008547008547,
        "min": 0.0,
        "max": 0.2937420178799489,
        "count": 13
      },
      "sbert_similarity": {
        "mean": 0.4108642391287364,
        "std": 0.175177807783144,
        "median": 0.4205699563026428,
        "min": 0.07978755235671997,
        "max": 0.7252469062805176,
        "count": 13
      }
    },
    "category_4": {
      "exact_match": {
        "mean": 0.1,
        "std": 0.30216609311120096,
        "median": 0.0,
        "min": 0,
        "max": 1,
        "count": 70
      },
      "f1": {
        "mean": 0.33134309964702474,
        "std": 0.35427250495460577,
        "median": 0.17786561264822137,
        "min": 0.0,
        "max": 1.0,
        "count": 70
      },
      "rouge1_f": {
        "mean": 0.36758411939425567,
        "std": 0.3534813958804818,
        "median": 0.263768115942029,
        "min": 0.0,
        "max": 1.0,
        "count": 70
      },
      "rouge2_f": {
        "mean": 0.24570484221759148,
        "std": 0.3716808604407428,
        "median": 0.0,
        "min": 0.0,
        "max": 1.0,
        "count": 70
      },
      "rougeL_f": {
        "mean": 0.35503352942130545,
        "std": 0.35294892606947587,
        "median": 0.20634920634920634,
        "min": 0.0,
        "max": 1.0,
        "count": 70
      },
      "bleu1": {
        "mean": 0.2956930146326194,
        "std": 0.3230633084776687,
        "median": 0.1834733893557423,
        "min": 0,
        "max": 1.0,
        "count": 70
      },
      "bleu2": {
        "mean": 0.23361309555766485,
        "std": 0.3236572233266125,
        "median": 0.043982640562744736,
        "min": 0,
        "max": 1.0,
        "count": 70
      },
      "bleu3": {
        "mean": 0.18857099397573085,
        "std": 0.29438350065792374,
        "median": 0.02512700616169594,
        "min": 0,
        "max": 1.0,
        "count": 70
      },
      "bleu4": {
        "mean": 0.1448266869027229,
        "std": 0.23788867343877657,
        "median": 0.01997782982731,
        "min": 0,
        "max": 1.0,
        "count": 70
      },
      "bert_precision": {
        "mean": 0.8899813515799386,
        "std": 0.05361140841715012,
        "median": 0.8777611255645752,
        "min": 0.7901706695556641,
        "max": 1.000000238418579,
        "count": 70
      },
      "bert_recall": {
        "mean": 0.8954034175191606,
        "std": 0.05862929342718482,
        "median": 0.8923087418079376,
        "min": 0.7622794508934021,
        "max": 1.000000238418579,
        "count": 70
      },
      "bert_f1": {
        "mean": 0.892426882471357,
        "std": 0.05421420115040154,
        "median": 0.8763012886047363,
        "min": 0.7892698049545288,
        "max": 1.000000238418579,
        "count": 70
      },
      "meteor": {
        "mean": 0.29187935811288124,
        "std": 0.3316469734729783,
        "median": 0.15058876811594202,
        "min": 0.0,
        "max": 0.9921875,
        "count": 70
      },
      "sbert_similarity": {
        "mean": 0.5037293622004134,
        "std": 0.3160218314839289,
        "median": 0.45341792702674866,
        "min": -0.026864223182201385,
        "max": 1.0000001192092896,
        "count": 70
      }
    },
    "category_5": {
      "exact_match": {
        "mean": 0.3191489361702128,
        "std": 0.47118642939800764,
        "median": 0,
        "min": 0,
        "max": 1,
        "count": 47
      },
      "f1": {
        "mean": 0.344017819893915,
        "std": 0.4598625393026488,
        "median": 0.0,
        "min": 0.0,
        "max": 1.0,
        "count": 47
      },
      "rouge1_f": {
        "mean": 0.3431633160916476,
        "std": 0.4601394896844417,
        "median": 0.0,
        "min": 0.0,
        "max": 1.0,
        "count": 47
      },
      "rouge2_f": {
        "mean": 0.3191489361702128,
        "std": 0.47118642939800764,
        "median": 0.0,
        "min": 0.0,
        "max": 1.0,
        "count": 47
      },
      "rougeL_f": {
        "mean": 0.3431633160916476,
        "std": 0.4601394896844417,
        "median": 0.0,
        "min": 0.0,
        "max": 1.0,
        "count": 47
      },
      "bleu1": {
        "mean": 0.3369661815794819,
        "std": 0.4634322371719807,
        "median": 0,
        "min": 0,
        "max": 1.0,
        "count": 47
      },
      "bleu2": {
        "mean": 0.3245669782438346,
        "std": 0.4677850965639917,
        "median": 0,
        "min": 0,
        "max": 1.0,
        "count": 47
      },
      "bleu3": {
        "mean": 0.3119754479386681,
        "std": 0.45811280370954216,
        "median": 0,
        "min": 0,
        "max": 1.0,
        "count": 47
      },
      "bleu4": {
        "mean": 0.2898364208257508,
        "std": 0.43719317526767865,
        "median": 0,
        "min": 0,
        "max": 1.0,
        "count": 47
      },
      "bert_precision": {
        "mean": 0.9104778348131383,
        "std": 0.06294706098731113,
        "median": 0.8781701922416687,
        "min": 0.8458929061889648,
        "max": 1.000000238418579,
        "count": 47
      },
      "bert_recall": {
        "mean": 0.8932819784955776,
        "std": 0.0802588816257364,
        "median": 0.8671826124191284,
        "min": 0.7347403168678284,
        "max": 1.000000238418579,
        "count": 47
      },
      "bert_f1": {
        "mean": 0.9014113050826053,
        "std": 0.07031412324097017,
        "median": 0.8687129020690918,
        "min": 0.7939961552619934,
        "max": 1.000000238418579,
        "count": 47
      },
      "meteor": {
        "mean": 0.3283437507971373,
        "std": 0.4608534413364685,
        "median": 0.0,
        "min": 0.0,
        "max": 0.9997724169321802,
        "count": 47
      },
      "sbert_similarity": {
        "mean": 0.3762322497217262,
        "std": 0.4353352870608328,
        "median": 0.1274149864912033,
        "min": -0.0485512837767601,
        "max": 1.000000238418579,
        "count": 47
      }
    }
  },
  "individual_results": [
    {
      "sample_id": 0,
      "question": "When did Caroline go to the LGBTQ support group?",
      "prediction": "May 2023",
      "reference": "7 May 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.8,
        "bleu1": 0.6065306597126334,
        "bleu2": 0.6065306597126334,
        "bleu3": 0.28369570381570614,
        "bleu4": 0.19180183554164504,
        "bert_precision": 0.9664860963821411,
        "bert_recall": 0.9103223085403442,
        "bert_f1": 0.9375638365745544,
        "meteor": 0.6465517241379309,
        "sbert_similarity": 0.9010101556777954
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie paint a sunrise?",
      "prediction": "last year",
      "reference": 2022,
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8694185018539429,
        "bert_recall": 0.9111440777778625,
        "bert_f1": 0.8897923827171326,
        "meteor": 0.0,
        "sbert_similarity": 0.3672371804714203
      }
    },
    {
      "sample_id": 0,
      "question": "What fields would Caroline be likely to pursue in her educaton?",
      "prediction": "career options",
      "reference": "Psychology, counseling certification",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8461050391197205,
        "bert_recall": 0.830493152141571,
        "bert_f1": 0.8382264375686646,
        "meteor": 0.0,
        "sbert_similarity": 0.3410683572292328
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline research?",
      "prediction": "The context does not mention what Caroline researched.",
      "reference": "Adoption agencies",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8402298092842102,
        "bert_recall": 0.822647750377655,
        "bert_f1": 0.8313458561897278,
        "meteor": 0.0,
        "sbert_similarity": 0.056297339498996735
      }
    },
    {
      "sample_id": 0,
      "question": "What is Caroline's identity?",
      "prediction": "Caroline is an artist and advocate for LGBTQ+ causes, with a focus on creating art that promotes love, acceptance, and hope.",
      "reference": "Transgender woman",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8037406206130981,
        "bert_recall": 0.8465873599052429,
        "bert_f1": 0.8246078491210938,
        "meteor": 0.0,
        "sbert_similarity": 0.35654544830322266
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie run a charity race?",
      "prediction": "25 May 2023",
      "reference": "The sunday before 25 May 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.36787944117144233,
        "bleu3": 0.36787944117144233,
        "bleu4": 0.20687381245863395,
        "bert_precision": 0.9534948468208313,
        "bert_recall": 0.8548346161842346,
        "bert_f1": 0.901473343372345,
        "meteor": 0.5165692007797271,
        "sbert_similarity": 0.8613141775131226
      }
    },
    {
      "sample_id": 0,
      "question": "When is Melanie planning on going camping?",
      "prediction": "next month",
      "reference": "June 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8738220930099487,
        "bert_recall": 0.8416211605072021,
        "bert_f1": 0.8574193716049194,
        "meteor": 0.0,
        "sbert_similarity": 0.567993700504303
      }
    },
    {
      "sample_id": 0,
      "question": "What is Caroline's relationship status?",
      "prediction": "Caroline is in a relationship with someone and is also considering adoption.",
      "reference": "Single",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8158373832702637,
        "bert_recall": 0.8712934255599976,
        "bert_f1": 0.8426539301872253,
        "meteor": 0.0,
        "sbert_similarity": 0.21480980515480042
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline give a speech at a school?",
      "prediction": "13 October 2023",
      "reference": "The week before 9 June 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0.12262648039048077,
        "bleu2": 0.04749303163563625,
        "bleu3": 0.044556965439969995,
        "bleu4": 0.0417991745584234,
        "bert_precision": 0.9385805130004883,
        "bert_recall": 0.8718153238296509,
        "bert_f1": 0.9039668440818787,
        "meteor": 0.08771929824561403,
        "sbert_similarity": 0.7575314044952393
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline meet up with her friends, family, and mentors?",
      "prediction": "Around late July 2023",
      "reference": "The week before 9 June 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2,
        "rouge1_f": 0.2,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2,
        "bleu1": 0.15163266492815836,
        "bleu2": 0.05536842069051654,
        "bleu3": 0.04649251600872798,
        "bleu4": 0.04873039689743777,
        "bert_precision": 0.9114874005317688,
        "bert_recall": 0.8786978125572205,
        "bert_f1": 0.8947923183441162,
        "meteor": 0.08620689655172413,
        "sbert_similarity": 0.7047868967056274
      }
    },
    {
      "sample_id": 0,
      "question": "How long has Caroline had her current group of friends for?",
      "prediction": "Approximately 2 years.",
      "reference": "4 years",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.25,
        "bleu2": 0.09128709291752769,
        "bleu3": 0.07665320007195604,
        "bleu4": 0.08034284189446518,
        "bert_precision": 0.9410083293914795,
        "bert_recall": 0.9376599788665771,
        "bert_f1": 0.9393311738967896,
        "meteor": 0.0,
        "sbert_similarity": 0.733866274356842
      }
    },
    {
      "sample_id": 0,
      "question": "Where did Caroline move from 4 years ago?",
      "prediction": "Not mentioned in the context.",
      "reference": "Sweden",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8193870782852173,
        "bert_recall": 0.8501977324485779,
        "bert_f1": 0.8345081210136414,
        "meteor": 0.0,
        "sbert_similarity": 0.15058183670043945
      }
    },
    {
      "sample_id": 0,
      "question": "How long ago was Caroline's 18th birthday?",
      "prediction": "About 2 years ago.",
      "reference": "10 years ago",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285715,
        "rouge1_f": 0.5714285714285715,
        "rouge2_f": 0.4,
        "rougeL_f": 0.5714285714285715,
        "bleu1": 0.4,
        "bleu2": 0.316227766016838,
        "bleu3": 0.1522474354853864,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.9329362511634827,
        "bert_recall": 0.9273823499679565,
        "bert_f1": 0.9301509857177734,
        "meteor": 0.16129032258064518,
        "sbert_similarity": 0.7955093383789062
      }
    },
    {
      "sample_id": 0,
      "question": "What career path has Caroline decided to persue?",
      "prediction": "counseling or working in mental health",
      "reference": "counseling or mental health for Transgender people",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6153846153846153,
        "rouge1_f": 0.6153846153846153,
        "rouge2_f": 0.3636363636363636,
        "rougeL_f": 0.6153846153846153,
        "bleu1": 0.5643211499270759,
        "bleu2": 0.4371212831160368,
        "bleu3": 0.16199718608951297,
        "bleu4": 0.10335094640592336,
        "bert_precision": 0.8734499216079712,
        "bert_recall": 0.8746764063835144,
        "bert_f1": 0.8740627765655518,
        "meteor": 0.5434782608695652,
        "sbert_similarity": 0.6239154934883118
      }
    },
    {
      "sample_id": 0,
      "question": "Would Caroline still want to pursue counseling as a career if she hadn't received support growing up?",
      "prediction": "If she hadn't received support growing up, Caroline would still want to pursue counseling as a career.",
      "reference": "Likely no",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8114858865737915,
        "bert_recall": 0.8121376037597656,
        "bert_f1": 0.8118115663528442,
        "meteor": 0.0,
        "sbert_similarity": 0.07978755235671997
      }
    },
    {
      "sample_id": 0,
      "question": "What activities does Melanie partake in?",
      "prediction": "Melanie partakes in camping, hiking, biking, and painting.",
      "reference": "pottery, camping, painting, swimming",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.4166666666666667,
        "bleu2": 0.19462473604038075,
        "bleu3": 0.0742797963075324,
        "bleu4": 0.045293761707938834,
        "bert_precision": 0.8812097311019897,
        "bert_recall": 0.880364179611206,
        "bert_f1": 0.8807867765426636,
        "meteor": 0.11363636363636363,
        "sbert_similarity": 0.4785575866699219
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie sign up for a pottery class?",
      "prediction": "Yesterday",
      "reference": "2 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8971229195594788,
        "bert_recall": 0.7942737936973572,
        "bert_f1": 0.8425713777542114,
        "meteor": 0.0,
        "sbert_similarity": 0.33360785245895386
      }
    },
    {
      "sample_id": 0,
      "question": "When is Caroline going to the transgender conference?",
      "prediction": "August 25th",
      "reference": "July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8354395627975464,
        "bert_recall": 0.8442529439926147,
        "bert_f1": 0.8398231267929077,
        "meteor": 0.0,
        "sbert_similarity": 0.6079060435295105
      }
    },
    {
      "sample_id": 0,
      "question": "Where has Melanie camped?",
      "prediction": "in the mountains, on a beach, and in the forest",
      "reference": "beach, mountains, forest",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5454545454545454,
        "rouge1_f": 0.4615384615384615,
        "rouge2_f": 0.0,
        "rougeL_f": 0.30769230769230765,
        "bleu1": 0.4166666666666667,
        "bleu2": 0.27524094128159016,
        "bleu3": 0.09337069741532533,
        "bleu4": 0.053863663688318665,
        "bert_precision": 0.8508570194244385,
        "bert_recall": 0.8661792278289795,
        "bert_f1": 0.8584497570991516,
        "meteor": 0.4054054054054054,
        "sbert_similarity": 0.7824088931083679
      }
    },
    {
      "sample_id": 0,
      "question": "What do Melanie's kids like?",
      "prediction": "Melanie's kids like the beach and playing with a kite.",
      "reference": "dinosaurs, nature",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8328942656517029,
        "bert_recall": 0.813457727432251,
        "bert_f1": 0.8230612277984619,
        "meteor": 0.0,
        "sbert_similarity": 0.08400939404964447
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go to the museum?",
      "prediction": "23 August 2023",
      "reference": "5 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.9835584163665771,
        "bert_recall": 0.9835584163665771,
        "bert_f1": 0.9835584163665771,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.881946325302124
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline have a picnic?",
      "prediction": "20 July 2023",
      "reference": "The week before 6 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4444444444444444,
        "rouge1_f": 0.4444444444444444,
        "rouge2_f": 0.28571428571428575,
        "rougeL_f": 0.4444444444444444,
        "bleu1": 0.24525296078096154,
        "bleu2": 0.21239529438966132,
        "bleu3": 0.11974448045963429,
        "bleu4": 0.08839449202722564,
        "bert_precision": 0.9514405727386475,
        "bert_recall": 0.8824111223220825,
        "bert_f1": 0.9156266450881958,
        "meteor": 0.3289473684210526,
        "sbert_similarity": 0.7213706970214844
      }
    },
    {
      "sample_id": 0,
      "question": "Would Caroline likely have Dr. Seuss books on her bookshelf?",
      "prediction": "Yes, Caroline likely has Dr. Seuss books on her bookshelf.",
      "reference": "Yes, since she collects classic children's books",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.23529411764705882,
        "rouge1_f": 0.22222222222222224,
        "rouge2_f": 0.0,
        "rougeL_f": 0.22222222222222224,
        "bleu1": 0.25,
        "bleu2": 0.1507556722888818,
        "bleu3": 0.06275677488087485,
        "bleu4": 0.03986357128268015,
        "bert_precision": 0.8939809799194336,
        "bert_recall": 0.905099093914032,
        "bert_f1": 0.8995056748390198,
        "meteor": 0.136986301369863,
        "sbert_similarity": 0.6086324453353882
      }
    },
    {
      "sample_id": 0,
      "question": "What books has Melanie read?",
      "prediction": "Melanie has read 'Charlotte's Web' and a book recommended by Caroline.",
      "reference": "\"Nothing is Impossible\", \"Charlotte's Web\"",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.25000000000000006,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.10482848367219184,
        "bleu3": 0.04649288856043541,
        "bleu4": 0.030206205252435548,
        "bert_precision": 0.8838757276535034,
        "bert_recall": 0.9014945030212402,
        "bert_f1": 0.8925982117652893,
        "meteor": 0.0,
        "sbert_similarity": 0.5350823402404785
      }
    },
    {
      "sample_id": 0,
      "question": "What does Melanie do to destress?",
      "prediction": "Melanie destresses by spending time in nature, such as camping, hiking, and being around campfires, as well as engaging in pottery and painting.",
      "reference": "Running, pottery",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.09523809523809525,
        "rouge1_f": 0.08,
        "rouge2_f": 0.0,
        "rougeL_f": 0.08,
        "bleu1": 0.07142857142857141,
        "bleu2": 0.016265001215808875,
        "bleu3": 0.010531409528076477,
        "bleu4": 0.007987276352377327,
        "bert_precision": 0.8155385255813599,
        "bert_recall": 0.878697395324707,
        "bert_f1": 0.8459407091140747,
        "meteor": 0.12195121951219513,
        "sbert_similarity": 0.39624592661857605
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline go to the LGBTQ conference?",
      "prediction": "Two days ago",
      "reference": "10 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7723919749259949,
        "bert_recall": 0.7883407473564148,
        "bert_f1": 0.7802848815917969,
        "meteor": 0.0,
        "sbert_similarity": 0.27674952149391174
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie read the book \"nothing is impossible\"?",
      "prediction": "13 October 2023",
      "reference": 2022,
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7924638986587524,
        "bert_recall": 0.8898553848266602,
        "bert_f1": 0.8383405804634094,
        "meteor": 0.0,
        "sbert_similarity": 0.638525128364563
      }
    },
    {
      "sample_id": 0,
      "question": "Would Caroline pursue writing as a career option?",
      "prediction": "Yes, Caroline is pursuing writing as a career option.",
      "reference": "LIkely no; though she likes reading, she wants to be a counselor",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.09999999999999999,
        "rouge1_f": 0.09523809523809525,
        "rouge2_f": 0.0,
        "rougeL_f": 0.09523809523809525,
        "bleu1": 0.1384182521267043,
        "bleu2": 0.03246195756111535,
        "bleu3": 0.021494944828486344,
        "bleu4": 0.01706602014357155,
        "bert_precision": 0.8881039023399353,
        "bert_recall": 0.8483178615570068,
        "bert_f1": 0.8677550554275513,
        "meteor": 0.08547008547008547,
        "sbert_similarity": 0.4812922179698944
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline go to the adoption meeting?",
      "prediction": "15 July 2023",
      "reference": "The friday before 15 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.36787944117144233,
        "bleu3": 0.36787944117144233,
        "bleu4": 0.20687381245863395,
        "bert_precision": 0.9536237716674805,
        "bert_recall": 0.8549337983131409,
        "bert_f1": 0.9015861749649048,
        "meteor": 0.5165692007797271,
        "sbert_similarity": 0.8067606687545776
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go to the pottery workshop?",
      "prediction": "25 August 2023",
      "reference": "The Friday before 15 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0.12262648039048077,
        "bleu2": 0.04749303163563625,
        "bleu3": 0.044556965439969995,
        "bleu4": 0.0417991745584234,
        "bert_precision": 0.9493988752365112,
        "bert_recall": 0.8762023448944092,
        "bert_f1": 0.9113332629203796,
        "meteor": 0.08771929824561403,
        "sbert_similarity": 0.7332463264465332
      }
    },
    {
      "sample_id": 0,
      "question": "Would Melanie be considered a member of the LGBTQ community?",
      "prediction": "Yes, Melanie would be considered a member of the LGBTQ community.",
      "reference": "Likely no, she does not refer to herself as part of it",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.08695652173913043,
        "rouge1_f": 0.08695652173913043,
        "rouge2_f": 0.0,
        "rougeL_f": 0.08695652173913043,
        "bleu1": 0.15384615384615385,
        "bleu2": 0.03580574370197164,
        "bleu3": 0.023547737997202885,
        "bleu4": 0.018476860420522198,
        "bert_precision": 0.8832228183746338,
        "bert_recall": 0.8672050833702087,
        "bert_f1": 0.8751406073570251,
        "meteor": 0.04201680672268908,
        "sbert_similarity": 0.5309807062149048
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go camping in June?",
      "prediction": "27 June",
      "reference": "The week before 27 June 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.33333333333333337,
        "rougeL_f": 0.5,
        "bleu1": 0.1353352832366127,
        "bleu2": 0.1353352832366127,
        "bleu3": 0.06330106782582003,
        "bleu4": 0.04279677428117006,
        "bert_precision": 0.9131029844284058,
        "bert_recall": 0.8236894607543945,
        "bert_f1": 0.8660945892333984,
        "meteor": 0.33482142857142855,
        "sbert_similarity": 0.6680141687393188
      }
    },
    {
      "sample_id": 0,
      "question": "What LGBTQ+ events has Caroline participated in?",
      "prediction": "LGBTQ conference, LGBTQ+ pride parade, LGBTQ support group, and an LGBTQ art show.",
      "reference": "Pride parade, school speech, support group",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4705882352941177,
        "rouge1_f": 0.42105263157894735,
        "rouge2_f": 0.23529411764705882,
        "rougeL_f": 0.42105263157894735,
        "bleu1": 0.35294117647058826,
        "bleu2": 0.2572478777137633,
        "bleu3": 0.1670021540516531,
        "bleu4": 0.07492402094280846,
        "bert_precision": 0.8780314326286316,
        "bert_recall": 0.9079028964042664,
        "bert_f1": 0.8927173614501953,
        "meteor": 0.38142620232172475,
        "sbert_similarity": 0.6297132968902588
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline go to a pride parade during the summer?",
      "prediction": "July 2023",
      "reference": "The week before 3 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.33333333333333337,
        "rougeL_f": 0.5,
        "bleu1": 0.1353352832366127,
        "bleu2": 0.1353352832366127,
        "bleu3": 0.06330106782582003,
        "bleu4": 0.04279677428117006,
        "bert_precision": 0.9107513427734375,
        "bert_recall": 0.8165690302848816,
        "bert_f1": 0.8610925078392029,
        "meteor": 0.33482142857142855,
        "sbert_similarity": 0.7879712581634521
      }
    },
    {
      "sample_id": 0,
      "question": "What events has Caroline participated in to help children?",
      "prediction": "Caroline participated in an LGBTQ conference, volunteered at an LGBTQ+ youth center, and joined a mentorship program for LGBTQ youth.",
      "reference": "Mentoring program, school speech",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.09523809523809523,
        "rouge1_f": 0.08333333333333334,
        "rouge2_f": 0.0,
        "rougeL_f": 0.08333333333333334,
        "bleu1": 0.08695652173913045,
        "bleu2": 0.01988106931218861,
        "bleu3": 0.012901430308862295,
        "bleu4": 0.009849349468888718,
        "bert_precision": 0.8342622518539429,
        "bert_recall": 0.8519064784049988,
        "bert_f1": 0.8429921269416809,
        "meteor": 0.0,
        "sbert_similarity": 0.3673502802848816
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go camping in July?",
      "prediction": "July 20th",
      "reference": "two weekends before 17 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.06766764161830635,
        "bleu2": 0.030261889307125377,
        "bleu3": 0.02355435073584915,
        "bleu4": 0.020237345084409754,
        "bert_precision": 0.8396520614624023,
        "bert_recall": 0.8153921365737915,
        "bert_f1": 0.8273442983627319,
        "meteor": 0.08928571428571427,
        "sbert_similarity": 0.5418974757194519
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline join a mentorship program?",
      "prediction": "Last weekend",
      "reference": "The weekend before 17 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.06766764161830635,
        "bleu2": 0.030261889307125377,
        "bleu3": 0.02355435073584915,
        "bleu4": 0.020237345084409754,
        "bert_precision": 0.9110257625579834,
        "bert_recall": 0.8219818472862244,
        "bert_f1": 0.8642162084579468,
        "meteor": 0.08928571428571427,
        "sbert_similarity": 0.6107604503631592
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie paint recently?",
      "prediction": "a horse",
      "reference": "sunset",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8435240983963013,
        "bert_recall": 0.9141997694969177,
        "bert_f1": 0.8774410486221313,
        "meteor": 0.0,
        "sbert_similarity": 0.07165352255105972
      }
    },
    {
      "sample_id": 0,
      "question": "What activities has Melanie done with her family?",
      "prediction": "Melanie has gone camping, attended beach outings, and played games with her family.",
      "reference": "Pottery, painting, camping, museum, swimming, hiking",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.10526315789473684,
        "rouge1_f": 0.10526315789473684,
        "rouge2_f": 0.0,
        "rougeL_f": 0.10526315789473684,
        "bleu1": 0.1875,
        "bleu2": 0.11180339887498951,
        "bleu3": 0.04610606439635425,
        "bleu4": 0.02878787818101127,
        "bert_precision": 0.8565183877944946,
        "bert_recall": 0.8410259485244751,
        "bert_f1": 0.8487014770507812,
        "meteor": 0.07462686567164178,
        "sbert_similarity": 0.3187120854854584
      }
    },
    {
      "sample_id": 0,
      "question": "In what ways is Caroline participating in the LGBTQ community?",
      "prediction": "Caroline is participating in the LGBTQ community by joining an activist group, attending conferences, organizing art shows, and hosting pride events.",
      "reference": "Joining activist group, going to pride parades, participating in an art show, mentoring program",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4571428571428571,
        "rouge1_f": 0.5142857142857143,
        "rouge2_f": 0.18181818181818185,
        "rougeL_f": 0.2857142857142857,
        "bleu1": 0.44,
        "bleu2": 0.2345207879911715,
        "bleu3": 0.1364424476515439,
        "bleu4": 0.057418658700625666,
        "bert_precision": 0.9023132920265198,
        "bert_recall": 0.8943397998809814,
        "bert_f1": 0.8983089327812195,
        "meteor": 0.42942176870748294,
        "sbert_similarity": 0.5797178745269775
      }
    },
    {
      "sample_id": 0,
      "question": "How many times has Melanie gone to the beach in 2023?",
      "prediction": "Melanie has gone to the beach once in 2023.",
      "reference": 2,
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8390164375305176,
        "bert_recall": 0.8757498264312744,
        "bert_f1": 0.8569896817207336,
        "meteor": 0.0,
        "sbert_similarity": 0.027985263615846634
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline join a new activist group?",
      "prediction": "20 July 2023",
      "reference": "The Tuesday before 20 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.36787944117144233,
        "bleu3": 0.36787944117144233,
        "bleu4": 0.20687381245863395,
        "bert_precision": 0.9568144679069519,
        "bert_recall": 0.8813892602920532,
        "bert_f1": 0.9175544381141663,
        "meteor": 0.5165692007797271,
        "sbert_similarity": 0.8518600463867188
      }
    },
    {
      "sample_id": 0,
      "question": "Would Melanie be more interested in going to a national park or a theme park?",
      "prediction": "national park",
      "reference": "National park; she likes the outdoors",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.5,
        "rouge2_f": 0.33333333333333337,
        "rougeL_f": 0.5,
        "bleu1": 0.0820849986238988,
        "bleu2": 0.0820849986238988,
        "bleu3": 0.038394038428908775,
        "bleu4": 0.025957555738330737,
        "bert_precision": 0.9354639053344727,
        "bert_recall": 0.8694859743118286,
        "bert_f1": 0.9012690782546997,
        "meteor": 0.08928571428571427,
        "sbert_similarity": 0.7252469062805176
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of art does Caroline make?",
      "prediction": "Caroline makes paintings, particularly focusing on LGBTQ themes and unity, and also creates pottery.",
      "reference": "abstract art",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8204284906387329,
        "bert_recall": 0.8592386245727539,
        "bert_f1": 0.8393851518630981,
        "meteor": 0.0,
        "sbert_similarity": 0.3965967893600464
      }
    },
    {
      "sample_id": 0,
      "question": "When is Melanie's daughter's birthday?",
      "prediction": "14 August",
      "reference": "13 August",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.5,
        "bleu2": 0.223606797749979,
        "bleu3": 0.1740444189610778,
        "bleu4": 0.1495348781221221,
        "bert_precision": 0.9991520643234253,
        "bert_recall": 0.9991520643234253,
        "bert_f1": 0.9991520643234253,
        "meteor": 0.25,
        "sbert_similarity": 0.9299021363258362
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline attend a pride parade in August?",
      "prediction": "25 August",
      "reference": "The Friday before 14 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.06766764161830635,
        "bleu2": 0.030261889307125377,
        "bleu3": 0.02355435073584915,
        "bleu4": 0.020237345084409754,
        "bert_precision": 0.9226551055908203,
        "bert_recall": 0.8170892000198364,
        "bert_f1": 0.8666693568229675,
        "meteor": 0.08928571428571427,
        "sbert_similarity": 0.5569943189620972
      }
    },
    {
      "sample_id": 0,
      "question": "Would Melanie be considered an ally to the transgender community?",
      "prediction": "Yes, Melanie would be considered an ally to the transgender community.",
      "reference": "Yes, she is supportive",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.13333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.13333333333333333,
        "bleu1": 0.15384615384615385,
        "bleu2": 0.11322770341445958,
        "bleu3": 0.05034417113155098,
        "bleu4": 0.03285702044797773,
        "bert_precision": 0.8802454471588135,
        "bert_recall": 0.9077224731445312,
        "bert_f1": 0.8937728404998779,
        "meteor": 0.10638297872340427,
        "sbert_similarity": 0.38908588886260986
      }
    },
    {
      "sample_id": 0,
      "question": "Who supports Caroline when she has a negative experience?",
      "prediction": "Melanie and her family support Caroline when she has a negative experience.",
      "reference": "Her mentors, family, and friends",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.35294117647058826,
        "rouge1_f": 0.35294117647058826,
        "rouge2_f": 0.0,
        "rougeL_f": 0.23529411764705882,
        "bleu1": 0.23076923076923078,
        "bleu2": 0.04385290096535146,
        "bleu3": 0.02691902405124093,
        "bleu4": 0.020448007360218394,
        "bert_precision": 0.874793529510498,
        "bert_recall": 0.8757562637329102,
        "bert_f1": 0.8752745985984802,
        "meteor": 0.17543859649122806,
        "sbert_similarity": 0.4683527946472168
      }
    },
    {
      "sample_id": 0,
      "question": "What types of pottery have Melanie and her kids made?",
      "prediction": "clay sculptures",
      "reference": "bowls, cup",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8815881609916687,
        "bert_recall": 0.8504478931427002,
        "bert_f1": 0.865738034248352,
        "meteor": 0.0,
        "sbert_similarity": 0.23033247888088226
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline and Melanie go to a pride fesetival together?",
      "prediction": "Last year",
      "reference": 2022,
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8711665868759155,
        "bert_recall": 0.9119585752487183,
        "bert_f1": 0.8910959959030151,
        "meteor": 0.0,
        "sbert_similarity": 0.3672371804714203
      }
    },
    {
      "sample_id": 0,
      "question": "What would Caroline's political leaning likely be?",
      "prediction": "progressive and supportive of LGBTQ rights",
      "reference": "Liberal",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7975392937660217,
        "bert_recall": 0.8624520301818848,
        "bert_f1": 0.8287265300750732,
        "meteor": 0.0,
        "sbert_similarity": 0.4432308077812195
      }
    },
    {
      "sample_id": 0,
      "question": "What has Melanie painted?",
      "prediction": "Melanie has painted a horse, flowers, a sunset, and pottery creations.",
      "reference": "Horse, sunset, sunrise",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.30769230769230765,
        "rouge1_f": 0.28571428571428575,
        "rouge2_f": 0.0,
        "rougeL_f": 0.28571428571428575,
        "bleu1": 0.26666666666666666,
        "bleu2": 0.1951800145897066,
        "bleu3": 0.06824738606716123,
        "bleu4": 0.039530883746698414,
        "bert_precision": 0.8388570547103882,
        "bert_recall": 0.844746470451355,
        "bert_f1": 0.8417915105819702,
        "meteor": 0.26315789473684215,
        "sbert_similarity": 0.4651416540145874
      }
    },
    {
      "sample_id": 0,
      "question": "What are Melanie's pets' names?",
      "prediction": "Luna and Oliver",
      "reference": "Oliver, Luna, Bailey",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.3422780793550613,
        "bleu2": 0.09373671250115519,
        "bleu3": 0.07816643970700751,
        "bleu4": 0.069372929071742,
        "bert_precision": 0.9074834585189819,
        "bert_recall": 0.8730453252792358,
        "bert_f1": 0.889931321144104,
        "meteor": 0.0,
        "sbert_similarity": 0.8309195041656494
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline apply to adoption agencies?",
      "prediction": "August 23rd",
      "reference": "The week of 23 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.06766764161830635,
        "bleu2": 0.030261889307125377,
        "bleu3": 0.02355435073584915,
        "bleu4": 0.020237345084409754,
        "bert_precision": 0.8690527677536011,
        "bert_recall": 0.8186139464378357,
        "bert_f1": 0.8430796265602112,
        "meteor": 0.08928571428571427,
        "sbert_similarity": 0.6310411691665649
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline draw a self-portrait?",
      "prediction": "23 August 2023",
      "reference": "The week before 23 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.36787944117144233,
        "bleu3": 0.36787944117144233,
        "bleu4": 0.20687381245863395,
        "bert_precision": 0.955659031867981,
        "bert_recall": 0.8824602961540222,
        "bert_f1": 0.9176022410392761,
        "meteor": 0.5165692007797271,
        "sbert_similarity": 0.8495972752571106
      }
    },
    {
      "sample_id": 0,
      "question": "What subject have Caroline and Melanie both painted?",
      "prediction": "A sunset with a palm tree",
      "reference": "Sunsets",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.2857142857142857,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2857142857142857,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8604836463928223,
        "bert_recall": 0.8904260396957397,
        "bert_f1": 0.8751987814903259,
        "meteor": 0.33333333333333337,
        "sbert_similarity": 0.6791712045669556
      }
    },
    {
      "sample_id": 0,
      "question": "What symbols are important to Caroline?",
      "prediction": "A necklace with a cross and a heart, and the LGBTQ center symbol.",
      "reference": "Rainbow flag, transgender symbol",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14285714285714288,
        "rouge1_f": 0.11764705882352941,
        "rouge2_f": 0.0,
        "rougeL_f": 0.11764705882352941,
        "bleu1": 0.13333333333333333,
        "bleu2": 0.030860669992418384,
        "bleu3": 0.020202551616772847,
        "bleu4": 0.015718877363021206,
        "bert_precision": 0.867361843585968,
        "bert_recall": 0.8845776915550232,
        "bert_f1": 0.8758851885795593,
        "meteor": 0.0,
        "sbert_similarity": 0.5839919447898865
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline encounter people on a hike and have a negative experience?",
      "prediction": "Never mentioned.",
      "reference": "The week before 25 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8934595584869385,
        "bert_recall": 0.7928289175033569,
        "bert_f1": 0.8401415944099426,
        "meteor": 0.0,
        "sbert_similarity": 0.16463962197303772
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie make a plate in pottery class?",
      "prediction": "25 August 2023",
      "reference": "24 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.5773502691896257,
        "bleu3": 0.3254992452916931,
        "bleu4": 0.24028114141347542,
        "bert_precision": 0.9997365474700928,
        "bert_recall": 0.9997365474700928,
        "bert_f1": 0.9997365474700928,
        "meteor": 0.625,
        "sbert_similarity": 0.9577715396881104
      }
    },
    {
      "sample_id": 0,
      "question": "Would Caroline be considered religious?",
      "prediction": "Not explicitly, but she shows devotion through actions like adoption and sharing her story.",
      "reference": "Somewhat, but not extremely religious",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.21052631578947364,
        "rouge1_f": 0.21052631578947364,
        "rouge2_f": 0.0,
        "rougeL_f": 0.10526315789473682,
        "bleu1": 0.1875,
        "bleu2": 0.11180339887498951,
        "bleu3": 0.04610606439635425,
        "bleu4": 0.02878787818101127,
        "bert_precision": 0.8605080246925354,
        "bert_recall": 0.8508398532867432,
        "bert_f1": 0.8556466698646545,
        "meteor": 0.16949152542372883,
        "sbert_similarity": 0.4205699563026428
      }
    },
    {
      "sample_id": 0,
      "question": "What instruments does Melanie play?",
      "prediction": "Melanie does not mention playing any specific instruments.",
      "reference": "clarinet and violin",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8503504991531372,
        "bert_recall": 0.8299134969711304,
        "bert_f1": 0.8400077223777771,
        "meteor": 0.0,
        "sbert_similarity": 0.4162810742855072
      }
    },
    {
      "sample_id": 0,
      "question": "What musical artists/bands has Melanie seen?",
      "prediction": "Melanie has seen a band playing on a stage in a park and Matt Patterson.",
      "reference": "Summer Sounds, Matt Patterson",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.23529411764705882,
        "rouge1_f": 0.2105263157894737,
        "rouge2_f": 0.11764705882352941,
        "rougeL_f": 0.2105263157894737,
        "bleu1": 0.12500000000000003,
        "bleu2": 0.09128709291752769,
        "bleu3": 0.04033183084278514,
        "bleu4": 0.026012784404037925,
        "bert_precision": 0.8612422347068787,
        "bert_recall": 0.8821622729301453,
        "bert_f1": 0.8715767860412598,
        "meteor": 0.09803921568627452,
        "sbert_similarity": 0.3031494617462158
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go to the park?",
      "prediction": "Yesterday",
      "reference": "27 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8929614424705505,
        "bert_recall": 0.8002703189849854,
        "bert_f1": 0.844078779220581,
        "meteor": 0.0,
        "sbert_similarity": 0.4052075743675232
      }
    },
    {
      "sample_id": 0,
      "question": "When is Caroline's youth center putting on a talent show?",
      "prediction": "Next month",
      "reference": "September 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.865223228931427,
        "bert_recall": 0.8351293206214905,
        "bert_f1": 0.8499099612236023,
        "meteor": 0.0,
        "sbert_similarity": 0.536835789680481
      }
    },
    {
      "sample_id": 0,
      "question": "Would Melanie likely enjoy the song \"The Four Seasons\" by Vivaldi?",
      "prediction": "Yes, Melanie would likely enjoy the song \"The Four Seasons\" by Vivaldi.",
      "reference": "Yes; it's classical music",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.11764705882352941,
        "rouge2_f": 0.0,
        "rougeL_f": 0.11764705882352941,
        "bleu1": 0.0625,
        "bleu2": 0.02041241452319315,
        "bleu3": 0.015007489164383502,
        "bleu4": 0.012300686288463768,
        "bert_precision": 0.8576990962028503,
        "bert_recall": 0.8742061853408813,
        "bert_f1": 0.8658739924430847,
        "meteor": 0.0,
        "sbert_similarity": 0.38833820819854736
      }
    },
    {
      "sample_id": 0,
      "question": "What are some changes Caroline has faced during her transition journey?",
      "prediction": "Transitioning wasn't easy and acceptance wasn't either, but the help from friends, family and people she looked up to was invaluable.",
      "reference": "Changes to her body, losing unsupportive friends",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.15384615384615385,
        "rouge1_f": 0.13333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.06666666666666667,
        "bleu1": 0.1153846153846154,
        "bleu2": 0.02148344622118299,
        "bleu3": 0.012993282967201706,
        "bleu4": 0.00956240657444202,
        "bert_precision": 0.8493766188621521,
        "bert_recall": 0.8655898571014404,
        "bert_f1": 0.8574065566062927,
        "meteor": 0.059523809523809514,
        "sbert_similarity": 0.32491281628608704
      }
    },
    {
      "sample_id": 0,
      "question": "What does Melanie do with her family on hikes?",
      "prediction": "roast marshmallows around the campfire",
      "reference": "Roast marshmallows, tell stories",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4444444444444445,
        "rouge1_f": 0.4444444444444445,
        "rouge2_f": 0.28571428571428575,
        "rougeL_f": 0.4444444444444445,
        "bleu1": 0.4,
        "bleu2": 0.316227766016838,
        "bleu3": 0.1522474354853864,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.9092127084732056,
        "bert_recall": 0.8872592449188232,
        "bert_f1": 0.8981018662452698,
        "meteor": 0.12195121951219512,
        "sbert_similarity": 0.8234604001045227
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline go biking with friends?",
      "prediction": "13 September 2023",
      "reference": "The weekend before 13 September 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.36787944117144233,
        "bleu3": 0.36787944117144233,
        "bleu4": 0.20687381245863395,
        "bert_precision": 0.9524567127227783,
        "bert_recall": 0.8835717439651489,
        "bert_f1": 0.9167219996452332,
        "meteor": 0.5165692007797271,
        "sbert_similarity": 0.8179513216018677
      }
    },
    {
      "sample_id": 0,
      "question": "How long has Melanie been practicing art?",
      "prediction": "Seven years",
      "reference": "Since 2016",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9037601947784424,
        "bert_recall": 0.9037601947784424,
        "bert_f1": 0.9037601947784424,
        "meteor": 0.0,
        "sbert_similarity": 0.4254474341869354
      }
    },
    {
      "sample_id": 0,
      "question": "What personality traits might Melanie say Caroline has?",
      "prediction": "Strength, courage, resilience, authenticity, and passion.",
      "reference": "Thoughtful, authentic, driven",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0.18181818181818182,
        "bleu2": 0.042640143271122095,
        "bleu3": 0.028234511901075607,
        "bleu4": 0.022416933501922302,
        "bert_precision": 0.905722439289093,
        "bert_recall": 0.8916805982589722,
        "bert_f1": 0.8986465930938721,
        "meteor": 0.0,
        "sbert_similarity": 0.45173972845077515
      }
    },
    {
      "sample_id": 0,
      "question": "What transgender-specific events has Caroline attended?",
      "prediction": "LGBTQ support group, transgender conference, LGBTQ conference, LGBTQ+ pride parade, LGBTQ art show",
      "reference": "Poetry reading, conference",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.15384615384615383,
        "rouge1_f": 0.125,
        "rouge2_f": 0.0,
        "rougeL_f": 0.125,
        "bleu1": 0.11764705882352941,
        "bleu2": 0.027116307227332027,
        "bleu3": 0.017693837921724947,
        "bleu4": 0.013679192123121896,
        "bert_precision": 0.8122414946556091,
        "bert_recall": 0.8746896982192993,
        "bert_f1": 0.8423097729682922,
        "meteor": 0.0,
        "sbert_similarity": 0.33206743001937866
      }
    },
    {
      "sample_id": 0,
      "question": "What book did Melanie read from Caroline's suggestion?",
      "prediction": "The book with a gold coin on its cover",
      "reference": "\"Becoming Nicole\"",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8049065470695496,
        "bert_recall": 0.8318539261817932,
        "bert_f1": 0.8181583881378174,
        "meteor": 0.0,
        "sbert_similarity": 0.00461356807500124
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie's friend adopt a child?",
      "prediction": "October 22nd",
      "reference": 2022,
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7953130006790161,
        "bert_recall": 0.9025552272796631,
        "bert_f1": 0.8455472588539124,
        "meteor": 0.0,
        "sbert_similarity": 0.3283904790878296
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie get hurt?",
      "prediction": "20 October 2023",
      "reference": "September 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.9044314622879028,
        "bert_recall": 0.9542666077613831,
        "bert_f1": 0.928680956363678,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.7736971378326416
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie's family go on a roadtrip?",
      "prediction": "20 July 2023",
      "reference": "The weekend before 20 October 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4444444444444444,
        "rouge1_f": 0.4444444444444444,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4444444444444444,
        "bleu1": 0.24525296078096154,
        "bleu2": 0.06716528945733123,
        "bleu3": 0.05600870148614917,
        "bleu4": 0.04970787578611783,
        "bert_precision": 0.9377862811088562,
        "bert_recall": 0.871595561504364,
        "bert_f1": 0.9034802317619324,
        "meteor": 0.17543859649122806,
        "sbert_similarity": 0.7790977358818054
      }
    },
    {
      "sample_id": 0,
      "question": "How many children does Melanie have?",
      "prediction": "The context does not specify the number of children Melanie has.",
      "reference": 3,
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8106542825698853,
        "bert_recall": 0.8249775767326355,
        "bert_f1": 0.817753255367279,
        "meteor": 0.0,
        "sbert_similarity": 0.12139337509870529
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go on a hike after the roadtrip?",
      "prediction": "After the road trip, Melanie went on a hike on 15 July 2023.",
      "reference": "19 October 2023",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.125,
        "rouge2_f": 0.0,
        "rougeL_f": 0.125,
        "bleu1": 0.06666666666666667,
        "bleu2": 0.02182178902359924,
        "bleu3": 0.016071866876084712,
        "bleu4": 0.013217947626377298,
        "bert_precision": 0.8611609935760498,
        "bert_recall": 0.902692973613739,
        "bert_f1": 0.8814379572868347,
        "meteor": 0.0,
        "sbert_similarity": 0.4238954186439514
      }
    },
    {
      "sample_id": 0,
      "question": "Would Melanie go on another roadtrip soon?",
      "prediction": "Melanie went on a road trip recently and is planning another one with her family.",
      "reference": "Likely no; since this one went badly",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.18181818181818182,
        "rouge1_f": 0.18181818181818182,
        "rouge2_f": 0.0,
        "rougeL_f": 0.09090909090909091,
        "bleu1": 0.12500000000000003,
        "bleu2": 0.028867513459481287,
        "bleu3": 0.018864614597621485,
        "bleu4": 0.014628063653657535,
        "bert_precision": 0.8545507192611694,
        "bert_recall": 0.8343161344528198,
        "bert_f1": 0.8443121910095215,
        "meteor": 0.12820512820512822,
        "sbert_similarity": 0.10896069556474686
      }
    },
    {
      "sample_id": 0,
      "question": "What items has Melanie bought?",
      "prediction": "None mentioned.",
      "reference": "Figurines, shoes",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8978649377822876,
        "bert_recall": 0.7719936370849609,
        "bert_f1": 0.8301852345466614,
        "meteor": 0.0,
        "sbert_similarity": 0.16318602859973907
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline pass the adoption interview?",
      "prediction": "On 2023-10-13",
      "reference": "The Friday before 22 October 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.2,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8397926688194275,
        "bert_recall": 0.8169229030609131,
        "bert_f1": 0.8281999230384827,
        "meteor": 0.0,
        "sbert_similarity": 0.6557557582855225
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie buy the figurines?",
      "prediction": "22 October 2023",
      "reference": "21 October 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.5773502691896257,
        "bleu3": 0.3254992452916931,
        "bleu4": 0.24028114141347542,
        "bert_precision": 0.9986072182655334,
        "bert_recall": 0.9986072182655334,
        "bert_f1": 0.9986072182655334,
        "meteor": 0.625,
        "sbert_similarity": 0.9786192178726196
      }
    },
    {
      "sample_id": 0,
      "question": "Would Caroline want to move back to her home country soon?",
      "prediction": "No specific mention of moving back to her home country is made in the context.",
      "reference": "No; she's in the process of adopting children.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.26086956521739135,
        "rouge1_f": 0.33333333333333337,
        "rouge2_f": 0.09090909090909091,
        "rougeL_f": 0.25,
        "bleu1": 0.3125,
        "bleu2": 0.14433756729740646,
        "bleu3": 0.05457178254306466,
        "bleu4": 0.032709344708772174,
        "bert_precision": 0.8667200803756714,
        "bert_recall": 0.8858332633972168,
        "bert_f1": 0.8761724233627319,
        "meteor": 0.2937420178799489,
        "sbert_similarity": 0.3723016381263733
      }
    },
    {
      "sample_id": 0,
      "question": "What did the charity race raise awareness for?",
      "prediction": "mental health",
      "reference": "mental health",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie realize after the charity race?",
      "prediction": "Taking care of our minds.",
      "reference": "self-care is important",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.22222222222222224,
        "rouge2_f": 0.0,
        "rougeL_f": 0.22222222222222224,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8889919519424438,
        "bert_recall": 0.9018319249153137,
        "bert_f1": 0.8953658938407898,
        "meteor": 0.0,
        "sbert_similarity": 0.4673510193824768
      }
    },
    {
      "sample_id": 0,
      "question": "How does Melanie prioritize self-care?",
      "prediction": "Melanie prioritizes self-care by running a charity race for mental health and spending quality, tranquil time with her family.",
      "reference": "by carving out some me-time each day for activities like running, reading, or playing the violin",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.17142857142857143,
        "rouge1_f": 0.2162162162162162,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1081081081081081,
        "bleu1": 0.19047619047619047,
        "bleu2": 0.030860669992418384,
        "bleu3": 0.01782456102251381,
        "bleu4": 0.01291802583371725,
        "bert_precision": 0.8639816045761108,
        "bert_recall": 0.8733724355697632,
        "bert_f1": 0.8686516284942627,
        "meteor": 0.09202453987730061,
        "sbert_similarity": 0.23245467245578766
      }
    },
    {
      "sample_id": 0,
      "question": "What are Caroline's plans for the summer?",
      "prediction": "Caroline's plans for the summer include continuing her education and exploring career options.",
      "reference": "researching adoption agencies",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8410235643386841,
        "bert_recall": 0.8456876873970032,
        "bert_f1": 0.8433491587638855,
        "meteor": 0.0,
        "sbert_similarity": 0.09348838031291962
      }
    },
    {
      "sample_id": 0,
      "question": "What type of individuals does the adoption agency Caroline is considering support?",
      "prediction": "LGBTQ+ folks",
      "reference": "LGBTQ+ individuals",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.5,
        "bleu2": 0.223606797749979,
        "bleu3": 0.1740444189610778,
        "bleu4": 0.1495348781221221,
        "bert_precision": 0.9660412073135376,
        "bert_recall": 0.9660412073135376,
        "bert_f1": 0.9660412073135376,
        "meteor": 0.25,
        "sbert_similarity": 0.9303994178771973
      }
    },
    {
      "sample_id": 0,
      "question": "Why did Caroline choose the adoption agency?",
      "prediction": "I chose them 'cause they help LGBTQ+ folks with adoption.",
      "reference": "because of their inclusivity and support for LGBTQ+ individuals",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.10526315789473685,
        "rouge1_f": 0.10526315789473685,
        "rouge2_f": 0.0,
        "rougeL_f": 0.10526315789473685,
        "bleu1": 0.0909090909090909,
        "bleu2": 0.03015113445777637,
        "bleu3": 0.022461584318318094,
        "bleu4": 0.018850319022747353,
        "bert_precision": 0.8810781836509705,
        "bert_recall": 0.8610537648200989,
        "bert_f1": 0.8709508776664734,
        "meteor": 0.05494505494505494,
        "sbert_similarity": 0.46339136362075806
      }
    },
    {
      "sample_id": 0,
      "question": "What is Caroline excited about in the adoption process?",
      "prediction": "Caroline is excited about the adoption process because she passed the adoption agency interviews and is moving towards her goal of having a family.",
      "reference": "creating a family for kids who need one",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13793103448275862,
        "rouge1_f": 0.125,
        "rouge2_f": 0.06666666666666667,
        "rougeL_f": 0.125,
        "bleu1": 0.07999999999999999,
        "bleu2": 0.05773502691896258,
        "bleu3": 0.025303453550777042,
        "bleu4": 0.016020720994064933,
        "bert_precision": 0.8327552676200867,
        "bert_recall": 0.836867094039917,
        "bert_f1": 0.8348060846328735,
        "meteor": 0.052083333333333336,
        "sbert_similarity": 0.3412903845310211
      }
    },
    {
      "sample_id": 0,
      "question": "What does Melanie think about Caroline's decision to adopt?",
      "prediction": "Melanie thinks Caroline's decision to adopt is kind and will benefit her future family.",
      "reference": "she thinks Caroline is doing something amazing and will be an awesome mom",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.29629629629629634,
        "rouge1_f": 0.3571428571428571,
        "rouge2_f": 0.15384615384615383,
        "rougeL_f": 0.3571428571428571,
        "bleu1": 0.3125,
        "bleu2": 0.20412414523193154,
        "bleu3": 0.06859746052812911,
        "bleu4": 0.03889818545474848,
        "bert_precision": 0.9135243892669678,
        "bert_recall": 0.9177302122116089,
        "bert_f1": 0.9156224727630615,
        "meteor": 0.24093511450381686,
        "sbert_similarity": 0.6212905645370483
      }
    },
    {
      "sample_id": 0,
      "question": "How long have Mel and her husband been married?",
      "prediction": "5 years",
      "reference": "Mel and her husband have been married for 5 years.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.33333333333333337,
        "rouge1_f": 0.33333333333333337,
        "rouge2_f": 0.19999999999999998,
        "rougeL_f": 0.33333333333333337,
        "bleu1": 0.011108996538242306,
        "bleu2": 0.011108996538242306,
        "bleu3": 0.005196068065373761,
        "bleu4": 0.0035129731579771513,
        "bert_precision": 0.8757777214050293,
        "bert_recall": 0.8306174874305725,
        "bert_f1": 0.8526000380516052,
        "meteor": 0.05434782608695652,
        "sbert_similarity": 0.34713199734687805
      }
    },
    {
      "sample_id": 0,
      "question": "What does Caroline's necklace symbolize?",
      "prediction": "love, faith and strength",
      "reference": "love, faith, and strength",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 0.8187307530779819,
        "bleu2": 0.7090416310250969,
        "bleu3": 0.5181569246973543,
        "bleu4": 0.2737591267534727,
        "bert_precision": 0.9906565546989441,
        "bert_recall": 0.9857159852981567,
        "bert_f1": 0.9881801009178162,
        "meteor": 0.6388888888888888,
        "sbert_similarity": 0.9917946457862854
      }
    },
    {
      "sample_id": 0,
      "question": "What country is Caroline's grandma from?",
      "prediction": "The context does not mention Caroline's grandma's country of origin.",
      "reference": "Sweden",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8094242811203003,
        "bert_recall": 0.8223021626472473,
        "bert_f1": 0.8158124089241028,
        "meteor": 0.0,
        "sbert_similarity": 0.184615358710289
      }
    },
    {
      "sample_id": 0,
      "question": "What was grandma's gift to Caroline?",
      "prediction": "a necklace",
      "reference": "necklace",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.5,
        "bleu2": 0.223606797749979,
        "bleu3": 0.1740444189610778,
        "bleu4": 0.1495348781221221,
        "bert_precision": 0.8547326326370239,
        "bert_recall": 0.9164280295372009,
        "bert_f1": 0.8845058083534241,
        "meteor": 0.45454545454545453,
        "sbert_similarity": 0.9594278931617737
      }
    },
    {
      "sample_id": 0,
      "question": "What is Melanie's hand-painted bowl a reminder of?",
      "prediction": "art and self-expression",
      "reference": "art and self-expression",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 0.9999997615814209,
        "bert_recall": 0.9999997615814209,
        "bert_f1": 0.9999997615814209,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie and her family do while camping?",
      "prediction": "They roasted marshmallows, told stories around the campfire, and enjoyed each other's company.",
      "reference": "explored nature, roasted marshmallows, and went on a hike",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.27272727272727276,
        "rouge1_f": 0.2608695652173913,
        "rouge2_f": 0.09523809523809525,
        "rougeL_f": 0.2608695652173913,
        "bleu1": 0.29411764705882354,
        "bleu2": 0.23483410915693106,
        "bleu3": 0.15725058206571177,
        "bleu4": 0.07158561577277536,
        "bert_precision": 0.8947172164916992,
        "bert_recall": 0.9091918468475342,
        "bert_f1": 0.9018964767456055,
        "meteor": 0.2718676122931442,
        "sbert_similarity": 0.5853726863861084
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of counseling and mental health services is Caroline interested in pursuing?",
      "prediction": "Counseling and mental health services for trans people and LGBTQ+ communities.",
      "reference": "working with trans people, helping them accept themselves and supporting their mental health",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4347826086956522,
        "rouge1_f": 0.41666666666666663,
        "rouge2_f": 0.1818181818181818,
        "rougeL_f": 0.24999999999999994,
        "bleu1": 0.35270071870442254,
        "bleu2": 0.23298642673655665,
        "bleu3": 0.07903658900236418,
        "bleu4": 0.04559460694781592,
        "bert_precision": 0.8778221607208252,
        "bert_recall": 0.8706084489822388,
        "bert_f1": 0.8742004036903381,
        "meteor": 0.24658203125000006,
        "sbert_similarity": 0.7888900637626648
      }
    },
    {
      "sample_id": 0,
      "question": "What workshop did Caroline attend recently?",
      "prediction": "LGBTQ support group",
      "reference": "LGBTQ+ counseling workshop",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8966650366783142,
        "bert_recall": 0.8552902340888977,
        "bert_f1": 0.8754890561103821,
        "meteor": 0.0,
        "sbert_similarity": 0.7010543346405029
      }
    },
    {
      "sample_id": 0,
      "question": "What was discussed in the LGBTQ+ counseling workshop?",
      "prediction": "The discussion focused on transgender journeys, community involvement, and the importance of fighting for trans rights and spreading awareness.",
      "reference": "therapeutic methods and how to best work with trans people",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14814814814814817,
        "rouge1_f": 0.13793103448275862,
        "rouge2_f": 0.0,
        "rougeL_f": 0.13793103448275862,
        "bleu1": 0.0909090909090909,
        "bleu2": 0.020806259464411975,
        "bleu3": 0.013510398157576722,
        "bleu4": 0.010331208012220438,
        "bert_precision": 0.8777000904083252,
        "bert_recall": 0.8719326257705688,
        "bert_f1": 0.874806821346283,
        "meteor": 0.09174311926605504,
        "sbert_similarity": 0.5172026753425598
      }
    },
    {
      "sample_id": 0,
      "question": "What motivated Caroline to pursue counseling?",
      "prediction": "I want to help people who have gone through the same things as me.",
      "reference": "her own journey and the support she received, and how counseling improved her life",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.07692307692307691,
        "rouge1_f": 0.07142857142857142,
        "rouge2_f": 0.0,
        "rougeL_f": 0.07142857142857142,
        "bleu1": 0.06666666666666667,
        "bleu2": 0.02182178902359924,
        "bleu3": 0.016071866876084712,
        "bleu4": 0.013217947626377298,
        "bert_precision": 0.8842028975486755,
        "bert_recall": 0.8664472103118896,
        "bert_f1": 0.8752349615097046,
        "meteor": 0.07142857142857142,
        "sbert_similarity": 0.4090031087398529
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of place does Caroline want to create for people?",
      "prediction": "a safe and loving home",
      "reference": "a safe and inviting place for people to grow",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.42857142857142855,
        "rouge1_f": 0.42857142857142855,
        "rouge2_f": 0.3333333333333333,
        "rougeL_f": 0.42857142857142855,
        "bleu1": 0.26959737847033294,
        "bleu2": 0.24610760938743165,
        "bleu3": 0.21016694651579904,
        "bleu4": 0.11948321931215808,
        "bert_precision": 0.9225904941558838,
        "bert_recall": 0.8910179734230042,
        "bert_f1": 0.9065294861793518,
        "meteor": 0.436046511627907,
        "sbert_similarity": 0.556976318359375
      }
    },
    {
      "sample_id": 0,
      "question": "Did Melanie make the black and white bowl in the photo?",
      "prediction": "Yes, Melanie made the black and white bowl in the photo.",
      "reference": "Yes",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.18181818181818182,
        "rouge1_f": 0.16666666666666669,
        "rouge2_f": 0.0,
        "rougeL_f": 0.16666666666666669,
        "bleu1": 0.07692307692307693,
        "bleu2": 0.025318484177091663,
        "bleu3": 0.018733084686676876,
        "bleu4": 0.01553712569276035,
        "bert_precision": 0.819658637046814,
        "bert_recall": 0.8515074253082275,
        "bert_f1": 0.8352795243263245,
        "meteor": 0.0,
        "sbert_similarity": 0.18057876825332642
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of books does Caroline have in her library?",
      "prediction": "Caroline has lots of kids' books- classics, stories from different cultures, educational books, all of that.",
      "reference": "kids' books - classics, stories from different cultures, educational books",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666667,
        "rouge1_f": 0.72,
        "rouge2_f": 0.6956521739130436,
        "rougeL_f": 0.72,
        "bleu1": 0.5238095238095238,
        "bleu2": 0.4855041562276122,
        "bleu3": 0.44645841517299345,
        "bleu4": 0.41247940778259484,
        "bert_precision": 0.9198082685470581,
        "bert_recall": 0.9485026597976685,
        "bert_f1": 0.9339351058006287,
        "meteor": 0.6526761648055449,
        "sbert_similarity": 0.6891958117485046
      }
    },
    {
      "sample_id": 0,
      "question": "What was Melanie's favorite book from her childhood?",
      "prediction": "Charlotte's Web",
      "reference": "\"Charlotte's Web\"",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 0.513417119032592,
        "bleu2": 0.513417119032592,
        "bleu3": 0.513417119032592,
        "bleu4": 0.288715663092199,
        "bert_precision": 0.8835265636444092,
        "bert_recall": 0.8591198921203613,
        "bert_f1": 0.8711523413658142,
        "meteor": 0.0,
        "sbert_similarity": 0.9549331665039062
      }
    },
    {
      "sample_id": 0,
      "question": "What book did Caroline recommend to Melanie?",
      "prediction": "I've got lots of kids' books- classics, stories from different cultures, educational books, all of that.",
      "reference": "\"Becoming Nicole\"",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8078689575195312,
        "bert_recall": 0.8268705010414124,
        "bert_f1": 0.8172593712806702,
        "meteor": 0.0,
        "sbert_similarity": -0.026864223182201385
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline take away from the book \"Becoming Nicole\"?",
      "prediction": "It made me feel connected and gave me a lot of hope for my own path.",
      "reference": "Lessons on self-acceptance and finding support",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.09523809523809522,
        "rouge1_f": 0.08695652173913043,
        "rouge2_f": 0.0,
        "rougeL_f": 0.08695652173913043,
        "bleu1": 0.0588235294117647,
        "bleu2": 0.019174124721184262,
        "bleu3": 0.014076093604378503,
        "bleu4": 0.011502783619900045,
        "bert_precision": 0.8656477332115173,
        "bert_recall": 0.8624590635299683,
        "bert_f1": 0.8640505075454712,
        "meteor": 0.14285714285714288,
        "sbert_similarity": 0.30920058488845825
      }
    },
    {
      "sample_id": 0,
      "question": "What are the new shoes that Melanie got used for?",
      "prediction": "Luna and Oliver! They are so sweet and playful - they really liven up the house! Just got some new shoes, too!",
      "reference": "Running",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7901706695556641,
        "bert_recall": 0.8372116088867188,
        "bert_f1": 0.8130112290382385,
        "meteor": 0.1612903225806452,
        "sbert_similarity": 0.06339344382286072
      }
    },
    {
      "sample_id": 0,
      "question": "What is Melanie's reason for getting into running?",
      "prediction": "To destress and clear my mind.",
      "reference": "To de-stress and clear her mind",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6153846153846153,
        "rouge2_f": 0.1818181818181818,
        "rougeL_f": 0.6153846153846153,
        "bleu1": 0.5714285714285714,
        "bleu2": 0.3086066999241838,
        "bleu3": 0.12657485247324374,
        "bleu4": 0.08307018474412793,
        "bert_precision": 0.9589047431945801,
        "bert_recall": 0.9577292203903198,
        "bert_f1": 0.9583166241645813,
        "meteor": 0.42592592592592593,
        "sbert_similarity": 0.5838675498962402
      }
    },
    {
      "sample_id": 0,
      "question": "What does Melanie say running has been great for?",
      "prediction": "running has been great for destress and clear my mind",
      "reference": "Her mental health",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8279678225517273,
        "bert_recall": 0.8535414934158325,
        "bert_f1": 0.840560257434845,
        "meteor": 0.0,
        "sbert_similarity": 0.16598263382911682
      }
    },
    {
      "sample_id": 0,
      "question": "What did Mel and her kids make during the pottery workshop?",
      "prediction": "clay sculptures",
      "reference": "pots",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8577408194541931,
        "bert_recall": 0.9177078008651733,
        "bert_f1": 0.8867116570472717,
        "meteor": 0.0,
        "sbert_similarity": 0.329764723777771
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of pot did Mel and her kids make with clay?",
      "prediction": "clay sculptures",
      "reference": "a cup with a dog face on it",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8924999237060547,
        "bert_recall": 0.8343054056167603,
        "bert_f1": 0.8624221086502075,
        "meteor": 0.0,
        "sbert_similarity": 0.033175110816955566
      }
    },
    {
      "sample_id": 0,
      "question": "What creative project do Mel and her kids do together besides pottery?",
      "prediction": "They make clay sculptures.",
      "reference": "painting",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8565536737442017,
        "bert_recall": 0.8672937154769897,
        "bert_f1": 0.8618902564048767,
        "meteor": 0.0,
        "sbert_similarity": 0.36311009526252747
      }
    },
    {
      "sample_id": 0,
      "question": "What did Mel and her kids paint in their latest project in July 2023?",
      "prediction": "They painted a sunset with a palm tree.",
      "reference": "a sunset with a palm tree",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8333333333333333,
        "rouge1_f": 0.8571428571428571,
        "rouge2_f": 0.8333333333333333,
        "rougeL_f": 0.8571428571428571,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.6454972243679028,
        "bleu3": 0.6227700724866008,
        "bleu4": 0.5873949094699213,
        "bert_precision": 0.9364925026893616,
        "bert_recall": 0.956539511680603,
        "bert_f1": 0.9464098215103149,
        "meteor": 0.8032258064516131,
        "sbert_similarity": 0.8602186441421509
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline see at the council meeting for adoption?",
      "prediction": "Caroline saw a group supporting each other and making good things happen.",
      "reference": "many people wanting to create loving homes for children in need",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8719939589500427,
        "bert_recall": 0.8691931962966919,
        "bert_f1": 0.870591402053833,
        "meteor": 0.0,
        "sbert_similarity": 0.13577795028686523
      }
    },
    {
      "sample_id": 0,
      "question": "What do sunflowers represent according to Caroline?",
      "prediction": "Warmth and happiness",
      "reference": "warmth and happiness",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 0.9189453125,
        "bert_recall": 0.9765268564224243,
        "bert_f1": 0.9468614459037781,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 0.9999998807907104
      }
    },
    {
      "sample_id": 0,
      "question": "Why are flowers important to Melanie?",
      "prediction": "Flowers bring joy, represent growth, beauty, and remind her to appreciate the small moments, especially in the context of her wedding decor memory.",
      "reference": "They remind her to appreciate the small moments and were a part of her wedding decor",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.611111111111111,
        "rouge1_f": 0.6153846153846153,
        "rouge2_f": 0.4864864864864865,
        "rougeL_f": 0.5641025641025642,
        "bleu1": 0.42857142857142855,
        "bleu2": 0.3779644730092272,
        "bleu3": 0.34123910574576927,
        "bleu4": 0.29615165360116247,
        "bert_precision": 0.8977870941162109,
        "bert_recall": 0.9180082082748413,
        "bert_f1": 0.9077849984169006,
        "meteor": 0.6277527589449202,
        "sbert_similarity": 0.7053731679916382
      }
    },
    {
      "sample_id": 0,
      "question": "What inspired Caroline's painting for the art show?",
      "prediction": "I painted this after I visited a LGBTQ center. I wanted to capture everyone's unity and strength.",
      "reference": "visiting an LGBTQ center and wanting to capture unity and strength",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.56,
        "rouge1_f": 0.6206896551724137,
        "rouge2_f": 0.37037037037037035,
        "rougeL_f": 0.6206896551724137,
        "bleu1": 0.3499999999999999,
        "bleu2": 0.27144835701531844,
        "bleu3": 0.16292721605868904,
        "bleu4": 0.07005082462127948,
        "bert_precision": 0.9122277498245239,
        "bert_recall": 0.9257928133010864,
        "bert_f1": 0.9189602732658386,
        "meteor": 0.5471498944405347,
        "sbert_similarity": 0.680251955986023
      }
    },
    {
      "sample_id": 0,
      "question": "How often does Melanie go to the beach with her kids?",
      "prediction": "usually only once or twice a year",
      "reference": "once or twice a year",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8333333333333333,
        "rouge1_f": 0.8333333333333333,
        "rouge2_f": 0.8,
        "rougeL_f": 0.8333333333333333,
        "bleu1": 0.7142857142857143,
        "bleu2": 0.6900655593423543,
        "bleu3": 0.6613898798842307,
        "bleu4": 0.6147881529512643,
        "bert_precision": 0.925447404384613,
        "bert_recall": 0.9496260285377502,
        "bert_f1": 0.937380850315094,
        "meteor": 0.9576923076923077,
        "sbert_similarity": 0.9151639938354492
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie and her family see during their camping trip last year?",
      "prediction": "They saw the forest and hiking trails.",
      "reference": "Perseid meteor shower",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8379966616630554,
        "bert_recall": 0.7689446806907654,
        "bert_f1": 0.8019870519638062,
        "meteor": 0.0,
        "sbert_similarity": 0.03627149015665054
      }
    },
    {
      "sample_id": 0,
      "question": "How did Melanie feel while watching the meteor shower?",
      "prediction": "She felt tiny and in awe of the universe.",
      "reference": "in awe of the universe",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7142857142857143,
        "rouge1_f": 0.7142857142857143,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.7142857142857143,
        "bleu1": 0.5,
        "bleu2": 0.4714045207910317,
        "bleu3": 0.44042320075255215,
        "bleu4": 0.392814650900513,
        "bert_precision": 0.9070153832435608,
        "bert_recall": 0.9342861175537109,
        "bert_f1": 0.9204488396644592,
        "meteor": 0.7349537037037037,
        "sbert_similarity": 0.6014809608459473
      }
    },
    {
      "sample_id": 0,
      "question": "Whose birthday did Melanie celebrate recently?",
      "prediction": "Melanie celebrated her daughter's birthday recently.",
      "reference": "Melanie's daughter",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.6,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.375,
        "bleu2": 0.07319250547114,
        "bleu3": 0.04610606439635425,
        "bleu4": 0.03655552228545124,
        "bert_precision": 0.8653501868247986,
        "bert_recall": 0.8935995101928711,
        "bert_f1": 0.8792479634284973,
        "meteor": 0.0,
        "sbert_similarity": 0.7502080798149109
      }
    },
    {
      "sample_id": 0,
      "question": "Who performed at the concert at Melanie's daughter's birthday?",
      "prediction": "Matt Patterson",
      "reference": "Matt Patterson",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "Why did Melanie choose to use colors and patterns in her pottery project?",
      "prediction": "To express herself and get creative, it's like therapy for me.",
      "reference": "She wanted to catch the eye and make people smile.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1904761904761905,
        "rouge1_f": 0.1818181818181818,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1818181818181818,
        "bleu1": 0.21428571428571425,
        "bleu2": 0.040599897147057515,
        "bleu3": 0.024859743585800658,
        "bleu4": 0.018798317647335087,
        "bert_precision": 0.8707165718078613,
        "bert_recall": 0.8940719366073608,
        "bert_f1": 0.8822397589683533,
        "meteor": 0.25302530253025296,
        "sbert_similarity": 0.336478590965271
      }
    },
    {
      "sample_id": 0,
      "question": "What pet does Caroline have?",
      "prediction": "Caroline does not have a pet mentioned in the context.",
      "reference": "guinea pig",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8438109159469604,
        "bert_recall": 0.7861658334732056,
        "bert_f1": 0.813969075679779,
        "meteor": 0.0,
        "sbert_similarity": 0.2611124515533447
      }
    },
    {
      "sample_id": 0,
      "question": "What pets does Melanie have?",
      "prediction": "a pup and a kitty",
      "reference": "Two cats and a dog",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4444444444444445,
        "rouge1_f": 0.4000000000000001,
        "rouge2_f": 0.25,
        "rougeL_f": 0.4000000000000001,
        "bleu1": 0.4,
        "bleu2": 0.316227766016838,
        "bleu3": 0.1522474354853864,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.933443009853363,
        "bert_recall": 0.9434006214141846,
        "bert_f1": 0.9383953809738159,
        "meteor": 0.37500000000000006,
        "sbert_similarity": 0.7613354921340942
      }
    },
    {
      "sample_id": 0,
      "question": "Where did Oliver hide his bone once?",
      "prediction": "in my slipper",
      "reference": "In Melanie's slipper",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.5714285714285715,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5714285714285715,
        "bleu1": 0.47768754038252614,
        "bleu2": 0.13082012065333476,
        "bleu3": 0.10909005447984235,
        "bleu4": 0.09681772177713914,
        "bert_precision": 0.9412858486175537,
        "bert_recall": 0.9195078611373901,
        "bert_f1": 0.9302694201469421,
        "meteor": 0.3333333333333333,
        "sbert_similarity": 0.6715268492698669
      }
    },
    {
      "sample_id": 0,
      "question": "What activity did Caroline used to do with her dad?",
      "prediction": "The context does not mention any specific activity that Caroline used to do with her dad.",
      "reference": "Horseback riding",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8182416558265686,
        "bert_recall": 0.7622794508934021,
        "bert_f1": 0.7892698049545288,
        "meteor": 0.0,
        "sbert_similarity": 0.04839414730668068
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline make for a local church?",
      "prediction": "a stained glass window",
      "reference": "a stained glass window",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9921875,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline find in her neighborhood during her walk?",
      "prediction": "Caroline found a gold coin on the book cover.",
      "reference": "a rainbow sidewalk",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.16666666666666666,
        "rouge1_f": 0.16666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.16666666666666666,
        "bleu1": 0.10000000000000002,
        "bleu2": 0.03333333333333335,
        "bleu3": 0.024950558772614845,
        "bleu4": 0.021105340631872645,
        "bert_precision": 0.857778787612915,
        "bert_recall": 0.8482168316841125,
        "bert_f1": 0.8529710173606873,
        "meteor": 0.13888888888888892,
        "sbert_similarity": 0.1453561633825302
      }
    },
    {
      "sample_id": 0,
      "question": "Which song motivates Caroline to be courageous?",
      "prediction": "Brave by Sara Bareilles",
      "reference": "Brave by Sara Bareilles",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.000000238418579,
        "bert_recall": 1.000000238418579,
        "bert_f1": 1.000000238418579,
        "meteor": 0.9921875,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "Which  classical musicians does Melanie enjoy listening to?",
      "prediction": "Bach and Mozart",
      "reference": "Bach and Mozart",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "Who is Melanie a fan of in terms of modern music?",
      "prediction": "Ed Sheeran's 'Perfect'",
      "reference": "Ed Sheeran",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.4,
        "bleu2": 0.316227766016838,
        "bleu3": 0.1522474354853864,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.892833411693573,
        "bert_recall": 0.9782232046127319,
        "bert_f1": 0.9335798621177673,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.7141446471214294
      }
    },
    {
      "sample_id": 0,
      "question": "How long has Melanie been creating art?",
      "prediction": "Seven years now, and I've finally found my real muses: painting and pottery.",
      "reference": "7 years",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14285714285714285,
        "rouge1_f": 0.125,
        "rouge2_f": 0.0,
        "rougeL_f": 0.125,
        "bleu1": 0.0588235294117647,
        "bleu2": 0.019174124721184262,
        "bleu3": 0.014076093604378503,
        "bleu4": 0.011502783619900045,
        "bert_precision": 0.8250266313552856,
        "bert_recall": 0.9018169641494751,
        "bert_f1": 0.8617144227027893,
        "meteor": 0.6048387096774195,
        "sbert_similarity": 0.3006328046321869
      }
    },
    {
      "sample_id": 0,
      "question": "What precautionary sign did Melanie see at the caf\u00e9?",
      "prediction": "thoughtful signs like this",
      "reference": "A sign stating that someone is not being able to leave",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.13333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.13333333333333333,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8430920839309692,
        "bert_recall": 0.818871796131134,
        "bert_f1": 0.8308054208755493,
        "meteor": 0.04854368932038835,
        "sbert_similarity": 0.3900822401046753
      }
    },
    {
      "sample_id": 0,
      "question": "What advice does Caroline give for getting started with adoption?",
      "prediction": "Do your research and find an adoption agency or lawyer.",
      "reference": "Do research, find an adoption agency or lawyer, gather necessary documents, and prepare emotionally.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.75,
        "rouge1_f": 0.75,
        "rouge2_f": 0.4545454545454546,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.48110303772731844,
        "bleu2": 0.35679556205304663,
        "bleu3": 0.3121843074321139,
        "bleu4": 0.2776433663357304,
        "bert_precision": 0.9532625079154968,
        "bert_recall": 0.9194575548171997,
        "bert_f1": 0.9360549449920654,
        "meteor": 0.4944477791116447,
        "sbert_similarity": 0.8361724615097046
      }
    },
    {
      "sample_id": 0,
      "question": "What setback did Melanie face in October 2023?",
      "prediction": "No setback mentioned for Melanie in October 2023.",
      "reference": "She got hurt and had to take a break from pottery.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.07961459006375435,
        "bleu2": 0.026703545307500107,
        "bleu3": 0.02011097358938166,
        "bleu4": 0.017182633466700372,
        "bert_precision": 0.8635607957839966,
        "bert_recall": 0.866338849067688,
        "bert_f1": 0.8649476170539856,
        "meteor": 0.0,
        "sbert_similarity": 0.2428119033575058
      }
    },
    {
      "sample_id": 0,
      "question": "What does Melanie do to keep herself busy during her pottery break?",
      "prediction": "She doesn't mention any specific activities during her pottery break.",
      "reference": "Read a book and paint.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.08333333333333333,
        "bleu2": 0.027524094128159027,
        "bleu3": 0.020427282860665447,
        "bleu4": 0.017033186037639283,
        "bert_precision": 0.8536350727081299,
        "bert_recall": 0.860802412033081,
        "bert_f1": 0.8572037816047668,
        "meteor": 0.0,
        "sbert_similarity": 0.11191874742507935
      }
    },
    {
      "sample_id": 0,
      "question": "What painting did Melanie show to Caroline on October 13, 2023?",
      "prediction": "No painting was mentioned as being shown by Melanie to Caroline on October 13, 2023.",
      "reference": "A painting inspired by sunsets with a pink sky.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1739130434782609,
        "rouge1_f": 0.16666666666666669,
        "rouge2_f": 0.0,
        "rougeL_f": 0.16666666666666669,
        "bleu1": 0.17647058823529413,
        "bleu2": 0.03321055820775358,
        "bleu3": 0.020227031939554035,
        "bleu4": 0.015138514598766055,
        "bert_precision": 0.8585124015808105,
        "bert_recall": 0.8636085987091064,
        "bert_f1": 0.8610529899597168,
        "meteor": 0.15625,
        "sbert_similarity": 0.2816947102546692
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of painting did Caroline share with Melanie on October 13, 2023?",
      "prediction": "A painting of a sunset on a small easel.",
      "reference": "An abstract painting with blue streaks on a wall.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.375,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.125,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.4,
        "bleu2": 0.21081851067789192,
        "bleu3": 0.08428689847931439,
        "bleu4": 0.05307712171072445,
        "bert_precision": 0.9166765809059143,
        "bert_recall": 0.9166573882102966,
        "bert_f1": 0.9166669845581055,
        "meteor": 0.2839506172839506,
        "sbert_similarity": 0.5449498295783997
      }
    },
    {
      "sample_id": 0,
      "question": "What was the poetry reading that Caroline attended about?",
      "prediction": "A transgender poetry reading where transgender people shared their stories through poetry, creating a safe place for self-expression and celebrating their identities.",
      "reference": "It was a transgender poetry reading where transgender people shared their stories.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6206896551724137,
        "rouge1_f": 0.5714285714285714,
        "rouge2_f": 0.5454545454545455,
        "rougeL_f": 0.5714285714285714,
        "bleu1": 0.4583333333333333,
        "bleu2": 0.42349477692995996,
        "bleu3": 0.4062000098523517,
        "bleu4": 0.3839817133079349,
        "bert_precision": 0.9197176694869995,
        "bert_recall": 0.9646424055099487,
        "bert_f1": 0.941644549369812,
        "meteor": 0.6794871794871795,
        "sbert_similarity": 0.8519989252090454
      }
    },
    {
      "sample_id": 0,
      "question": "What did the posters at the poetry reading say?",
      "prediction": "The posters were amazing, so much pride and strength!",
      "reference": "\"Trans Lives Matter\"",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.823750376701355,
        "bert_recall": 0.8320397138595581,
        "bert_f1": 0.8278743028640747,
        "meteor": 0.13888888888888892,
        "sbert_similarity": 0.08765633404254913
      }
    },
    {
      "sample_id": 0,
      "question": "What does Caroline's drawing symbolize for her?",
      "prediction": "It symbolizes her feelings and the beauty of nature.",
      "reference": "Freedom and being true to herself.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.13333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.13333333333333333,
        "bleu1": 0.2,
        "bleu2": 0.04714045207910317,
        "bleu3": 0.03136318607896979,
        "bleu4": 0.025098621243978974,
        "bert_precision": 0.8762882351875305,
        "bert_recall": 0.8687394261360168,
        "bert_f1": 0.8724974989891052,
        "meteor": 0.07936507936507937,
        "sbert_similarity": 0.3497246503829956
      }
    },
    {
      "sample_id": 0,
      "question": "How do Melanie and Caroline describe their journey through life together?",
      "prediction": "Melanie and Caroline describe their journey through life together as a shared experience of growth, support, and making a positive impact, emphasizing family bonds, mutual encouragement, and the importance of inclusivity and acceptance.",
      "reference": "An ongoing adventure of learning and growing.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1142857142857143,
        "rouge1_f": 0.1,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1,
        "bleu1": 0.07692307692307693,
        "bleu2": 0.014227759830611801,
        "bleu3": 0.008581506542254062,
        "bleu4": 0.006243702571093031,
        "bert_precision": 0.8525494933128357,
        "bert_recall": 0.9031349420547485,
        "bert_f1": 0.8771135210990906,
        "meteor": 0.10416666666666667,
        "sbert_similarity": 0.20521962642669678
      }
    },
    {
      "sample_id": 0,
      "question": "What happened to Melanie's son on their road trip?",
      "prediction": "He got into an accident.",
      "reference": "He got into an accident",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 0.8333333333333334,
        "bleu2": 0.816496580927726,
        "bleu3": 0.7955364837549187,
        "bleu4": 0.7598356856515925,
        "bert_precision": 0.9935367703437805,
        "bert_recall": 0.9922503232955933,
        "bert_f1": 0.9928930997848511,
        "meteor": 0.7937500000000002,
        "sbert_similarity": 0.9795499444007874
      }
    },
    {
      "sample_id": 0,
      "question": "How did Melanie's son handle the accident?",
      "prediction": "We were so lucky he was okay.",
      "reference": "He was scared but reassured by his family",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.26666666666666666,
        "rouge1_f": 0.26666666666666666,
        "rouge2_f": 0.15384615384615383,
        "rougeL_f": 0.26666666666666666,
        "bleu1": 0.25,
        "bleu2": 0.18898223650461357,
        "bleu3": 0.08622792534209149,
        "bleu4": 0.05873949094699214,
        "bert_precision": 0.881624698638916,
        "bert_recall": 0.8645632863044739,
        "bert_f1": 0.8730106353759766,
        "meteor": 0.2373417721518987,
        "sbert_similarity": 0.3516126871109009
      }
    },
    {
      "sample_id": 0,
      "question": "How did Melanie feel about her family after the accident?",
      "prediction": "Melanie felt lucky to have her family; they bring so much joy and love.",
      "reference": "They are important and mean the world to her",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.34782608695652173,
        "rouge1_f": 0.34782608695652173,
        "rouge2_f": 0.0,
        "rougeL_f": 0.17391304347826086,
        "bleu1": 0.25,
        "bleu2": 0.040824829046386304,
        "bleu3": 0.023713072854409926,
        "bleu4": 0.017395797375642234,
        "bert_precision": 0.8743017315864563,
        "bert_recall": 0.8947839736938477,
        "bert_f1": 0.8844243288040161,
        "meteor": 0.21052631578947367,
        "sbert_similarity": 0.3270632326602936
      }
    },
    {
      "sample_id": 0,
      "question": "How did Melanie's children handle the accident?",
      "prediction": "Thankfully, my son's ok and that was a reminder that life is precious and to cherish our family.",
      "reference": "They were scared but resilient",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8539778590202332,
        "bert_recall": 0.8505253791809082,
        "bert_f1": 0.8522481322288513,
        "meteor": 0.0,
        "sbert_similarity": 0.17003850638866425
      }
    },
    {
      "sample_id": 0,
      "question": "How did Melanie feel after the accident?",
      "prediction": "Melanie felt lucky to have her family, as they bring so much joy and love.",
      "reference": "Grateful and thankful for her family",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.28571428571428575,
        "rouge1_f": 0.28571428571428575,
        "rouge2_f": 0.10526315789473682,
        "rougeL_f": 0.19047619047619044,
        "bleu1": 0.17647058823529413,
        "bleu2": 0.10502100630210073,
        "bleu3": 0.043244627469917335,
        "bleu4": 0.026920508809559315,
        "bert_precision": 0.872789740562439,
        "bert_recall": 0.8894978165626526,
        "bert_f1": 0.8810645937919617,
        "meteor": 0.14492753623188406,
        "sbert_similarity": 0.4666317105293274
      }
    },
    {
      "sample_id": 0,
      "question": "What was Melanie's reaction to her children enjoying the Grand Canyon?",
      "prediction": "Thankfully, they enjoyed the Grand Canyon a lot!",
      "reference": "She was happy and thankful",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.15384615384615385,
        "rouge2_f": 0.0,
        "rougeL_f": 0.15384615384615385,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.863541305065155,
        "bert_recall": 0.8647509813308716,
        "bert_f1": 0.8641457557678223,
        "meteor": 0.0,
        "sbert_similarity": 0.23415347933769226
      }
    },
    {
      "sample_id": 0,
      "question": "What do Melanie's family give her?",
      "prediction": "love, joy, and happiness",
      "reference": "Strength and motivation",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.28571428571428575,
        "rouge1_f": 0.28571428571428575,
        "rouge2_f": 0.0,
        "rougeL_f": 0.28571428571428575,
        "bleu1": 0.16666666666666669,
        "bleu2": 0.05773502691896258,
        "bleu3": 0.04506819749679463,
        "bleu4": 0.040824829046386304,
        "bert_precision": 0.8891538381576538,
        "bert_recall": 0.9076160788536072,
        "bert_f1": 0.8982900977134705,
        "meteor": 0.16129032258064518,
        "sbert_similarity": 0.3118934631347656
      }
    },
    {
      "sample_id": 0,
      "question": "How did Melanie feel about her family supporting her?",
      "prediction": "Melanie felt lucky and grateful for her family's support, as they bring joy and love into her life.",
      "reference": "She appreciated them a lot",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8503235578536987,
        "bert_recall": 0.8951960206031799,
        "bert_f1": 0.8721829652786255,
        "meteor": 0.07936507936507937,
        "sbert_similarity": 0.44344449043273926
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie do after the road trip to relax?",
      "prediction": "The kids loved it and it was a nice way to relax after the road trip.",
      "reference": "Went on a nature walk or hike",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.09523809523809523,
        "rouge1_f": 0.08695652173913043,
        "rouge2_f": 0.0,
        "rougeL_f": 0.08695652173913043,
        "bleu1": 0.0588235294117647,
        "bleu2": 0.019174124721184262,
        "bleu3": 0.014076093604378503,
        "bleu4": 0.011502783619900045,
        "bert_precision": 0.8499734997749329,
        "bert_recall": 0.8401772975921631,
        "bert_f1": 0.8450470566749573,
        "meteor": 0.06329113924050632,
        "sbert_similarity": 0.31323835253715515
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline realize after her charity race?",
      "prediction": "self-care is important",
      "reference": "self-care is important",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 0.9999997019767761,
        "bert_recall": 0.9999997019767761,
        "bert_f1": 0.9999997019767761,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "What are Melanie's plans for the summer with respect to adoption?",
      "prediction": "Not mentioned in the conversation",
      "reference": "researching adoption agencies",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8703114986419678,
        "bert_recall": 0.86725914478302,
        "bert_f1": 0.868782639503479,
        "meteor": 0.0,
        "sbert_similarity": 0.010256883688271046
      }
    },
    {
      "sample_id": 0,
      "question": "What type of individuals does the adoption agency Melanie is considering support?",
      "prediction": "LGBTQ+ individuals",
      "reference": "LGBTQ+ individuals",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "Why did Melanie choose the adoption agency?",
      "prediction": "because of their inclusivity and support for LGBTQ+ individuals",
      "reference": "because of their inclusivity and support for LGBTQ+ individuals",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.000000238418579,
        "bert_recall": 1.000000238418579,
        "bert_f1": 1.000000238418579,
        "meteor": 0.9993141289437586,
        "sbert_similarity": 1.000000238418579
      }
    },
    {
      "sample_id": 0,
      "question": "What is Melanie excited about in her adoption process?",
      "prediction": "creating a family for kids who need one",
      "reference": "creating a family for kids who need one",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.9990234375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What does Melanie's necklace symbolize?",
      "prediction": "love, faith, and strength",
      "reference": "love, faith, and strength",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9999997615814209,
        "bert_recall": 0.9999997615814209,
        "bert_f1": 0.9999997615814209,
        "meteor": 0.9921875,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What country is Melanie's grandma from?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Sweden",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8494402170181274,
        "bert_recall": 0.8910870552062988,
        "bert_f1": 0.8697653412818909,
        "meteor": 0.0,
        "sbert_similarity": 0.1274149864912033
      }
    },
    {
      "sample_id": 0,
      "question": "What was grandma's gift to Melanie?",
      "prediction": "Not mentioned in the conversation",
      "reference": "necklace",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8458929061889648,
        "bert_recall": 0.8909751176834106,
        "bert_f1": 0.8678489923477173,
        "meteor": 0.0,
        "sbert_similarity": 0.17644496262073517
      }
    },
    {
      "sample_id": 0,
      "question": "What was grandpa's gift to Caroline?",
      "prediction": "Not mentioned in the conversation",
      "reference": "necklace",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8458929061889648,
        "bert_recall": 0.8909751176834106,
        "bert_f1": 0.8678489923477173,
        "meteor": 0.0,
        "sbert_similarity": 0.17644496262073517
      }
    },
    {
      "sample_id": 0,
      "question": "What is Caroline's hand-painted bowl a reminder of?",
      "prediction": "art and self-expression",
      "reference": "art and self-expression",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 0.9999997615814209,
        "bert_recall": 0.9999997615814209,
        "bert_f1": 0.9999997615814209,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline and her family do while camping?",
      "prediction": "explored nature, roasted marshmallows, and went on a hike",
      "reference": "explored nature, roasted marshmallows, and went on a hike",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.000000238418579,
        "bert_recall": 1.000000238418579,
        "bert_f1": 1.000000238418579,
        "meteor": 0.9993141289437586,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of counseling and mental health services is Melanie interested in pursuing?",
      "prediction": "working with trans people, helping them accept themselves and supporting their mental health",
      "reference": "working with trans people, helping them accept themselves and supporting their mental health",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9997724169321802,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of counseling workshop did Melanie attend recently?",
      "prediction": "Not mentioned in the conversation",
      "reference": "LGBTQ+ counseling workshop",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8808152675628662,
        "bert_recall": 0.822563648223877,
        "bert_f1": 0.8506934642791748,
        "meteor": 0.0,
        "sbert_similarity": 0.11074960231781006
      }
    },
    {
      "sample_id": 0,
      "question": "What motivated Melanie to pursue counseling?",
      "prediction": "Not mentioned in the conversation",
      "reference": "her own journey and the support she received, and how counseling improved her life",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.11764705882352941,
        "rouge1_f": 0.10526315789473682,
        "rouge2_f": 0.0,
        "rougeL_f": 0.10526315789473682,
        "bleu1": 0.027067056647322542,
        "bleu2": 0.009569649651041096,
        "bleu3": 0.007666923387854731,
        "bleu4": 0.007271361304044331,
        "bert_precision": 0.8843754529953003,
        "bert_recall": 0.8464586734771729,
        "bert_f1": 0.8650017380714417,
        "meteor": 0.038167938931297704,
        "sbert_similarity": 0.0042390767484903336
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of place does Melanie want to create for people?",
      "prediction": "a safe and inviting place for people to grow",
      "reference": "a safe and inviting place for people to grow",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.9993141289437586,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "Did Caroline make the black and white bowl in the photo?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Yes",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.848052978515625,
        "bert_recall": 0.8904046416282654,
        "bert_f1": 0.8687129020690918,
        "meteor": 0.0,
        "sbert_similarity": 0.11724148690700531
      }
    },
    {
      "sample_id": 0,
      "question": "What are the new shoes that Caroline got used for?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Running",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8501409292221069,
        "bert_recall": 0.8962050080299377,
        "bert_f1": 0.8725653886795044,
        "meteor": 0.0,
        "sbert_similarity": 0.16544067859649658
      }
    },
    {
      "sample_id": 0,
      "question": "What is Caroline's reason for getting into running?",
      "prediction": "Not mentioned in the conversation",
      "reference": "To de-stress and clear her mind",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8849432468414307,
        "bert_recall": 0.8652690649032593,
        "bert_f1": 0.8749955296516418,
        "meteor": 0.0,
        "sbert_similarity": 0.15259525179862976
      }
    },
    {
      "sample_id": 0,
      "question": "What does Caroline say running has been great for?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Her mental health",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8695222735404968,
        "bert_recall": 0.8530548810958862,
        "bert_f1": 0.8612099289894104,
        "meteor": 0.0,
        "sbert_similarity": 0.2080099880695343
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie see at the council meeting for adoption?",
      "prediction": "Not mentioned in the conversation",
      "reference": "many people wanting to create loving homes for children in need",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.12500000000000003,
        "rouge1_f": 0.12500000000000003,
        "rouge2_f": 0.0,
        "rougeL_f": 0.12500000000000003,
        "bleu1": 0.060238842382440407,
        "bleu2": 0.02129764696972561,
        "bleu3": 0.01706305179528683,
        "bleu4": 0.016182712188007015,
        "bert_precision": 0.8724705576896667,
        "bert_recall": 0.8342359066009521,
        "bert_f1": 0.8529249429702759,
        "meteor": 0.04807692307692308,
        "sbert_similarity": 0.013970758765935898
      }
    },
    {
      "sample_id": 0,
      "question": "What inspired Melanie's painting for the art show?",
      "prediction": "Not mentioned in the conversation",
      "reference": "visiting an LGBTQ center and wanting to capture unity and strength",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8672127723693848,
        "bert_recall": 0.8251692056655884,
        "bert_f1": 0.8456687331199646,
        "meteor": 0.0,
        "sbert_similarity": -0.0016056075692176819
      }
    },
    {
      "sample_id": 0,
      "question": "What inspired Caroline's sculpture for the art show?",
      "prediction": "visiting an LGBTQ center and wanting to capture unity and strength",
      "reference": "visiting an LGBTQ center and wanting to capture unity and strength",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9996243425995492,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "How often does Caroline go to the beach with her kids?",
      "prediction": "Not mentioned in the conversation",
      "reference": "once or twice a year",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8639419078826904,
        "bert_recall": 0.807560920715332,
        "bert_f1": 0.8348005414009094,
        "meteor": 0.0,
        "sbert_similarity": 0.05305672436952591
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline and her family see during their camping trip last year?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Perseid meteor shower",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8497328758239746,
        "bert_recall": 0.7694871425628662,
        "bert_f1": 0.8076215982437134,
        "meteor": 0.0,
        "sbert_similarity": 0.033341582864522934
      }
    },
    {
      "sample_id": 0,
      "question": "How did Caroline feel while watching the meteor shower?",
      "prediction": "Not mentioned in the conversation",
      "reference": "in awe of the universe",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4000000000000001,
        "rouge1_f": 0.4000000000000001,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4000000000000001,
        "bleu1": 0.4,
        "bleu2": 0.10000000000000002,
        "bleu3": 0.0712114757473708,
        "bleu4": 0.06389431042462725,
        "bert_precision": 0.8885139226913452,
        "bert_recall": 0.8671826124191284,
        "bert_f1": 0.8777186870574951,
        "meteor": 0.20000000000000004,
        "sbert_similarity": 0.1592198759317398
      }
    },
    {
      "sample_id": 0,
      "question": "Why did Caroline choose to use colors and patterns in her pottery project?",
      "prediction": "Not mentioned in the conversation",
      "reference": "She wanted to catch the eye and make people smile.",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.13333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.13333333333333333,
        "bleu1": 0.060238842382440407,
        "bleu2": 0.02129764696972561,
        "bleu3": 0.01706305179528683,
        "bleu4": 0.016182712188007015,
        "bert_precision": 0.8825770616531372,
        "bert_recall": 0.8720123767852783,
        "bert_f1": 0.8772628903388977,
        "meteor": 0.052631578947368425,
        "sbert_similarity": 0.04110630601644516
      }
    },
    {
      "sample_id": 0,
      "question": "Is Oscar Melanie's pet?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Yes",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.848052978515625,
        "bert_recall": 0.8904046416282654,
        "bert_f1": 0.8687129020690918,
        "meteor": 0.0,
        "sbert_similarity": 0.11724148690700531
      }
    },
    {
      "sample_id": 0,
      "question": "Where did Oscar hide his bone once?",
      "prediction": "Not mentioned in the conversation",
      "reference": "In Melanie's slipper",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.22222222222222224,
        "rouge2_f": 0.0,
        "rougeL_f": 0.22222222222222224,
        "bleu1": 0.2,
        "bleu2": 0.07071067811865477,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.05372849659117709,
        "bert_precision": 0.8764923810958862,
        "bert_recall": 0.8316311836242676,
        "bert_f1": 0.8534726500511169,
        "meteor": 0.15625,
        "sbert_similarity": 0.17263294756412506
      }
    },
    {
      "sample_id": 0,
      "question": "What activity did Melanie used to do with her dad?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Horseback riding",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8533902168273926,
        "bert_recall": 0.7734711766242981,
        "bert_f1": 0.8114677667617798,
        "meteor": 0.0,
        "sbert_similarity": 0.05627930164337158
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie make for a local church?",
      "prediction": "Not mentioned in the conversation",
      "reference": "a stained glass window",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8690841794013977,
        "bert_recall": 0.8363255262374878,
        "bert_f1": 0.8523901700973511,
        "meteor": 0.0,
        "sbert_similarity": 0.0361701101064682
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie find in her neighborhood during her walk?",
      "prediction": "Not mentioned in the conversation",
      "reference": "a rainbow sidewalk",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8700211048126221,
        "bert_recall": 0.8560807108879089,
        "bert_f1": 0.8629945516586304,
        "meteor": 0.0,
        "sbert_similarity": -0.0485512837767601
      }
    },
    {
      "sample_id": 0,
      "question": "Which song motivates Melanie to be courageous?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Brave by Sara Bareilles",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8708803057670593,
        "bert_recall": 0.8365833163261414,
        "bert_f1": 0.8533872961997986,
        "meteor": 0.0,
        "sbert_similarity": -0.004309769719839096
      }
    },
    {
      "sample_id": 0,
      "question": "What type of instrument does Caroline play?",
      "prediction": "Not mentioned in the conversation",
      "reference": "clarinet and violin",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8583570718765259,
        "bert_recall": 0.7918205857276917,
        "bert_f1": 0.8237473964691162,
        "meteor": 0.0,
        "sbert_similarity": 0.012888381257653236
      }
    },
    {
      "sample_id": 0,
      "question": "Which classical musicians does Caroline enjoy listening to?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Bach and Mozart",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.873533308506012,
        "bert_recall": 0.8278999328613281,
        "bert_f1": 0.8501046895980835,
        "meteor": 0.0,
        "sbert_similarity": 0.10292410850524902
      }
    },
    {
      "sample_id": 0,
      "question": "Who is Caroline a fan of in terms of modern music?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Ed Sheeran",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8636482357978821,
        "bert_recall": 0.7347403168678284,
        "bert_f1": 0.7939961552619934,
        "meteor": 0.0,
        "sbert_similarity": 0.09608791023492813
      }
    },
    {
      "sample_id": 0,
      "question": "What precautionary sign did Caroline see at the caf\u00e9?",
      "prediction": "A sign stating that someone is not being able to leave",
      "reference": "A sign stating that someone is not being able to leave",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9996243425995492,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "What setback did Caroline face recently?",
      "prediction": "Not mentioned in the conversation",
      "reference": "She got hurt and had to take a break from pottery.",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8869463205337524,
        "bert_recall": 0.8621916770935059,
        "bert_f1": 0.874393880367279,
        "meteor": 0.0,
        "sbert_similarity": 0.03774166479706764
      }
    },
    {
      "sample_id": 0,
      "question": "What does Caroline do to keep herself busy during her pottery break?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Read a book and paint.",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8759546279907227,
        "bert_recall": 0.8547957539558411,
        "bert_f1": 0.8652458786964417,
        "meteor": 0.0,
        "sbert_similarity": 0.08983373641967773
      }
    },
    {
      "sample_id": 0,
      "question": "What was the poetry reading that Melanie attended about?",
      "prediction": "It was a transgender poetry reading where transgender people shared their stories.",
      "reference": "It was a transgender poetry reading where transgender people shared their stories.",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9997106481481481,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What happened to Caroline's son on their road trip?",
      "prediction": "He got into an accident",
      "reference": "He got into an accident",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.996,
        "sbert_similarity": 1.000000238418579
      }
    },
    {
      "sample_id": 0,
      "question": "How did Caroline's son handle the accident?",
      "prediction": "Not mentioned in the conversation",
      "reference": "He was scared but reassured by his family",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8755668997764587,
        "bert_recall": 0.845429539680481,
        "bert_f1": 0.8602343797683716,
        "meteor": 0.0,
        "sbert_similarity": 0.12110542505979538
      }
    },
    {
      "sample_id": 0,
      "question": "How did Caroline feel about her family after the accident?",
      "prediction": "Not mentioned in the conversation",
      "reference": "They are important and mean the world to her",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14285714285714285,
        "rouge1_f": 0.14285714285714285,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14285714285714285,
        "bleu1": 0.08986579282344431,
        "bleu2": 0.03177235575108143,
        "bleu3": 0.02545508208534111,
        "bleu4": 0.02414176971688927,
        "bert_precision": 0.8852692246437073,
        "bert_recall": 0.8500819206237793,
        "bert_f1": 0.867318868637085,
        "meteor": 0.05813953488372093,
        "sbert_similarity": 0.18959122896194458
      }
    },
    {
      "sample_id": 0,
      "question": "How did Caroline's children handle the accident?",
      "prediction": "Not mentioned in the conversation",
      "reference": "They were scared but resilient",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8781701922416687,
        "bert_recall": 0.8506823182106018,
        "bert_f1": 0.8642077445983887,
        "meteor": 0.0,
        "sbert_similarity": 0.07462228834629059
      }
    },
    {
      "sample_id": 0,
      "question": "How did Caroline feel after the accident?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Grateful and thankful for her family",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8705779314041138,
        "bert_recall": 0.8196113705635071,
        "bert_f1": 0.8443261384963989,
        "meteor": 0.0,
        "sbert_similarity": 0.012023810297250748
      }
    },
    {
      "sample_id": 0,
      "question": "What was Caroline's reaction to her children enjoying the Grand Canyon?",
      "prediction": "She was happy and thankful",
      "reference": "She was happy and thankful",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.000000238418579,
        "bert_recall": 1.000000238418579,
        "bert_f1": 1.000000238418579,
        "meteor": 0.996,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline do after the road trip to relax?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Went on a nature walk or hike",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8826771974563599,
        "bert_recall": 0.8326032161712646,
        "bert_f1": 0.8569092750549316,
        "meteor": 0.0,
        "sbert_similarity": 0.06870591640472412
      }
    },
    {
      "sample_id": 0,
      "question": "What does Caroline love most about camping with her family?",
      "prediction": "Being present and bonding with her family",
      "reference": "Being present and bonding with her family",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9999997019767761,
        "bert_recall": 0.9999997019767761,
        "bert_f1": 0.9999997019767761,
        "meteor": 0.9985422740524781,
        "sbert_similarity": 1.0
      }
    }
  ]
}